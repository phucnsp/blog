{
  
    
  
    
        "post1": {
            "title": "Machine Learning with Graph - Preliminaries part 2",
            "content": "This is my personal notes for The Standford CS224W Course: Machine Learning with Graphs - Fall 2019 which I have been studying from its public resources for a couple of months. I will try to keep this blog series live update as my main source knowlege about graph. The course covers tremendous knowledge for understanding and effectively learning from large-scale network. In general, it is organized into 3 big sections: . preliminaries: provides basic background about networks/graphs. | network methods: methods and algorithms which help us dealing with graph data structure. | machine learning with networks: showing how we are applying machine learning into this field. | . This notebook is part 2 of the preliminary section in which we will talk about network properties and some random graph models. . Here is the link to other parts: 1. . Network Properties: How to Measure a Network . In this section, we will take a look on four key network properties which can be used to characterize a graph. These properties are extremely useful when we want to compare our model and real-world networks to see whether it fits. . Degree Distribution: $P(k)$ | Path Length: $h$ | Clustering Coefficient: $C$ | Connected Components: $s$ | . The definitions will be presented for undirected graphs, but can be easily extended to directed graphs. . Degree Distribution $P(k)$ . The degree distribution $P(k)$ measures the probability that a randomly chosen node has degree $k$. The degree distribution of a graph $G$ can be summarized by a normalized histogram, where we normalize the histogram by the total number of nodes. We can compute the degree distribution of a graph by: begin{equation} P(k)= frac{N_k}{N}, N_{k}= #nodes _with _degree _k, N= #nodes _in _graph end{equation} . . Note: To extend these definitions to a directed graph, we compute separately both in-degree and out-degree distribution. . Paths in a Graph . A path is a sequence of nodes in which each node is linked to the next one: begin{equation} P_{n}= left {i_{0}, i_{1}, i_{2}, ldots, i_{n} right } end{equation} such that $ left { left(i_{0}, i_{1} right), left(i_{1}, i_{2} right), left(i_{2}, i_{3} right), ldots, left(i_{n-1}, i_{n} right) right } in E$ . Note: a path can intersect itself and pass through the same edge multiple times, e.g ACBDCDEG The distance(shortest path, geodesic) between a pair of nodes is defined as the number of edges along the shortest path connecting the nodes. If two nodes are not connected, the distance is usually defined as infinite(or zero). In a directed graph, paths need to follow the direction of the arrows. Thus, distance is not symmetric for directed graphs. For a graph with weighted edges, the distance is the minimum number of edge weight needed to traverse to get from one node to another. The Diameter of a graph is the maximum(shortest path) distance between any pair of nodes in a graph. The average path length for a connected graph or a strongly connected directed graph is the average shortest path between all connected nodes. We compute the average path length as: begin{equation} hat{h}= frac{1}{2E_{ max}} sum_{i, j neq i} h_{i j} end{equation} where: $E_{max}$ is the max number of edges or node pairs; that is, $E_{max}= left( begin{array}{c}N 2 end{array} right)= frac{N(N-1)}{2}$. $h_{ij}$ is the distance from node $i$ to node $j$. . Note: we only compute the average path length over connected pairs of nodes, and thus ignore infinite length paths. In addition, the measure also applied to (strongly) connected components of a graph. . Clustering Coefficient . The clustering coefficient (for undirected graph) measures what proportion of node $i$&#39;s neighbors are connected. For node $i$ with degree $k_i$, we compute the clustering coeffient as begin{equation} C_i = frac{2e_i}{k_i(k_i-1)}, C_{i} in [0,1] end{equation} where: $e_i$ is the number of edges between the neighbors of node $i$. . $ frac{k_{i}(k_{i}-1)}{2}$ is the maximum number of edges between the $k_i$ neighbors . Note: the clustering coefficient is undefined for nodes with degree 0 or 1. . We can also compute the average clustering coefficient as begin{equation} C = frac{1}{N} sum_{i}^{N} C_{i}. end{equation} . The average clustering coefficient allows us to see if edges appear more densely in parts of the network. In social networks, the average clustering coefficient tends to be very high indicating that, as we expect, friends of friends tend to know each other. . . Connectivity . The connectivity of a graph measures the size of the largest connected component. The largest connected component is the largest set where any two vertices can be joined by a path. . Note: Largest component = Giant component . To find connected components: . Start from a random node and perform breadth first search (BFS). If you are not familiar wit BFS, see here. | Label the nodes that BFS visits | If all the nodes are visited, the network is connected | Otherwise find an unvisited node and repeat BFS | Measure Network Properties of some real-world Networks . Now we will take a look at a real-world network, the protein-protein interation network, and see how much the four key properties are. . Some observations from this network are: . The degree distribution is highly skewed, in a sense that majority of proteins have small degree and a few proteins have very high degree. On average, a node has around 3 neighbors. | The diameter of this graph is around 6, it means within 6 steps you can reach any node in the (connected) graph. Does it surprise you? empirically, even with other networks with million of nodes, the diameter is also somewhere lower than 10. This characteristic relates to the concept of depth in graph neural network which we will see later. | The average clustering is around 0.12, that means, on average, only around 10% of node&#39;s neightbors are connected one another. | Lastly, the largest component has over 80% nodes of the network. | . So are these values &quot;expected&quot;? Are they &quot;surprising&quot;? So far we dont know, we need a model to see whether they are suprising or not. In the next part, we will go through some random graph models. Their power to model real-world networks is low but it helps us to have kind of baseline model to compare with other more powerful models. In addition, these random models are simple enough that we can calculate their properties by hand and see if a property can be easily obtained by random. . Erdos-Renyi Random Graph Model . The Erdos-Renyi Random Graph Model is the simplest model of graphs. This simple model has proven networks properties and is a good baseline to compare real-world graph properties with. This random graph model comes in two variants: . $G_{np}$: undirected graph on $n$ nodes where each edge $(u,v)$ appears i.i.d with probability $p$. | $G_{nm}$: undirected graph with $n$ nodes, and $m$ edges picked uniformly at random. | Note that both the $G_{np}$ and $G_{nm}$ graph are not uniquely determined, but rather the result of a random procedure. We can have many different realizations given the same $n$ and $p$ . Let&#39;s take a look on the key properties of this random graph model. . Degree Distribution of $G_{np}$ . The degree distribution of $G_{np}$ is binomial. Briefly remind, this distribution can be thought of as simply the probability of a SUCCESS or FAILURE outcome in an experiment that is repeated multiple times. It is a discrete analogue of a Gaussian and has a bell-shape. Binomial distribution has two variable: $n$ - number of times the experiment runs, $p$ - the probability of one specific outcome. . . Clustering Coefficient of $G_{np}$ . Recall that the clustering coefficient is computed as: . begin{equation} C_{i}=2 frac{e_i}{k_{i}(k_{i}-1)} end{equation}where $e_i$ is the number of edges between $i$&#39;s neighbors. Edges in $G_{np}$ appear i.i.d with probability $p$, so the expected $e_i$ for $G_{np}$ is: . begin{equation} mathbb{E} left[e_{i} right]=p frac{k_{i} left(k_{i}-1 right)}{2} end{equation} . Note: This is because $ frac{k_i(k_i-1)}{2}$ is the number of distinct pairs of neighbors of node $i$ of degree $k_i$, and each pair is connected with probability $p$. . Thus, the expected clustering coefficient is . begin{equation} mathbb{E} left[C_{i} right]= frac{p cdot k_{i} left(k_{i}-1 right)}{k_{i} left(k_{i}-1 right)}=p= frac{ bar{k}}{n-1} approx frac{ bar{k}}{n} end{equation}where $ bar{k}$ is the average degree. From this, we can see that the clustering coefficient of $G_{np}$ is very small because $ bar{k}$ is really small compare to number of nodes $n$. If we generate bigger and bigger graphs with fixed average degree $ bar{k}$, then $C$ decreases with graph size $n$. begin{equation} mathbb{E} left[C_{i} right] rightarrow 0, n rightarrow infty end{equation} . Path Length of $G_{np}$ . To discuss the path length of $G_{np}$, we first introduce the concept of expansion. Graph $G(V, E)$ has expansion $ alpha$: if $ forall S subset V$, the number of edges leaving $S geq alpha cdot min (|S|,|V backslash S|)$ (for any subset of nodes, the number of edges leaving is greater than $ alpha$ * the length of subset). Expansion answers the question &quot;if we pick a random set of nodes, how many edges are going to leave the set?&quot;. Expansion is a measure of robustness: to disconnect $l$ nodes, one must cut $ geq alpha cdot ell $ edges. Equivalently, we can say a graph $G(V, E)$ has an expansion $ alpha$ such that: begin{equation} alpha= min _{S subset V} frac{ # text { edges leaving } S}{ min (|S|,|V backslash S|)} end{equation} . An important fact about expansion is that in a graph with $n$ nodes with expansion $ alpha$, for all pairs of nodes, there is a pair of path length $O(( log n) / alpha)$ connecting them. Intuition, we can think of it like if we start with a subset of node and for each step the number of nodes visited increase with a factor of $ alpha$. Thus, it will take $ log n$ of steps to visit all nodes. For the case of random graph $G_{np}$ and $ log n &gt; np &gt; c$, $c$ is a constant and $np$ is the average degree. begin{equation} diam(G_{np}) = O( frac{ log n}{ log (np)}) end{equation} . Note: the reason we want to have average degree in between logn and a constant c is because we need a reasonable degree to have small diameter. . Thus, we can see that random graphs have good expansion so it takes as logarithmic number of steps for BFS to visit all nodes. Key thing to note, the path length of $G_{np}$ is $O( log n)$. From this result, we can see that $G_{np}$ can grow very large, but nodes will still remain a few hops apart. . Connectivity of $G_{np}$ . The graphic below shows the evolution of a $G_{np}$ random graph as $p$ change. We can see that there is an emergency of a giant component when average degree higher than 1. . if $k=1- epsilon$, then all components are of size $ Omega( log n)$. | if $k=1+ epsilon$, there exists 1 component of size $ Omega(n)$, and all other componnents have size $ Omega( log n)$. In other words, if $ bar{k}&gt;1$, we expect a single large component. In addition, in this case, each node has at least one edge in expectation. | . So as average degree $k&lt;1$, fraction of nodes in largest component does not change much but it will start to increase exponentially as $k&gt;1$. . Comparing a real network with $G_{np}$ . In order to see if our random model fits well with a real network, let&#39;s take a look on MSN Messenger network in 1 month of activify which has 180 million users and 1.3 billion edges. . As we can see, giant connected component and average path length might fit between them but clustering coefficient and degree distribution do not. The problem with the random network model are: . Degree distribution differs from that of real networks | Giant component in most real networks does NOT emerge through a phase transition | No local structure - clustering coefficient is too low. Thus, we can conclude that real networks obviously are not random. | . Although the random network $G_{np}$ is WRONG to model real network but it is still useful because: . It is the reference model for the rest of the class | It will help us calculate many quantities, that can then be compared to the real data | It will help us understand to what degree a particular property is the result of some random process. | . The Small-World Model . This model is the improvement of random model in which it tackles the problem of constructing graph with low average path length and high clustering. The Small-World model was first introduced in 1998 by Ducan J.Watts and Steven Strogatz. To create such model, we employ the following steps: . Start with a low-dimensional regular lattice(ring) by connecting each node to $k$ neighbors on its right and $k$ neightbors on its left, with $k geq 2$. So we have a lattice with high clustering coefficient. | Rewire each edge with probability $p$ by moving its endpoint to a randomly chosen node. This step will add/remove edges to create shortcuts to join remote parts of the lattice. So we are introducing randomness (shortcut) into the lattice. The rewiring allows us to &quot;interpolate&quot; between a regular lattice and a random graph. . Note: Clustering implies edge &quot;locality&quot;. Randomness enables &quot;shortcuts&quot; | Then, we make the following observations: . At $p=0$ where no rewiring has occured, this remains a grid network with high clustering, high diameter. | For $0&lt;p&lt;1$ some edges have been rewired, but most of the structure remains. This implies both locality and shortcuts. This allows for both high clustering and low diameter. | At $p=1$ where all edges have been randomly rewired, this is a Erdos-Renyi (ER) random graph with low clustering, low diameter. . Note: Small world model are parameterized by the probability of rewiring $p in[0,1]$ | . From a social network perspective, the phenomenon of introducing shortcuts while keeping strong structure is intuitive. While most our friends are local, but we also have a few long distance friendships in different countries which is enough to collapse the diamter of the human social network, explaining the popular notion of Six Degrees of Separation&quot;. In summary, the small world model: . provides insight on the interplay between clustering and the small-world. | captures the structure of many realistic networks. | accounts for the high clustering of real networks. | but it does not lead to the correct degree distribution of real-world network. | . Kronecker Graph Model . Models of graph generation have been studied extensively. Such models allow us to generate graphs for simulations and hypothesis testing when collecting the real graph is difficult, and also forces us to examine the network properties that generative models should obey to be considered realistic. In formulating graph generation models, there are two important considerations: . The ability to generate realistic networks. | The mathematical tractability of the models, which allows for the rigorous analysis of network properties. | . The Kronecker Graph Model is a recursive graph generation model that combines both mathematical tractability and realistic static and temporal network properties. The intuition underlying the Kronecker Graph Model is self-similarity, where the whole has the same shape as one or more of its parts. . Kronecker Product . The Kronecker product $ otimes$, a non-standard matrix operation, is a way of generating self-similar matrics. For two arbitarily sized matrices $ mathbf{A} in mathbb{R}^{n times m}$ and $ mathbf{B} in mathbb{R}^{k times l}, mathbf{A} otimes mathbf{B} in mathbb{R}^{n k times m l}$, their Kronecker product is: begin{equation} mathbf{C}={ mathbf{A}} otimes mathbf{B} doteq left( begin{array}{cccc} a_{1,1} mathbf{B} &amp; a_{1,2} mathbf{B} &amp; dots &amp; a_{1, m} mathbf{B} a_{2,1} mathbf{B} &amp; a_{2,2} mathbf{B} &amp; dots &amp; a_{2, m} mathbf{B} vdots &amp; vdots &amp; ddots &amp; vdots a_{n, 1} mathbf{B} &amp; a_{n, 2} mathbf{B} &amp; dots &amp; a_{n, m} mathbf{B} end{array} right) end{equation} . For example, we have that $$ begin{bmatrix} 1&amp;2 3&amp;4 end{bmatrix} otimes begin{bmatrix} 0&amp;5 6&amp;7 end{bmatrix} = begin{bmatrix} 1 begin{bmatrix} 0&amp;5 6&amp;7 end{bmatrix} &amp;2 begin{bmatrix} 0&amp;5 6&amp;7 end{bmatrix} 3 begin{bmatrix} 0&amp;5 6&amp;7 end{bmatrix} &amp;4 begin{bmatrix} 0&amp;5 6&amp;7 end{bmatrix} end{bmatrix} = begin{bmatrix} 1 times 0 &amp; 1 times 5 &amp; 2 times 0 &amp; 2 times 5 1 times 6 &amp; 1 times 7 &amp; 2 times 6 &amp; 2 times 7 3 times 0 &amp; 3 times 5 &amp; 4 times 0 &amp; 4 times 5 3 times 6 &amp; 3 times 7 &amp; 4 times 6 &amp; 4 times 7 end{bmatrix} = begin{bmatrix} 0 &amp; 5 &amp; 0 &amp; 10 6 &amp; 7 &amp; 12 &amp; 14 0 &amp; 15 &amp; 0 &amp; 20 18 &amp; 21 &amp; 24 &amp; 28 end{bmatrix}$$ . Kronecker Graphs . To use the Kronecker product in graph generation, we define the Kronecker product of two graphs as the Kronecker product of the adjacency matrices of the two graphs. Kronecker graph is obtained by growing sequence of graphs by iterating the Kronecker product over the initiator matrix $K_1$ (an adjacency matrix of a graph): $$K_1^{[m]}=K_m = underbrace{K_1 otimes K_1 otimes dots K_1}_{ text{m times}}= K_{m-1} otimes K_1$$ . Intuitively, the Kronecker power construction can be imagined as recursive growth of the communities within the graph, with nodes in the community recursively getting expanded into miniature copies of the community. . One can easily use multiple initiator matrices $(K_{1}^{ prime}, K_{1}^{ prime prime}, K_{1}^{ prime prime prime})$ (even of different sizes), which iteratively affects the structure of the larger graph. . Stochastic Kronecker Graphs . Up to now, we have only considered $K_1$ initiator matrices with binary values $ {0,1 }$. However, such graphs generated from such initiator matrices have &quot;staircase&quot; effects in the degree distributions and individual values occur very frequently because of the discrete nature of $K_1$. To negate this effect, stochasticity is introduced by relaxing the assumption that the entries in the initiator matrix can only take binary values. Instead entries in $ Theta_1$ can take values on the interval $[0, 1]$, and each represents the probability of that particular edge appearing. Then the matrix (and all the generated larger matrix products) represents the probability distribution over all possible graphs from that matrix. . More concretely, for probability matrix $ Theta_1$, we compute the $k^{th}$ Kronecker power $ Theta_k$ as the large stochastic adjacency matrix. Each entry $p_{uv}$ in $ Theta_k$ then represents the probability of edge $(u, v)$ appearing. . . Notes:the probabilities do not have to sum up to 1 as each the probability of each edge appearing is independent from other edges. To obtain an instance of a graph, we then sample from the distribution by sampling each edge with probability given by the corresponding entry in the stochastic adjacency matrix. The sampling can be thought of as the outcomes of flipping biased coins where the bias is parameterized from each entry in the matrix. . However, this means that the time to naively generate an instance is quadratic in the size of the graph, $O(N^2)$! too slow. Is there a faster way? . Fast Generation of Stochastic Kronecker Graphs . A fast heuristic procedure that takes time linear in the number of edges to generate a graph exists. The general idea can be described as follows: for each edge, we recursively choose sub-regions of the large stochastic matrix with probability proportional to $p_{uv} in Theta_{1}$ until we descend to a single cell of the large stochastic matrix. We replace the edge there. For a Kronecker graph of $k^{th}$ power, $ Theta_k$, the descent will take $k$ steps. For example, we consider the case where $ Theta_1$ is a $2 x 2$ matrix, such that begin{equation} Theta= left[ begin{array}{ll} a &amp; b c &amp; d end{array} right] end{equation} . For graph $G$ with $n = 2^k$ nodes, we need to define how many edges we want to generate (we can reference of the ratio between number of nodes and number of edges). Then, the algorithm to generate each edge is shown below: . In practice, the stochastic Kronecker graph model is able to generate graphs that match the properties of real world networks well. To read more about the Kronecker Graph models, refer to J Leskovec et al., Kronecker Graphs: An Approach to Modeling Networks (2010). . References . main source: Standford CS224W Machine Learning with Graphs | other sources Graph on structured documents blog post | Graph theory blog post | Graph embedding blog post | Graph convolution blog post | Geometric deep learning blog post | review papers about graphs: 1, 2 | Pytorch Geometric:1, 2 | . | .",
            "url": "https://phucnsp.github.io/blog/cs224w/2020/06/13/ML-graph-premilinary-part2.html",
            "relUrl": "/cs224w/2020/06/13/ML-graph-premilinary-part2.html",
            "date": " • Jun 13, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Machine Learning with Graph - Preliminaries part 1",
            "content": "This is my personal notes for The Standford CS224W Course: Machine Learning with Graphs - Fall 2019 which I have been studying from its public resources for a couple of months. I will try to keep this blog series live update as my main source knowlege about graph. The course covers tremendous knowledge for understanding and effectively learning from large-scale network. In general, it is organized into 3 big sections: . preliminaries: provides basic background about networks/graphs. | network methods: methods and algorithms which help us dealing with graph data structure. | machine learning with networks: showing how we are applying machine learning into this field. | . This notebook is part 1 of the preliminary section in which we will introduce about networks/graphs, its basic concepts and how to represent it. . Here is the link to other parts: 1. . Network Introduction and why should we care? . Networks are a general language for describing complex systems of interacting entities. Pictorially, rather than thinking that our dataset consists of a set of isolated data points, we consider interactions and relationships between these points. In general, there are 2 types of networks/graphs: . Networks are those that can be interpretated as examples of phenomena that appear in natural world, that&#39;s why it is also called Natural Graphs. A couple examples include The human social network(a collection of 7+ billion individuals) | Internet communication systems(a collection of electronic devices) | . | Information Graphs are those that can be interpretated as a data structure useful which are created in a heuristic way for solving a specific prediction problem. In this case, we&#39;re more interested in relationships between entities so we can efficiently perform learning tasks. Some examples include Information/knowledge (are organized and linked for a specific purpose,e.g google search engine) | Scene graphs (how objects in a scene relate to one another) | Similarity networks (in which similar points in a dataset are connected) | . | . While working with Networks, some of the main questions that we need to think about are How are these systems organized? what are their design properties? Behind many systems there is an intricate wiring diagram, a network, that defines the interactions between the components. We will never be able to model and predict these systems unless we understand the networks behind them. . Note: Networks: Knowledge Discovery . In the other hand, working with Graphs leverages another questions How do we take advantage of relational structure for better prediction? Complex domains (knowledge, text, images, etc) have rich relational structure, which can be represented as a relational graph for numerous prediction tasks. By explicitly modeling relationships and applying machine learning into graph, we achieve better performance in many tasks. . Note: Graphs: Machine Learning . Sometimes the distinction between network and graph is blurred and we will use them interchangeably in this blog series. . There are many ways to analyse a network/graph: . Node classification, where we predict the type/color of a given node | Link prediction, where we predict whether two nodes are linked | Community detection, where we identify densely linked clusters of nodes | Similarity computation, where we measure the similarity of two nodes or networks. . Note: two popular graph analysis tools are SNAP and NetworkX | . Nowadays, network is applied in various fields with many successes. Some of the main applications include . Network application in the social networks. We can map a social network, e.g. facebook social network, as a graph and play around with it, try to understand how it is organized? how a group of people is connected? how to advertise a product more efficiently in our network?. One of the popular tasks with social network is community detection. Throughout this task, we can explore a lot of information stays behind our network. . | Network application in infrastructure design. A typical example is the light outage in East Coast of America back in 2003. This is a kind of cascading failures where outage at a city led to failures at another cities. So the question is how can we design a robust system? how can we prevent such kind of cascading failures? By mapping the power grid as a graph and modeling this cascading behaviour, we can improve the power infrastructure design. In reality failures follow reproducible laws, that can be quantified and even predicted using the tools of networks. . | Network application in Knowledge (Knowledge Graphs). We can encode knowledge of a domain into graph and use it for downstream tasks. These graphs can even be heterogeneous, in a sense that nodes and links can be in different types. Or it can be multimodal where nodes/edges can be categorised into different groups. . | . Network application in Online Media. We can use data from twitter to map polical blogs into graph and analyse the political polarization between them. Or we can use graph&#39;s power to detect whether a given Wikipedia article is a hoax. In addition, we can also use graph in viral marketing in which we try to maximize the social impact by selecting a subset of famous people to advertise for us. And another application is product adoption where you have a new product and you are interested in how far your product will spread via inviations, how to optimize this spread. . | Network application in Biomedicine. A typical example application of graph in this field is to predict side effects of a given pair of drugs. This is a kind of link prediction where we need to predict which kind of effects might occur between two drug nodes. . | . Altogether, networks are a universal language for describing complex data, and generalize across a variety of different fields. With increased data availability and a variety of computational challenges, learning about networks leaves one poised to make a wide variety of contributions. . Basic Concepts . Structure of Graphs . A network/graph $G(N,E)$ is a collection of objects (nodes) $N$ where some pairs of objects are connected by edges/links $E$: . Objects: nodes, vertices N | Interactions: links, edges E | System: network, graph G(N,E) . Note: Technically, a network often refers to real systems (the web, a social network, etc) while a graph often refers to the mathematical representation of a network (a web graph, social graph, etc). . Note: groups of technical terms - network, node, link vs. graph, vertex, edge. However, in most cases these terms are used interchangeably. | . Both nodes and edges can contain information that is encoded as node attributes, edge attributes, respectively. There are many options for edge attributes such as: . Weight (e.g. frequency of communication) | Ranking (best friend, second best friend...) | Type (friend, relative, co-worker) | Sign: Trust vs. Distrust | Properties depending on the structure of the rest of the graph: number of common friends. | . Undirected graphs are those with symmetrical/reciprocal links(e.g.friendship on Facebook). Node degree $k_i$ of node $i$ in an undirected graph is defined as the number of edges adjacent to node $i$. For example the degree of node A in the undirected graph below $k_A=4$. The average degree is then begin{equation} bar{k}= langle k rangle= frac{1}{|N|} sum_{i=1}^{|N|} k_{i}= frac{2|E|}{N} end{equation} . Directed graphs are those with directed links(e.g.following on Twitter). In these graphs, In-degree $k_{i}^{in}$ is defined as the number of edges entering node $i$. Similarly, out-degree $k_{i}^{out}$ is defined as the number of edges leaving node $i$. The (total) degree of a node is the sum of in- and out-degrees. For example the degree of node C in directed graph below: $k_{C}^{in}=2, k_{C}^{out}=1, k_C=2$ The average degree is $ bar{k}= langle k rangle= frac{|E|}{N}$ . Complete graphs are undirected graphs with the maximum number of edges. Intuitively speaking, complete graph has all pairs of nodes connected. The complete graph has number of edges $|E|= left( begin{array}{c}N 2 end{array} right)= frac{N(N-1)}{2}$ and average degree $ bar{k}=|N|-1$. . Bipartite Graphs are those whose nodes can be divided into two disjoint/independent sets $U$ and $V$ such that every edge connects a node in $U$ to a node in $V$, there are no edges between nodes in $U$ and between nodes in $V$. Examples author-to-papers(they authored), actors-to-movies(they appeared in), etc. Bipartite graphs can be projected into Folded Networks by creating edges within independent sets $U$ or $V$ if they share at least one common neightbor. Looking at node 1 of projection $U$ in bipartite graph below, it is connected to node 2 and node 3 because node 1, 2, 3 all have common neightbor A. . Other Graph Types we briefly note that graphs can also include self-edges(self-loops), weights associated with edges, and multiple edges connecting nodes. These attributes can be encoded in graph representation with ease. . Graph Representation . There a 3 ways to represent a graph $G$: . Adjacency matrix: is one of the most common way to represent a matrix. The image below shows all information we need to know about directed and undirected graph. Notice that adjacency matrix is symmetric for undirected graph and asymmetric for directed graph. However, most real-world networks are sparse ($|E| ll E_{ max}$ or $ bar{k} ll |N|-1$). As a consequence, the adjacency matrix is filled with zeros, it is an undesirable property! . | Edge List: a graph can be represented as a set of edges(edge list). An example of edge list with the directed graph above [(1,4), (4,2), (4,3), (1,2)]. This representation makes edge lookups harder but preserves memory (alleviate mentioned above with adjacency matrix). . | Adjacency list: This method is easier to work with if the network is large and sparse. It also allows us to quickly retrieve all neightbors of a given node. Example adjacency list for the directed graph above {1:[4,2], 2:[], 3:[], 4:[2,3]} . | . Graph Connectivity . A undirected graph is called connected if there is a path between any pair of nodes in the graph. A graph is called disconnected if it is made up by two or more connected components. . The image below shows primary concepts about connectivity characteristic of an undirected graph such as connected(undirected) graph, disconnected(undirected) graph, giant component, isolated node, bridge edge, articulated node. In addition, the adjacency matrix of such networks can be represented as a block-diagonal form. . We can further extend the connectivity concepts from undirected to directed graphs by defining strongly connected directed graph, weakly connected directed graph, strongly connected components. . Note: strong connected components (SCCs) can be defined as strongly connected subgraphs of a bigger graph. . Altogether, the way we define a network, the way we choose a proper network representation of a given domain/problem determines our ability to use network successfully. In some cases, there is a unique, unambiguous representation but in other cases, the representation is by no means unique. The way we assign links will also determine the nature of question we can study. Here is some examples of network representations: . Email network: can be represented as directed multigraph with self-edges graphs. | Facebook friendships: can be represented as undirected, unweighted graphs. | Citation networks: can be represented as unweighted, directed, acyclic graphs. | Collaboration networks: can be represented as undirected multigraph or weighted graphs. | Mobile phone calls: can be represented as directed, (weighted?) multigraph. | Protein interactions: can be represented as undirected, unweighted with self-interactions graphs. | . References . main source: Standford CS224W Machine Learning with Graphs | other sources Graph on structured documents blog post | Graph theory blog post | Graph embedding blog post | Graph convolution blog post | Geometric deep learning blog post | review papers about graphs: 1, 2 | Pytorch Geometric:1, 2 | . | .",
            "url": "https://phucnsp.github.io/blog/cs224w/2020/05/27/ML-graph-premilinary-part1.html",
            "relUrl": "/cs224w/2020/05/27/ML-graph-premilinary-part1.html",
            "date": " • May 27, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "How Tesla uses neural network at scale in production",
            "content": "At the Scaled Machine Learning Conference this year 2020, Andrej Kaparthy - Director of AI at Tesla - has given a spectacular talk about how Tesla is applying AI into their system. There are so much information and distilled knowlege came out from this talk which made me can not resist to write this blog post. If you want to access to other talks from Scaled Machine Learning Conference, go here. . Note: all the images used in this note are from the slide used in Andrej&#8217;s video. . Overview . The talk is about AI for Full-Self Driving where Andrej talked about how Tesla are improving the safety and convenience of driving, how they deploy deep learning into production and supports all the features of autopilot today, how the neural net is eating through the software stack and how they are putting vision and AI at the front and center of this effort. . Full self-driving is a non-trivial task which requires you to not only follow the driving law but also to satisfy massive number of users. Tesla has built their cars to be like a real computer with eyes(cameras) on it. Beside the main functions of self-driving, they also have other great functionalities such as active safely (e.g auto detect pedestrians even when self-driving mode is off) and auto parking (auto search for the parking lot). . Different with other companies where Lidar is used as car&#39;s eyes, Tesla is using vision-based approach with cameras. The advantage of this approach is its scalability where cameras can be easily installed in millions of car. . Note: Briefly describe about Lidar, it shoots out the laser, create point cloud map and print out high definition map. . . HydraNet . Using the images from cameras, Tesla AI team has built a very large network for detecting objects that their cars have to encounter on the street. As you can see the image on the left below, in order to make decision the car needs to detect a lot of objects around it such as lane lines, static objects, road signs, crosswalks, etc. Their huge object detection network, HydraNet, has shared backbone and multiple heads, each of them is responsible for a number of tasks. And for each detecting task, there can be multiple subtasks come with it and if we list them out all, the number can be even thousand of tasks. . . Data engine and operation vacation . In order to boost the performance of HydraNet, Tesla needs a lot of data. So you have thousand of tasks to solve and each of them requires thousands of images. Wouldn&#39;t you need thousand of engineer to make it work? . Note: Tesla has around dozens of engineer! Small team but super elite. . In order to deal with such large amount of work with not so many engineers, Andrej has introduced the concept of operation vacation and data engine. . Operation vacation means the engineers might take vacation while the system still operate well. They have tried to develop as much automation machinery to support the development of new tasks and remove engineers from that loop. The infrastructure has been built so that the labeling team or PMs can actually create new detectors whenever they have new tasks. So the process, from starting point of a new detector to actually deploy it, might have a latency but fully automatic. . Tesla has built infrastructures for classes of task in order to automate as much as possible. Getting an example of a new task detecting caution lights which might be categorized as a landmark task. If this task is a member of task family which Tesla has built infrastructure for, then the things is super easy. They just need to plug-and-play the prototype infrastructure of landmark task and go through the Data Engine. . . Data Engine is the process by which they iteractively apply active learning to source additional examples in cases detector misbehaving. For example the task detecting caution lights, first they have to label a seed set of images in order to have a seed set of unit tests. The network head is then trained on current data and if it fails on the test set, they will spin on data engine to get more data and thus improve the accuracy. An approximate trigger will be trained offline and ship to the fleet(the Tesla&#39;s million car unit running on the street) in order to source more images in failing scenarios. These harvested images are then labeled and fed into training set. The network is trained again, the test set is enriched as well. These processes are iteractive and they have seen the accuracy has gone like from 40% to 99%. . Note: Data Engine helps to accumulate large dataset in the full tail distribution. . Breaking down the steps of applying data engine for the task detecting caution lights: . - Init new layer of annotation: CAUTION_LIGHT (LANDMARK) - Label a seed set of images - Develop a seed set of unit tests - Train network head on current data - While QA does not sign off: - While unit tests fail: - Deploy a trigger(an offline trained approximate detector) to the fleet(millions of Tesla car running on the street) in order to source more images in failing scenarios. - Label the resulting images and incorporate them into the training set. - Retrain network on data - Enrich/grow set of unit tests - Deploy to run on the FSD computer - (Optional) Deploy to the fleet in shadow mode - Collect telemetry and evaluate the performance of the feature - Ship feature . Evaluation metrics . In the above part, we have seen the tremendous efforts that Andrej&#39;s team has been working on for growing their training set. But Tesla has also placed just as much work into massaging the test set as they do in the training set. They spent a lot of time, inspired by test-driven development, to build out very complicated test set. Their test set is treated like unit tests which has to cover all cases. This set is so important and it is the objective for the whole system improvement. Whether you are going in right or wrong direction, it all depends on the quality of the test set. . A mechanism has been built for creating and massaging the test set. And this set is a growing collection thanks to the fleet running along the street and encounters different problems daily. . Just reporting the loss function or the mean average precision on the test set is not enough for Tesla. Below is an example of test set for stop sign detector which is actually broken down into all these different issues like heavy rain/snow, heavily occluded, tilted stop signs and each of them is tracked separately and is pursued one-by-one with data engine to actually make them work. . . Modeling: Bird&#39;s Eye View networks . In this part, Andrej talked about how the neural network has to change in order to actually support full self-driving. . The self driving system can not just work with raw predictions from 2d pixel space, it is needed to project them out to some kind of top-down view. A traditional approach is to create occupancy tracker where 2D images are projected into 3D and stitched up across cameras and then across time. This tracker will keep the temporal context and create a small local map which helps the car winds its way thru the parking lot for example(see top left image below). However, there are a lot of problems doing the stitching because these cameras are pointing in arbitrary directions and it is very hard to align them across cameras. Very difficult to develop. . . So Tesla&#39;s AI team has decided to move from occupancy tracker (software 1.0 approach) to BEV Net (software 2.0 approach) and they see typically that works really well. . Briefly remind about software 1.0 and software 2.0 that Andrej has many times mentioned in his previous talks. Software 1.0 is the traditional way of coding where developers directly write the code to create the software. Software 2.0 is the neural network-based approach where developers indirectly write the code. The optimization algorithm is the one compile data into code. Software 2.0 is extremely useful when the problem is so messy and super hard to code. The trend is moving toward software 2.0 but each of them has their own pros and cons. The best way is still to combine them in a clever way. . As shown in the image above, images from cameras are fed to backbone and then going to a Fusion layer that stitches up the feature maps accross different view and also does the projection from image space to bird eye view. And then you have a Temporal module to actually smooth out these predictions. The BEV Net decoder actually create top-down space. That is how we can directly predict objects in top-down view from camera images. . Most of the things that Andrej has described so far reply primarily on supervise learning where Tesla AI team has spent efforts to create and massage massive dataset. However, he have seen a lot of progress today in self-supervise learning so he wants to leverage some of that to speed up the process and the rate at which they learn from very little supervised data. So let&#39;s wait for Andrej&#39;s next talk with self-supervised models deployed in Tesla autopilot system. .",
            "url": "https://phucnsp.github.io/blog/self-taught/2020/04/30/tesla-nn-in-production.html",
            "relUrl": "/self-taught/2020/04/30/tesla-nn-in-production.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Pytorch part 2 - neural net from scratch",
            "content": "In part 1 of this serie, we have gone through the basic elements of neural network. In this part we will start writing a program. . Computer programs in general consist of two primary components, code and data. With traditional programming, the programmer’s job is to explicitly write the software or code to perform computations. But with deep learning and neural networks, this job explicitly belongs to the optimization algorithm. It will compile our data into code which is actually neural net&#39;s weights. The programmer’s job is to oversee and guide the learning process though training. We can think of this as an indirect way of writing software or code. . In reality, creating and trainning model is just one of the stages in a fullscale machine learning project. There are 4 fundamental stages that a machine learning project need to have: . Project planning and project setup: gather team, define requirements, goals and allocate resources. | Data collection and labelling: define which data to collect and label them. | Training and debugging: start implementing, debugging and improving model. This notebook will focus on this stage where we will create and train a simple model using Pytorch framework. | Deploying and testing write tests to prevent regresison, roll out in production. . Important: it is worth to note that machine learning project does not fit well with either waterfall or agile workflow. It is somewhere in between them and the world is in progress to figure out a new workflow for it. However, it can be sure that this type of project needs to be highly iteractive and flexible. We will try to cover this topic more detail in later parts of this serie. | So now we will go to the main topic today. There are 3 steps that we need to iteractively tackle during the training and debugging stage: . data and data processing: data augmentation, data transformation, data cleaning, etc | create and train model guide the optimization algorithm toward the right direction. | debug and improve: analyse model&#39;s results to see where might need to improve. | . Debugging machine learning is always a hot topic and hard to digest, we will cover it in a separate notebook. Today we will talk about data and model, but not too fast. In order to fully understand exactly what and how things are doing, we will create and train a very basic neural network from scratch which initially only use the most basic Pytorch tensor functionality and gradually refactor it using Pytorch built-in modules. . Remind the most fundamental Pytorch modules which we will repeatly work with along the way. . Package Description &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; . torch | The top-level PyTorch package and tensor library. | . torch.nn | A subpackage that contains modules and extensible classes for building neural networks. | . torch.autograd | A subpackage that supports all the differentiable Tensor operations in PyTorch. | . torch.nn.functional | A functional interface that contains operations used for building neural net like loss, activation, layer operations... | . torch.optim | A subpackage that contains standard optimization operations like SGD and Adam. | . torch.utils | A subpackage that contains utility classes like data sets and data loaders that make data preprocessing easier. | . torchvision | A package that provides access to popular datasets, models, and image transformations for computer vision. | . Section 1: data and data processing . Data is the primary ingredient of deep learning. Before feeding data into our network, we need to consider many aspects such as: . Who created the dataset? | How was the dataset created? | What transformations were used? | What intent does the dataset have? | Possible unintentional consequences? | Is the dataset biased? | Are there ethical issues with the dataset? | . In this tutorial, we will use the well-prepared Fashion-MNIST dataset which was created by research lab of Zalando - a German based multi-national fashion commerce company. The dataset was designed to mirror the original MNIST dataset as closely as possible while introducing higher difficulty in training due to simply having more complex data than hand written images. The abstract from its paper: . We present Fashion-MNIST, a new dataset comprising of 28 × 28 grayscale images of 70, 000 fashion products from 10 categories, with 7, 000 images per category. The training set has 60, 000 images and the test set has 10, 000 images. Fashion-MNIST is intended to serve as a direct dropin replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist. . The Fashion-MNIST was built, unlike the hand-drawn MNIST dataset, from actual images on Zalando’s website. However, they have been transformed to more closely correspond to the MNIST specifications. This is the general conversion process that each image from the site went through: . Converted to PNG | Trimmed | Resized | Sharpened | Extended | Negated | Gray-scaled | . The dataset has the following ten classes of fashion items: . idx2clas = {0 : &quot;T-shirt/top&quot;, 1 : &quot;Trouser&quot;, 2 : &quot;Pullover&quot;, 3 : &quot;Dress&quot;, 4 : &quot;Coat&quot;, 5 : &quot;Sandal&quot;, 6 : &quot;Shirt&quot;, 7 : &quot;Sneaker&quot;, 8 : &quot;Bag&quot;, 9 : &quot;Ankle boot&quot;} . A sample of the items look like this: . . That&#39;s enough background information about the dataset. Now we will prepare data for our network. . The general idea of this step is to transform our dataset into tensor format so we can take advantages of GPU&#39;s parallel computing for later steps such as data augmentation, training model, etc. . We&#39;ll follow the ETL process to prepare data: . Extract: Get the Fashion-MNIST image data from the source. | Transform: Put our data into tensor form. | Load: Put our data into an object to make it easily accessible. | . The Fashion-MNIST source code can be accessed here. We will use pathlib for dealing with paths and will download 4 parts - training set images, training set labels, test set images and test set labels - using requests library. Since this dataset has been stored using pickle, a python-specific format for serializing data, we need to unzip and deserialize it in order to read the content. . # a. extract data from source from pathlib import Path import requests PATH_ROOT = Path(&quot;data/fashion_mnist&quot;) PATH_ROOT.mkdir(parents=True, exist_ok=True) URLs = [ &quot;http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz&quot;, &quot;http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz&quot;, &quot;http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz&quot;, &quot;http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz&quot; ] def download_data(path_root, url): filename = Path(url).name if not (path_root / filename).exists(): content = requests.get(url).content (path_root / filename).open(&quot;wb&quot;).write(content) for url in URLs: download_data(PATH_ROOT, url) . def load_data(path, kind=&#39;train&#39;): import os import gzip import numpy as np &quot;&quot;&quot;Load MNIST data from `path`&quot;&quot;&quot; labels_path = os.path.join(path, &#39;%s-labels-idx1-ubyte.gz&#39; % kind) images_path = os.path.join(path, &#39;%s-images-idx3-ubyte.gz&#39; % kind) with gzip.open(labels_path, &#39;rb&#39;) as lbpath: labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8) with gzip.open(images_path, &#39;rb&#39;) as imgpath: images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 784) return images, labels x_train, y_train = load_data(PATH_ROOT, kind=&#39;train&#39;) x_valid, y_valid = load_data(PATH_ROOT, kind=&#39;t10k&#39;) . . Note: I am using the word valid and test interchanged but in reality they are different. . This dataset is in numpy array format. Each image is 28*28 and is being stored as a flattened row of length 784. Let&#39;s reshape and take a look at one. . x_train.shape, y_train.shape . ((60000, 784), (60000,)) . y_train.max(), y_train.min() . (9, 0) . x_valid.shape, y_valid.shape . ((10000, 784), (10000,)) . y_valid.max(), y_valid.min() . (9, 0) . from matplotlib import pyplot as plt import numpy as np plt.imshow(x_train[0].reshape(28,28), cmap=&quot;gray&quot;) idx2clas[y_train[0].item()] . &#39;Ankle boot&#39; . So we have finished the extract data step and now we will to to transform step. In the context of image, there are 2 basic transform steps are convert to tensor and normalization. Normalization is a standard step in image processing which helps faster convergence. . import torch # transform 1. convert to tensor x_train, x_valid = map(lambda p: torch.tensor(p, dtype=torch.float32), (x_train, x_valid)) y_train, y_valid = map(lambda p: torch.tensor(p, dtype=torch.int64), (y_train, y_valid)) . # transform 2. normalize images mean = x_train.mean() std = x_train.std() print(f&quot;mean: {mean}, std: {std}&quot;) x_train, x_valid = map(lambda p: (p - mean)/std, (x_train, x_valid)) . mean: 72.9342041015625, std: 90.02118682861328 . x_train.mean(), x_train.std() . (tensor(-4.0474e-07), tensor(1.0000)) . x_valid.mean(), x_valid.std() . (tensor(0.0023), tensor(0.9984)) . . Important: we need to set dtype=torch.float32 in order to be able to make matrix multiplication later on with linear layer&#8217;s weight matrix. Two tensor have to have the same datatype and device so as to do operations. And finally, load data into an object to make it easily accessible. This task normally is handled by Pytorch DataLoader object but since we are building everything from scratch, let&#39;s use the traditional for loop to access batches of data. . # c. load batches of data batch_size = 64 nr_iters = len(x_train) // batch_size for i in range((nr_iters + 1)): start_index = i * batch_size end_index = start_index + batch_size xs = x_train[start_index:end_index] ys = y_train[start_index:end_index] print(xs.shape) print(ys.shape) . Section 2: create and train model (from scratch) . For our 10-class classification problem, we will create a simple network which contains only a linear layer and a non-linear layer - softmax. . In general, a layer contains 2 parts: . data: represents the state of that layer. In particular, they are weight and bias - learnable parameters which are updated/learned during training process. | transformation: the operation which transform layer&#39;s input to output using learnable parameters. | . Linear layers&#39;s data is weight matrix tensor and bias tensor while its transformation is the matrix multiplication. Weight matrix defines the linear function that maps a 1-dimentional tensor with 784 elements to a 1-dimensional tensor with 10 elements. . Note: Briefly remind the mathematical function of linear layer. Given $A$, $x$, $b$, $y$ are Weight matrix tensor, Input tensor, Bias tensor and Output tensor, respectively. Mathematical notation of a linear transformation is: $y=Ax+b$ . The weight matrix tensor will be initialized following the recommendation from Xavier initialisation paper. This paper tackled the problem with randomly initialized weight drawn from Gaussian distribution which caused hard convergence for deep network. . Tip: we set requires_grad_ after initialization, since we don&#8217;t want that step included in the gradident. The trailing _ in Pytorch signifies that the operation is performed in-place. . import math weights = torch.randn(784,10) / math.sqrt(784) weights.requires_grad_() bias = torch.zeros(10, requires_grad=True) . Thanks to Pytorch&#39;s ability to calculate gradients automatically, we can use any standard Python function (or callable object) as a model. The log_softmax function is implemented using log-sum-exp trick for numerically stable. We will not go to detail this trick but you can go here or here for details explanation. The formular for this trick is: $$ log _softmax(x) = x - logsumexp(x) $$ . def log_softmax(x): return x - x.exp().sum(-1).log().unsqueeze(-1) def simplenet(x): return log_softmax(x @ weights + bias) . . Tip: @ stand for dot product operation. Now we have had a simple network and data setup. Let&#39;s try to predict a batch. . bs = 16 xs, ys = x_train[:bs], y_train[:bs] preds = simplenet(xs) preds.shape . torch.Size([16, 10]) . preds . tensor([[-1.4405, -3.5105, -2.7565, -1.4291, -2.4875, -2.0924, -4.0225, -4.0119, -2.5381, -2.2188], [-4.8259, -3.7734, -2.0315, -2.3298, -0.9287, -2.6093, -3.6926, -2.2518, -1.9918, -5.2320], [-2.6215, -2.3225, -1.9596, -1.8593, -2.6342, -2.3268, -1.6219, -2.6988, -2.7874, -3.3023], [-3.2222, -3.1183, -1.6531, -1.5975, -2.2923, -2.7484, -1.7113, -2.3899, -2.6972, -4.0560], [-4.0472, -4.3383, -2.3563, -1.5838, -0.7511, -2.6288, -3.0189, -3.5576, -3.2544, -4.6537], [-2.2369, -3.2984, -0.9843, -2.8722, -2.3741, -1.8116, -4.1416, -5.1253, -2.9522, -2.3493], [-1.8748, -4.8596, -2.7152, -1.2111, -3.2741, -2.0744, -2.4669, -2.8398, -2.7450, -2.2655], [-2.7887, -4.9238, -1.3429, -3.5693, -2.6362, -1.4553, -5.4066, -6.6005, -3.2211, -1.2336], [-3.9236, -2.5255, -2.9410, -0.6889, -4.0574, -3.9685, -2.6249, -1.6157, -4.2080, -3.7726], [-1.7502, -1.8132, -1.7902, -1.9453, -3.3830, -3.7103, -2.6871, -3.1987, -2.2021, -2.5852], [-3.7683, -3.3224, -2.6373, -2.0115, -0.7844, -2.8311, -2.6774, -2.6067, -2.6543, -4.9239], [-3.2636, -3.2367, -1.4613, -2.2395, -2.4437, -2.0885, -2.9661, -3.5086, -1.3777, -3.2262], [-1.4874, -4.1664, -2.1750, -1.5891, -3.1738, -2.5524, -2.2160, -3.7046, -2.8008, -2.0662], [-2.6747, -3.6151, -2.5766, -1.4708, -3.7484, -3.1546, -1.2979, -3.1983, -2.5499, -1.9650], [-2.2535, -4.0095, -3.1679, -0.7156, -3.5641, -2.4768, -2.4170, -3.0718, -3.2315, -2.8394], [-1.1524, -4.3791, -3.3212, -1.0543, -3.4922, -2.1228, -4.1253, -3.9749, -2.9178, -3.0380]], grad_fn=&lt;SubBackward0&gt;) . What we have done is one forward pass, we load a batch of image add feed it through the network. The result will not be better than a random prediction at this stage because we start with random weights. . It can be seen in the preds tensor that it contains not only the tensor values but also a gradient function. As mentioned in part 1, pytorch use dynamic computational graph to track function operations that occur on tensors. These graph are then used to compute the derivatives. . preds.grad_fn . &lt;SubBackward0 at 0x12bde0310&gt; . Now we need to define the loss function which is the model&#39;s objective. Our weights and bias will be updated in the direction which make this loss decreased. One of the most common loss function is negative log-likelihood. . def nll(input, target): return -input[range(input.shape[0]), target.tolist()].mean() loss_function = nll loss_function(preds, ys) . tensor(2.9355, grad_fn=&lt;NegBackward&gt;) . Next, We will define a metric. During the training, reducing the loss is what our model tries to do but it is hard for us, as human, can intuitively understand how good the weights set are along the way. So we need a human-interpretable value which help us understand the training progress and it is the metric. . Important: while training your model, there will be the case when your loss has stopped decreasing but your accuracy is still increasing. The recommendation here is to save both models, one at minimum loss and one at maximum accuracy. And you, yourself, need to make decision which one to choose. For me, it is always the maximum accuracy because the accuracy measurement is what we care about. . def accuracy(input, target): return (torch.argmax(input, dim=1) == target).float().mean() accuracy(preds, ys) . tensor(0.1250) . We are now ready to begin the training process. The training process is an iterative process which including following steps: . Get a batch from the training set. Since we have 60,000 samples in our training set, we will have 938 iterations with batch_size 64. Something to notice, batch_size will directly impact to the number of times the weights updated. In our case, the weights will be updated 938 times by the end of each loop. So far, there is no rule-of-thump for selecting the value of batch size so we still need to do trial and error to figure out the best value. . Important: to be simple, I am not shuffling the training set at this stage. In reality, the training set should be shuffled to prevent correlation between batches and overfitting. If we keep feeding the network batch-by-batch in an exact order many times, the network might remember this order and causes overfitting with it. On the other hand, the validation loss will be identical whether we shuffle the validation set or not. Since shuffling takes extra time, it makes no sense to shuffle the validation data. | . | Pass batch to network. . | Calculate the loss value. . | Calculate the gradient of the loss function w.r.t the network&#39;s weights. . Calculating the gradients is very easy using PyTorch. Since PyTorch has created a computation graph under the hood. As our batch tensor steps forward through our network, all the computations are recorded in the computational graph. And this graph is then used by PyTorch to calculate the gradients of the loss function with respect to the network&#39;s weights. | . | Update the weights. . The gradients calculated from step 4 are used by the optimizer to update the respective weights. | We have disabled PyTorch gradient tracking at this step because we don&#39;t want these actions to be recorded for our next calculation of the gradient. There are many ways to disable this functionality, please check Random topics at the end of notebook for more information. | After updating the weight, we need to zero out the gradients because the gradients will be calculated and added to the grad attributes of our network&#39;s parameters after calling loss.backward() at the next iteration. . Note: Zero out the gradient after updating parameters is not always the case, there are some special cases where we want to accumulate gradient. But you only have to deal with it at advance level. So take care^^. | . | Repeat steps 1-5 until one epoch is completed. . | Calculate mean loss of validation set . Note: We can use a batch size for the validation set that is twice as large as that for the training set. This is because the validation set does not need backpropagation and thus takes less memory (it doesn’t need to store the gradients). We take advantage of this to use a larger batch size and compute the loss more quickly. . | Repeat steps 1-6 for as many epochs required to reach the minimum loss. | We will use Stochastic Gradient Descent (SGD) optimizer to update our learnable parameters during training. lr tells the optimizer how far to step in the direction of minimizing loss function. . lr = 0.01 epochs = 5 batch_size = 64 nr_iters = len(x_train) // bs for epoch in range(epochs): for i in range((nr_iters + 1)): # step 1. get batch of training set start_index = i * batch_size end_index = start_index + batch_size xs = x_train[start_index:end_index] ys = y_train[start_index:end_index] # step 2. pass batch to network preds = simplenet(xs) # step 3. calculate the loss loss = loss_function(preds, ys) # step 4. calculate the gradient of the loss w.r.t the network&#39;s parameters loss.backward() with torch.no_grad(): # step 5. update the weights using SGD algorithm weights -= lr * weights.grad bias -= lr * bias.grad weights.grad.zero_() bias.grad.zero_() # step 6. calculate mean of valid loss after each epoch to see the improvement batch_size_valid = batch_size * 2 nr_iters_valid = len(x_valid) // batch_size_valid total_loss = 0 total_acc = 0 with torch.no_grad(): for i in range((nr_iters_valid + 1)): start_index = i * batch_size_valid end_index = start_index + batch_size_valid xs = x_valid[start_index:end_index] ys = y_valid[start_index:end_index] preds = simplenet(xs) loss = loss_function(preds, ys) total_loss += loss.item() * xs.shape[0] acc = accuracy(preds, ys) total_acc += acc.item() * xs.shape[0] print(f&quot;epoch {epoch}, valid_loss {total_loss / len(x_valid)}, accuracy {total_acc / len(x_valid)}&quot;) . epoch 0, valid_loss 0.5325431520462036, accuracy 0.8139 epoch 1, valid_loss 0.49808417506217956, accuracy 0.8252 epoch 2, valid_loss 0.48265715327262876, accuracy 0.8305 epoch 3, valid_loss 0.4735100971221924, accuracy 0.8335 epoch 4, valid_loss 0.46733067717552185, accuracy 0.8356 . During the first training epoches, the valid loss should decrease and the accuracy should increase. Otherwise, you did something wrong. . Section 3: refactor model using Pytorch built-in modules . We will gradually refactor our simplenet with Pytorch built-in modules, so that it does the same thing as before but start taking advantage of Pytorch&#39;s modules to make it more concise, more understandable and/or flexible. To make things more gradual and more understandable, the refactoring will be divided into 3 stages. . Refactor stage 1 . Refactor loss fuction with torch.nn.functional.cross_entropy function. | Refactor model with nn.Module, nn.Parameter class. | Refactor optimization algorithm with model.parameters and model.zero_grad method. | . We first will refactor the log_softmax and nll method with Pytorch built-in function torch.nn.functional.cross_entropy that combines the two. So we can even remove the activation function from our model. . import torch.nn.functional as F # old code # def log_softmax(x): return x - x.exp().sum(-1).log().unsqueeze(-1) # def simplenet(x): return log_softmax(x @ weights + bias) # refactor code loss_function = F.cross_entropy def simplenet(x): return x @ weights + bias . Next we will refactor our simplenet using torch.nn module. . torch.nn is PyTorch’s neural network (nn) library which contains the primary components to construct network&#39;s layers. Within the torch.nn package, there is a class called Module, and it is the base class for all of neural network modules, including layers. All of the layers in PyTorch need to extend this base class in order to inherit all of PyTorch’s built-in functionality within the nn.Module class. . Note: nn.Module (uppercase M) is a Pytorch specific concept, and is a class we&#8217;ll be using a lot. Do not confuse with the Python concept of a (lowercase m) module, which is a file of Python code that can be imported. . In order to create model using nn.Module, we have 3 essential steps: . Create a neural network class that extends the nn.Module base class. | Define the network&#39;s layers as class attributes in __init__ method. The layers&#39;s learnable parameters are initialized in this step. But they need to be wrapped in nn.Parameters class in order to help nn.Module know those are learnable parameter. The weight tensor inside every layer is an instance of this Parameter class. PyTorch’s nn.Module class is basically looking for any attributes whose values are instances of the Parameter class, and when it finds an instance of the parameter class, it keeps track of it. Take a look at Random topics section for more detail information about network parameters. | . | Define the network&#39;s transformation (operation) in forward method. Every Pytorch nn.Module has a forward() method and so when we are building layers and networks, we must provide an implementation of the forward() method. The forward method is the actual transformation. | The tensor input is passed forward though each layer transformation until the tensor reaches the output layer. The composition of all the individual layer forward passes defines the overall forward pass transformation for the network. The goal of the overall transformation is to transform or map the input to the correct prediction output class, and during the training process, the layer weights (data) are updated in such a way that cause the mapping to adjust to make the output closer to the correct prediction. | When we implement the forward() method of our nn.Module subclass, we will typically use layers&#39;attributes and functions from the nn.functional package. This package provides us with many neural network operations that we can use for building layers. | . | import math import torch.nn as nn # old code # weights = torch.randn(784,10) / math.sqrt(784) # weights.requires_grad_() # bias = torch.zeros(10, requires_grad=True) # def simplenet(x): return log_softmax(x @ weights + bias) # refactor code class SimpleNet(nn.Module): def __init__(self): super().__init__() self.weights = nn.Parameter(torch.randn(784,10) / math.sqrt(784)) self.bias = nn.Parameter(torch.zeros(10)) def forward(self, x): return x @ self.weights + self.bias simplenet = SimpleNet() . One thing to point out that Pytorch neural network modules are callable Python objects. It means we can call the SimplenNet&#39;s object as it was a function. What makes this possible is that PyTorch module classes implement a special Python function called __call__(). which will be invoked anytime the object instance is called. After the object instance is called, the __call__() method is invoked under the hood, and the __call__() in turn invokes the forward() method. Instead of calling the forward() method directly, we call the object instance. This applies to all PyTorch neural network modules, namely, networks and layers. . In order to access the model parameters, we can use parameters() or named_parameters() method. Next we will refactor optimization algorithm using nn.Module.parameters and nn.Module.zero_grad method. . lr = 0.01 epochs = 5 batch_size = 64 nr_iters = len(x_train) // bs for epoch in range(epochs): for i in range((nr_iters + 1)): # step 1. get batch of training set start_index = i * batch_size end_index = start_index + batch_size xs = x_train[start_index:end_index] ys = y_train[start_index:end_index] # step 2. pass batch to network preds = simplenet(xs) # step 3. calculate the loss loss = loss_function(preds, ys) # step 4. calculate the gradient of the loss w.r.t the network&#39;s parameters loss.backward() with torch.no_grad(): # step 5. update the weights using SGD algorithm # old code # weights -= lr * weights.grad # bias -= lr * bias.grad # weights.grad.zero_() # bias.grad.zero_() # refactor code for p in simplenet.parameters(): p -= lr * p.grad simplenet.zero_grad() # step 6. calculate mean of valid loss after each epoch to see the improvement batch_size_valid = batch_size * 2 nr_iters_valid = len(x_valid) // batch_size_valid total_loss = 0 total_acc = 0 with torch.no_grad(): for i in range((nr_iters_valid + 1)): start_index = i * batch_size_valid end_index = start_index + batch_size_valid xs = x_valid[start_index:end_index] ys = y_valid[start_index:end_index] preds = simplenet(xs) loss = loss_function(preds, ys) total_loss += loss.item() * xs.shape[0] acc = accuracy(preds, ys) total_acc += acc.item() * xs.shape[0] print(f&quot;epoch {epoch}, valid_loss {total_loss / len(x_valid)}, accuracy {total_acc / len(x_valid)}&quot;) . epoch 0, valid_loss 0.5341379848957062, accuracy 0.8131 epoch 1, valid_loss 0.49990872111320495, accuracy 0.8275 epoch 2, valid_loss 0.48446780610084533, accuracy 0.8317 epoch 3, valid_loss 0.47528300595283507, accuracy 0.8343 epoch 4, valid_loss 0.4690569020748138, accuracy 0.8355 . Ok so we have finished refactor stage 1. Let&#39;s move to refactor stage 2. . Refactor stage 2 . Refactor model with nn.Linear class. | Refactor data setup with torch.utils.data.TensorDataset, torch.utils.data.DataLoader. | Refactor optimization algorithm with torch.optim module. | . Pytorch nn.Linear class does all the things that we have done for linear layer, including intialize learnable parameters and define linear operation. . Note: We used the abbreviation fc below because linear layers are also called fully connected layers or dense layer. Thus, linear = dense = fully connected. . class SimpleNet(nn.Module): def __init__(self): super().__init__() self.fc = nn.Linear(784, 10) def forward(self, x): return self.fc(x) simplenet = SimpleNet() simplenet . SimpleNet( (fc): Linear(in_features=784, out_features=10, bias=True) ) . Next is Dataset and DataLoader. . torch.utils.data.Dataset is an abstract class for representing a dataset. An abstract class is a Python class that has methods we must implement, in our case are __getitem__ and __len__. In order to create a custom dataset, we need to subclass the Dataset class and override __len__, that provides the size of the dataset, and __getitem__, supporting integer indexing in range from 0 to len(self) exclusive. Upon doing this, our new subclass can then be passed to the a PyTorch DataLoader object. . PyTorch’s TensorDataset is a Dataset wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. This will make it easier to access both the independent and dependent variables in the same line as we train. . torch.utils.data.DataLoader is responsible for managing batches. It makes life easier to iterate over batches. We can create a DataLoader from any Dataset. . from torch.utils.data import TensorDataset from torch.utils.data import DataLoader batch_size = 64 train_ds = TensorDataset(x_train, y_train) valid_ds = TensorDataset(x_valid, y_valid) train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True) valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2, shuffle=False) . The code in training loop is now changed from . for i in range((nr_iters + 1)): # step 1. get batch of training set start_index = i * batch_size end_index = start_index + batch_size xs = x_train[start_index:end_index] ys = y_train[start_index:end_index] # step 2. pass batch to network preds = simplenet(xs) . to . for xs,ys in train_dl: preds = simplenet(xs) . And torch.optim package. This module provides various optimization algorithms. Its API provides step and zero_grad method for weight updating and zero out gradient which will help us refactor our code further. . Here is the training loop after applying all of those above steps. . from torch import optim lr = 0.01 epochs = 5 simplenet = SimpleNet() opt = optim.SGD(simplenet.parameters(), lr=lr) for epoch in range(epochs): simplenet.train() for xs,ys in train_dl: preds = simplenet(xs) loss = loss_function(preds, ys) loss.backward() opt.step() opt.zero_grad() simplenet.eval() with torch.no_grad(): total_loss = sum(loss_function(simplenet(xs),ys)*len(xs) for xs, ys in valid_dl) total_acc = sum(accuracy(simplenet(xs),ys)*len(xs) for xs, ys in valid_dl) print(f&quot;epoch {epoch}, valid_loss {total_loss / len(x_valid)}, accuracy {total_acc / len(x_valid)}&quot;) . epoch 0, valid_loss 0.5226423740386963, accuracy 0.8163999915122986 epoch 1, valid_loss 0.49401089549064636, accuracy 0.82669997215271 epoch 2, valid_loss 0.48364755511283875, accuracy 0.8306000232696533 epoch 3, valid_loss 0.4771580994129181, accuracy 0.8323000073432922 epoch 4, valid_loss 0.4616956412792206, accuracy 0.8371000289916992 . . Note: we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases. . Done! We have finished refactor stage 2 thanks to Pytorch built-in modules. Our training loop is now dramatically smaller and easier to understand. The refactor stage 3 does not introduce any new Pytorch modules. It is only an bonus step which help the code a bit cleaner and less code. . Refactor stage 3 . get_data function returns dataloaders for the training set and validation set. . def get_data(train_ds, valid_ds, bs): return ( DataLoader(train_ds, batch_size=bs, shuffle=True), DataLoader(valid_ds, batch_size=bs, shuffle=False) ) . get_model function returns instance of our model and the optimizer. . def get_model(model, lr): m = model() opt = optim.SGD(m.parameters(), lr=lr) return m, opt . calc_loss_batch function returns loss value of batch and number of samples in that batch. We create this function because we go through this process twice, calculating the loss for both the training set and the validation set. We pass an optimizer in for the training set, and use it to perform backprop. For the validation set, we don’t pass an optimizer, so the method doesn’t perform backprop. As a bonus, the accuracy is calculated if it is not None. . def calc_loss_batch(model, loss_func, xs, ys, opt=None, metric=None): loss = loss_func(model(xs), ys) if opt is not None: loss.backward() opt.step() opt.zero_grad() if metric is not None: acc = metric(model(xs), ys) return loss.item(), acc.item(), len(xs) else: return loss.item(), len(xs) . fit function runs the necessary operations to train our model and compute the training loss, as well as validation losses and validation accuracy at each epoch. . import numpy as np def fit(epochs, model, loss_func, metric, opt, train_dl, valid_dl): for epoch in range(epochs): model.train() for xs, ys in train_dl: loss_batch(model, loss_func, xs, ys, opt) model.eval() with torch.no_grad(): losses, accs, nums = zip(*[loss_batch(model, loss_func, xs, ys, metric=metric) for xs,ys in valid_dl]) total_loss = np.sum(np.multiply(losses, nums)) total_acc = np.sum(np.multiply(accs, nums)) print(f&quot;epoch {epoch}, valid_loss {total_loss / np.sum(nums)}, accuracy {total_acc / np.sum(nums)}&quot;) . bs = 64 lr = 0.01 epochs = 5 train_dl, valid_dl = get_data(train_ds, valid_ds, bs) model, opt = get_model(model=SimpleNet, lr=lr) fit(epochs, model, loss_function, accuracy, opt, train_dl, valid_dl) . epoch 0, valid_loss 0.5245783556461334, accuracy 0.8162 epoch 1, valid_loss 0.49596426906585694, accuracy 0.8261 epoch 2, valid_loss 0.4808829068660736, accuracy 0.8334 epoch 3, valid_loss 0.47098530049324033, accuracy 0.8347 epoch 4, valid_loss 0.4786785946369171, accuracy 0.831 . Done! So we have gone through all 3 refactor stages and now we have a clean and flexible function for getting data, create and training model. In part 3 of this serie, we will use those functions to train a Convolutional Neural Network (CNN). . Section 4: random topics . Disabling PyTorch Gradient Tracking . Get predictions for the entire training set Note at the top, we have annotated the function using the @torch.no_grad() PyTorch decoration. This is because we want this functions execution to omit gradient tracking. This is because gradient tracking uses memory, and during inference (getting predictions while not training) there is no need to keep track of the computational graph. The decoration is one way of locally turning off the gradient tracking feature while executing specific functions. We specifically need the gradient calculation feature anytime we are going to calculate gradients using the backward() function. Otherwise, it is a good idea to turn it off because having it off will reduce memory consumption for computations, e.g. when we are using networks for predicting (inference). . As another example, we can use Python&#39;s with context manger keyword to specify that a specify block of code should exclude gradient computations. Both of these options are valid . @torch.no_grad() def get_all_preds(model, loader): all_preds = torch.tensor([]) for batch in loader: imgs, lbs = batch preds = model(imgs) all_preds = torch.cat((all_preds, preds), dim=0) return all_preds # Locally Disabling PyTorch Gradient Tracking with torch.no_grad(): prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000) train_preds = get_all_preds(model, prediction_loader) . For more information, please check here . Reference . Some good sources: . pytorch zero to all | deeplizard | effective pytorch | what is torch.nn really? | recommend walk with pytorch | official tutorial | DL(with Pytorch) | Pytorch project template | nlp turorial with pytorch | UDACITY course | awesome pytorch list | deep learning with pytorch | others: https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc | Grokking Algorithms: An illustrated guide for programmers and other curious people 1st Edition | . | .",
            "url": "https://phucnsp.github.io/blog/self-taught/2020/03/22/self-taught-pytorch-part2-nn-from-scratch.html",
            "relUrl": "/self-taught/2020/03/22/self-taught-pytorch-part2-nn-from-scratch.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Pytorch part 1 - tensor and Pytorch tensor",
            "content": "Section 1: Introducing Pytorch, CUDA and GPU . PyTorch is a deep learning framework and a scientific computing package. The scientific computing aspect of PyTorch is primarily a result PyTorch’s tensor library and associated tensor operations. That means you can take advantage of Pytorch for many computing tasks, thanks to its supporting tensor operation, without touching deep learning modules. . Important to note that PyTorch tensors and their associated operations are very similar to numpy n-dimensional arrays. A tensor is actually an n-dimensional array. . Pytorch build its library around Object Oriented Programming(OOP) concept. With object oriented programming, we orient our program design and structure around objects (take a look at Random topics for more information). The tensor in Pytorch is presented by the object torch.Tensor which is created from numpy ndarray objects. Two objects share memory. This makes the transition between PyTorch and NumPy very cheap from a performance perspective. . With PyTorch tensors, GPU support is built-in. It’s very easy with PyTorch to move tensors to and from a GPU if we have one installed on our system. Tensors are super important for deep learning and neural networks because they are the data structure that we ultimately use for building and training our neural networks. . Talking a bit about history. The initial release of PyTorch was in October of 2016, and before PyTorch was created, there was and still is, another framework called Torch which is also a machine learning framework but is based on the Lua programming language. The connection between PyTorch and this Lua version, called Torch, exists because many of the developers who maintain the Lua version are the individuals who created PyTorch. And they have been working for Facebook since then till now. . Note: Facebook Created PyTorch . Below are the primary PyTorch modules we’ll be learning about and using as we build neural networks along the way. . Package Description &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; . torch | The top-level PyTorch package and tensor library. | . torch.nn | A subpackage that contains modules and extensible classes for building neural networks. | . torch.autograd | A subpackage that supports all the differentiable Tensor operations in PyTorch. | . torch.nn.functional | A functional interface that contains operations used for building neural net like loss, activation, layer operations... | . torch.optim | A subpackage that contains standard optimization operations like SGD and Adam. | . torch.utils | A subpackage that contains utility classes like data sets and data loaders that make data preprocessing easier. | . torchvision | A package that provides access to popular datasets, models, and image transformations for computer vision. | . Why use PyTorch for deep learning? . PyTorch’s design is modern, Pythonic. When we build neural networks with PyTorch, we are super close to programming neural networks from scratch. When we write PyTorch code, we are just writing and extending standard Python classes, and when we debug PyTorch code, we are using the standard Python debugger. It’s written mostly in Python, and only drops into C++ and CUDA code for operations that are performance bottlenecks. | It is a thin framework, which makes it more likely that PyTorch will be capable of adapting to the rapidly evolving deep learning environment as things change quickly over time. | Stays out of the way and this makes it so that we can focus on neural networks and less on the actual framework. | . Why PyTorch is great for deep learning research . The reason for this research suitability is that Pytorch use dynamic computational graph, in contrast with tensorfow which uses static computational graph, in order to calculate derivatives. . Computational graphs are used to graph the function operations that occur on tensors inside neural networks. These graphs are then used to compute the derivatives needed to optimize the neural network. Dynamic computational graph means that the graph is generated on the fly as the operations are created. Static graphs that are fully determined before the actual operations occur. . It just so happens that many of the cutting edge research topics in deep learning are requiring or benefiting greatly from dynamic graphs. . Installing PyTorch . The recommended best option is to use the Anaconda Python package manager. With Anaconda, it&#39;s easy to get and manage Python, Jupyter Notebook, and other commonly used packages for scientific computing and data science, like PyTorch! . Let’s go over the steps: . Download and install Anaconda (choose the latest Python version). | Go to PyTorch&#39;s site and find the get started locally section. | Specify the appropriate configuration options for your particular environment. | Run the presented command in the terminal to install PyTorch | . For the example: conda install pytorch torchvision cudatoolkit=10.0 -c pytorch . Notice that we are installing both PyTorch and torchvision. Also, there is no need to install CUDA separately. The needed CUDA software comes installed with PyTorch if a CUDA version is selected in step (3). All we need to do is select a version of CUDA if we have a supported Nvidia GPU on our system. . !conda list torch . # packages in environment at /Users/phucnsp/anaconda3/envs/fastai2: # # Name Version Build Channel pytorch 1.4.0 py3.7_0 pytorch torchsummary 1.5.1 pypi_0 pypi torchvision 0.5.0 py37_cpu pytorch . import torch torch.__version__ # to verify pytorch version . &#39;1.4.0&#39; . torch.cuda.is_available() # to verify our GPU capabilities . False . Why deep learning uses GPUs . To understand CUDA, we need to have a working knowledge of graphics processing units (GPUs). A GPU is a processor that is good at handling specialized computations. This is in contrast to a central processing unit (CPU), which is a processor that is good at handling general computations. CPUs are the processors that power most of the typical computations on our electronic devices. . A GPU can be much faster at computing than a CPU. However, this is not always the case. The speed of a GPU relative to a CPU depends on the type of computation being performed. The type of computation most suitable for a GPU is a computation that can be done in parallel. . Parallel computing is a type of computation where by a particular computation is broken into independent smaller computations that can be carried out simultaneously. The resulting computations are then recombined, or synchronized, to form the result of the original larger computation. The number of tasks that a larger task can be broken into depends on the number of cores contained on a particular piece of hardware. Cores are the units that actually do the computation within a given processor, and CPUs typically have four, eight, or sixteen cores while GPUs have potentially thousands. . So why deep learning uses them - Neural networks are embarrassingly parallel. Tasks that embarrassingly parallel are ones where it’s easy to see that the set of smaller tasks are independent with respect to each other. Many of the computations that we do with neural networks can be easily broken into smaller computations in such a way that the set of smaller computations do not depend on one another. One such example is a convolution. . GPU, CUDA and Nvidia . GPU computing In the beginning, the main tasks that were accelerated using GPUs were computer graphics. That&#39;s why we have the name graphics processing unit. But in recent years, many more varieties parallel tasks have emerged. One such task as we have seen is deep learning. Deep learning along with many other scientific computing tasks that use parallel programming techniques are leading to a new type of programming model called GPGPU or general purpose GPU computing. . GPU computing practically began with the introduction of CUDA by NVIDIA and Stream by AMD. These are APIs designed by the GPU vendors to be used together with the hardware that they provide. Nvidia is a technology company that designs GPUs, and they have created CUDA as a software platform that pairs with their GPU hardware making it easier for developers to build software that accelerates computations using the parallel processing power of Nvidia GPUs. . GPU computing stack concept The stack comprises of: . GPU as the hardware on the bottom | CUDA as the software architecture on top of the GPU | And finally libraries like cuDNN on top of CUDA. | . Sitting on top of CUDA and cuDNN is PyTorch, which is the framework were we’ll be working that ultimately supports applications on top. Developers use CUDA by downloading the CUDA toolkit which comes with specialized libraries like cuDNN - the CUDA Deep Neural Network library. With PyTorch, CUDA comes baked in from the start. There are no additional downloads required. All we need is to have a supported Nvidia GPU, and we can leverage CUDA using PyTorch. We don’t need to know how to use the CUDA API directly. . After all, PyTorch is written in all of these: Python, C++, CUDA . . Suppose we have the following code: . t = torch.tensor([1,2,3]) . The tensor object created in this way is on the CPU by default. As a result, any operations that we do using this tensor object will be carried out on the CPU. Now, to move the tensor onto the GPU, we just write: . t = t.cuda() . This ability makes PyTorch very flexible because computations can be selectively carried out either on the CPU or on the GPU. . . Note: GPU Can Be Slower Than CPU. The answer is that a GPU is only faster for particular (specialized) tasks. For example, moving data from the CPU to the GPU is costly, so in this case, the overall performance might be slower if the computation task is a simple one. Moving relatively small computational tasks to the GPU won’t speed us up very much and may indeed slow us down. Remember, the GPU works well for tasks that can be broken into many smaller tasks, and if a compute task is already small, we won’t have much to gain by moving the task to the GPU.For this reason, it’s often acceptable to simply use a CPU when just starting out, and as we tackle larger more complicated problems, begin using the GPU more heavily. . Section 2: Introducing Tensors . Tensors - Data Structures of Deep Learning . A tensor is the primary data structure used by neural networks. The inputs, outputs, and transformations within neural networks are all represented using tensors, and as a result, neural network programming utilizes tensors heavily. . The below concepts, that we met in math or computer science, are all refered to tensor in deep learning. . indexes required math computer science . 0 | scalar | number | . 1 | vector | array | . 2 | matrix | 2d-array | . The relationship within each of these pairs, for example vector and array, is that both elements require the same number of indexes to refer to a specific element within the data structure. . # an array or a vector requires 1 index to access its element a = [1,2,3,4] a[3] . 4 . # an matrix or 2d-array requires 2 index to access its element a = [ [1, 2, 3, 4], [5, 6, 7, 8] ] a[0][2] . 3 . When more than two indexes are required to access a specific element, we stop giving specific names to the structures, and we begin using more general language. . In mathematics, we stop using words like scalar, vector, and matrix, and we start using the word tensor or nd-tensor. The n tells us the number of indexes required to access a specific element within the structure. | In computer science, we stop using words like, number, array, 2d-array, and start using the word multidimensional array or nd-array. The n tells us the number of indexes required to access a specific element within the structure. | . The reason we say a tensor is a generalization form is because we use the word tensor for all values of n like so: . A scalar is a 0 dimensional tensor | A vector is a 1 dimensional tensor | A matrix is a 2 dimensional tensor | A nd-array is an n dimensional tensor | . . Note: Tensors and nd-arrays are the same thing! . Fundamental tensor attributes for deep learning - Rank, Axes, and Shape. . These concepts build on one another starting with rank, then axes, and building up to shape. . The rank of a tensor refers to the number of dimensions present within the tensor. A rank-2 tensor means all of the following: a matrix, a 2d-array, a 2d-tensor. . An axis of a tensor is a specific dimension of a tensor. Let&#39;s get an example how to access elements of an axis. . dd = [ [1,2,3], [4,5,6], [7,8,9] ] . Each element along the first axis, is an array: . dd[0], dd[1], dd[2] . ([1, 2, 3], [4, 5, 6], [7, 8, 9]) . Each element along the second axis, is a number: . dd[0][0], dd[1][0], dd[2][0] . (1, 4, 7) . . Note: with tensors, the elements of the last axis are always numbers. Every other axis will contain n-dimensional arrays. . The shape of a tensor gives us the length of each axis of the tensor. . Note: The shape of a tensor is important because it encodes all of the relevant information about axes, rank, and therefore indexes. Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called reshaping. . t = torch.tensor([ [1,2,3], [5,6,7] ], dtype=torch.float) t.shape . torch.Size([2, 3]) . . Note: size and shape of a tensor are the same thing. . Section 3: Pytorch Tensors . PyTorch tensors are the data structures we&#39;ll be using when programming neural networks in PyTorch. . The tensor in Pytorch is presented by the object torch.Tensor which is created from numpy ndarray objects. Two objects share memory. This makes the transition between PyTorch and NumPy very cheap from a performance perspective. . When programming neural networks, data preprocessing is often one of the first steps in the overall process, and one goal of data preprocessing is to transform the raw input data into tensor form. . torch.Tensor class and its attributes . PyTorch tensors are instances of the torch.Tensor Python class. First, let’s look at a few torch.Tensor&#39;s tensor attributes. . tensor.dtype: | tensor.device | tensor.layout | . t = torch.Tensor() . The dtype specifies the type of the data that is contained within the tensor. . t.dtype . torch.float32 . Table below shows all the tensor types that Pytorch supports. Each type has a CPU and GPU version. Tensors contain uniform (of the same type) numerical data. . . The device specifies the device (CPU or GPU) where the tensor&#39;s data is allocated. This determines where tensor computations for the given tensor will be performed. . t.device . device(type=&#39;cpu&#39;) . PyTorch supports the use of multiple devices, and they are specified using an index like so: . device = torch.device(&#39;cuda:0&#39;) device . device(type=&#39;cuda&#39;, index=0) . If we have a device like above, we can create a tensor on the device by passing the device to the tensor’s constructor. . import torch t = torch.tensor([1,2,3], dtype=torch.int, device=device) . The layout specifies how the tensor is stored in memory. . t.layout . torch.strided . To learn more about stride check here. . Create a new tensor using data . These are the primary ways of creating tensor objects (instances of the torch.Tensor class), with data (array-like) in PyTorch: . torch.Tensor(data) is the constructor of the torch.Tensor class | torch.tensor(data): is the factory function that constructs torch.Tensor objects. | torch.as_tensor(data) | torch.from_numpy(data) | Let’s look at each of these. They all accept some form of data and give us an instance of the torch.Tensor class. Sometimes when there are multiple ways to achieve the same result, things can get confusing, so let’s break this down. . import numpy as np data = np.array([1,2,3]) o1 = torch.Tensor(data) o2 = torch.tensor(data) o3 = torch.as_tensor(data) o4 = torch.from_numpy(data) print(o1) print(o2) print(o3) print(o4) . tensor([1., 2., 3.]) tensor([1, 2, 3]) tensor([1, 2, 3]) tensor([1, 2, 3]) . The table below compare 4 options and propose which one to use. . torch.tensor is best option to go daily ^^. It copy data which help us prevent hidden mistake caused by sharing data. In addition, it is better than torch.Tensor thanks to better doc and more config options. | torch.as_tensor is recommened in case we want to improve the performance and want to levarage data share memory characteristic of Pytorch. However, it is always better to start with copy data to make sure your program works first and go to performance improvement later. This option is better than torch.from_numpy because it accepts a wide variety of array-like objects including other Pytorch tensor. | . method which one to use &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; dtype &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; data in memory . torch.Tensor(data) | | infer from default dtype. | copy | . torch.tensor(data) | best option to go | inferred from input or explicitly set. | copy | . torch.as_tensor(data) | use for improve performance | inferred from input or explicitly set. | share | . torch.from_numpy(data) | | inferred from input or explicitly set. | share | . Data in memory is shared means that the actual data in memory exists in a single place. As a result, any changes that occur in the underlying data will be reflected in both objects, the torch.Tensor and the numpy.ndarray. Sharing data is more efficient and uses less memory than copying data because the data is not written to two locations in memory. However, there are something to keep in mind about memory sharing: . Since numpy.ndarray objects are allocated on the CPU, the as_tensor() function must copy the data from the CPU to the GPU when a GPU is being used. | The memory sharing of as_tensor() doesn’t work with built-in Python data structures like list. | The as_tensor() call requires developer knowledge of the sharing feature. This is necessary so we don’t inadvertently make an unwanted change in the underlying data without realizing the change impacts multiple objects. | The as_tensor() performance improvement will be greater if there are a lot of back and forth operations between numpy.ndarray objects and tensor objects. However, if there is just a single load operation, there shouldn’t be much impact from a performance perspective. | . Tips, In order to convert multiple arrays to tensor we can use map . import numpy as np import torch a = np.array([1,2,3]) b = np.array([3,4,5]) c = np.array([1]) a, b, c = map(torch.tensor, (a, b, c)) a, b, c . (tensor([1, 2, 3]), tensor([3, 4, 5]), tensor([1])) . Create a new tensor without data . Here are some other creation options that are available. . # create identity matrix torch.eye(2) . tensor([[1., 0.], [0., 1.]]) . # create a tensor of zeros with the shape of specified shape argument torch.zeros([2,2]) . tensor([[0., 0.], [0., 0.]]) . # create a tensor of ones with the shape of specified shape argument torch.ones([2,2]) . tensor([[1., 1.], [1., 1.]]) . # Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1) # The shape of the tensor is defined by the variable argument size. torch.rand([2,2]) . tensor([[0.3088, 0.4226], [0.8102, 0.9129]]) . # Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 # (also called the standard normal distribution). torch.randn(2, 3) . tensor([[-1.1778, -1.0388, -0.1459], [-0.1746, 0.5764, -1.1491]]) . This is a small subset of the available creation functions that don’t require data. Check with the PyTorch documentation for the full list. . Section 4: Pytorch Tensor Operation Types . We have the following high-level categories of tensor operations: . Reshaping operation type: gave us the ability to position our elements along particular axes. | Element-wise operation type: allow us to perform operations on elements between two tensors. | Reduction operation type: allow us to perform operations on elements within a single tensor. | Access operation type allow us to access to each numerical elements within a single tensor. | . There are a lot of individual operations out there, so much so that it can sometimes be intimidating when you&#39;re just beginning, but grouping similar operations into categories based on their likeness can help make learning about tensor operations more manageable. . Important: Tensor operations between tensors must happen between tensors with the same type of data and on the same device Before going to each category, firstly we will take a look on the concept of broadcasting . Broadcasting Tensors . To understand this concept, let&#39;s take a look at an example. Suppose we have the following tensors. . t1 = torch.tensor([ [1,1], [1,1] ], dtype=torch.float32) t2 = torch.tensor([2,4], dtype=torch.float32) t1.shape, t2.shape . (torch.Size([2, 2]), torch.Size([2])) . What will be the result of this element-wise addition operation, t1 + t2 ? . t1 + t2 . tensor([[3., 5.], [3., 5.]]) . Even though these two tenors have differing shapes, the element-wise operation is possible, and broadcasting is what makes the operation possible. The lower rank tensor t2 will be transformed via broadcasting to match the shape of the higher rank tensor t1, and the element-wise operation will be performed as usual. . we can check the broadcast transformation using the broadcast_to() numpy function. . import numpy as np np.broadcast_to(t2.numpy(), t1.shape) . array([[2., 4.], [2., 4.]], dtype=float32) . t1 + t2 . tensor([[3., 5.], [3., 5.]]) . After broadcasting, the addition operation between these two tensors is a regular element-wise operation between tensors of the same shape. . Tensor reshape operation type . Reshaping operations are perhaps the most important type of tensor operations because the shape of a tensor gives us something concrete we can use to shape an intuition for our tensors. . Note: Reshaping changes the shape but not the underlying data elements. . Using the reshape() function, we can specify the row x column shape that we are seeking. Notice that the product of the shape&#39;s components has to be equal to the number of elements in the original tensor. . Pytorch has another function called view() that does the same thing as reshape function. . import torch t = torch.tensor([ [1,1,1,1], [2,2,2,2], [3,3,3,3] ], dtype=torch.float32) . t.reshape([3,4]) . tensor([[1., 1., 1., 1.], [2., 2., 2., 2.], [3., 3., 3., 3.]]) . Another common reshape type operation is squeeze, unsqueeze. Those operations change the shape of our tensors is by squeezing and unsqueezing them. . Squeezing a tensor removes the dimensions or axes that have a length of one. | Unsqueezing a tensor adds a dimension with a length of one. | . t.reshape([1,12]).shape . torch.Size([1, 12]) . t.reshape([1,12]).squeeze().shape . torch.Size([12]) . t.reshape([1,12]).unsqueeze(dim=0).shape . torch.Size([1, 1, 12]) . Another common reshape type operation is flattening. A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flatted out before the fully connected layer will accept the input. A flatten operation on a tensor reshapes the tensor to have a shape that is equal to the number of elements contained in the tensor. This is the same thing as a 1d-array of elements. . These are some ways to flatten a tensor. . t.reshape(1,-1)[0] . tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]) . t.reshape(-1) . tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]) . t.view(t.numel()) . tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]) . t.flatten() . tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]) . In addition, it is possible to flatten only specific parts of a tensor. . t = torch.tensor([[ [1,1,1,1], [2,2,2,2], [3,3,3,3] ]], dtype=torch.float32) t.shape . torch.Size([1, 3, 4]) . t.flatten(start_dim=1).shape . torch.Size([1, 12]) . Take a deeper look inside the flatten operation, it is actually a composition of reshape and squeeze operation. . Note: flatten operation = reshape operation + squeeze operation . def flatten_ex(t): t = t.reshape(1, -1) t = t.squeeze() return t flatten_ex(t) . tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]) . Tensor element-wise operation type . An element-wise operation is an operation between two tensors that operates on corresponding elements within the respective tensors. Two tensors must have the same shape in order to perform element-wise operations on them. . Some common element-wise operations . t1 = torch.tensor([[1,2], [3,4]], dtype=torch.float32) t2 = torch.tensor([[9,8], [7,6]], dtype=torch.float32) . t1 + t2 # equivalent with t1.add(t2) . tensor([[10., 10.], [10., 10.]]) . t1 + 2 # equivalent with t1.add(2) . tensor([[3., 4.], [5., 6.]]) . t1 - 2 # equivalent with t1.sub(2) . tensor([[-1., 0.], [ 1., 2.]]) . t1 * 2 # equivalent with t1.mul(2) . tensor([[2., 4.], [6., 8.]]) . t1 / 2 # equivalent with t1.div(2) . tensor([[0.5000, 1.0000], [1.5000, 2.0000]]) . Comparison Operation is element-wise type operation . t.eq(0) . tensor([[ True, False, True], [False, True, False], [ True, False, True]]) . t.gt(0) . tensor([[False, True, False], [ True, False, True], [False, True, False]]) . t.lt(0) . tensor([[False, False, False], [False, False, False], [False, False, False]]) . With element-wise operations that are functions, it’s fine to assume that the function is applied to each element of the tensor. . t.abs() . tensor([[0., 1., 0.], [2., 0., 2.], [0., 3., 0.]]) . t.sqrt() . tensor([[0.0000, 1.0000, 0.0000], [1.4142, 0.0000, 1.4142], [0.0000, 1.7321, 0.0000]]) . t.neg() . tensor([[-0., -1., -0.], [-2., -0., -2.], [-0., -3., -0.]]) . t.neg().abs() . tensor([[0., 1., 0.], [2., 0., 2.], [0., 3., 0.]]) . . Note: Element-wise = Component-wise = Point-wise . Tensor reduction operations type . A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor. Tensors give us the ability to manage our data. The tensor can be reduced to a single scalar value or reduced along an axis. . Let’s look at common tensor reduction operations: . import torch t = torch.tensor([ [0,1,0], [2,0,2], [0,3,0] ], dtype=torch.float32) . Reducing to a tensor with a single element . t.sum(), t.prod(), t.mean(), t.std() . (tensor(8.), tensor(0.), tensor(0.8889), tensor(1.1667)) . Reducing tensors By Axes . t.sum(dim=0) . tensor([2., 4., 2.]) . Argmax tensor reduction operation is very common in neural network. This operation returns the index location of the maximum value inside a tensor. In practice, we often use the argmax() function on a network’s output prediction tensor, to determine which category has the highest prediction value. . t.max(dim=1) . torch.return_types.max( values=tensor([1., 2., 3.]), indices=tensor([1, 2, 1])) . t.argmax(dim=1) . tensor([1, 2, 1]) . Tensor access operation type . This operation provides the ability to access data within the tensor. Common tensor access operations are item(), tolist(), numpy() . t.mean().item() . 0.8888888955116272 . t.mean(dim=0).tolist() . [0.6666666865348816, 1.3333333730697632, 0.6666666865348816] . t.mean(dim=0).numpy() . array([0.6666667, 1.3333334, 0.6666667], dtype=float32) . Random topics . Object Oriented Programming and Why Pytorch select it. . When we’re writing programs or building software, there are two key components, code and data. With object oriented programming, we orient our program design and structure around objects. Objects are defined in code using classes. A class defines the object&#39;s specification or spec, which specifies what data and code each object of the class should have. When we create an object of a class, we call the object an instance of the class, and all instances of a given class have two core components: . Methods(code) | Attributes(data) | . In a given program, many objects, a.k.a instances of a given class have the same available attributes and the same available methods. The difference between objects of the same class is the values contained within the object for each attribute. Each object has its own attribute values. These values determine the internal state of the object. The code and data of each object is said to be encapsulated within the object. . Let’s build a simple class to demonstrate how classes encapsulate data and code: . class Sample: #class declaration def __init__(self, name): #class constructor (code) self.name = name #attribute (data) def set_name(self, name): #method declaration (code) self.name = name #method implementation (code) . Let&#39;s switch gears now and look at how object oriented programming fits in with PyTorch. . The primary component we&#39;ll need to build a neural network is a layer, and so, as we might expect, PyTorch&#39;s neural network library contains classes that aid us in constructing layers. As we know, deep neural networks are built using multiple layers. This is what makes the network deep. Each layer in a neural network has two primary components: . A transformation (code) | A collection of weights (data) | . Like many things in life, this fact makes layers great candidates to be represented as objects using Object Oriented Programming - OOP. . References . Some good sources: . pytorch zero to all | deeplizard | effective pytorch | what is torch.nn really? | recommend walk with pytorch | official tutorial | DL(with Pytorch) | Pytorch project template | nlp turorial with pytorch | UDACITY course | awesome pytorch list | deep learning with pytorch | others: https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc | Grokking Algorithms: An illustrated guide for programmers and other curious people 1st Edition | . | .",
            "url": "https://phucnsp.github.io/blog/self-taught/2020/03/22/self-taught-pytorch-part1-tensor.html",
            "relUrl": "/self-taught/2020/03/22/self-taught-pytorch-part1-tensor.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Tutorial jupyter notebook and  Fastpages",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](data/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ . Useful links for jupyter notebook . http://blog.juliusschulz.de/blog/ultimate-ipython-notebook#document-metadata | .",
            "url": "https://phucnsp.github.io/blog/tutorial/2020/02/20/tutorial-notebook-fastpage.html",
            "relUrl": "/tutorial/2020/02/20/tutorial-notebook-fastpage.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Singapore, quan sát và suy ngẫm",
            "content": "Đợt tháng mười vừa rồi tôi có dịp đi dạo quanh anh hàng xóm Singapore. Cũng lâu rồi tôi mới có dịp đi ra khỏi Vietnam, tôi thích cái cảm giác được bay, được transit ở một sân bay nào đó, ngồi ngắm những người xa lạ rảo bước qua lại, cô đơn nhưng thú vị. Singapore nổi tiếng bởi sự hiện đại, bởi những tòa nhà chọc trời, bởi Universal Studio…nhưng đối với tôi thì những cái đó không có nhiều hứng thú vì tôi cũng đã đi nhiều thành phố trên thế giới rồi. Tôi đi Singapore chơi đơn giản giới vì tôi muốn dẫn vợ đi nước ngoài cho biết và cũng vì tôi cần đi du lịch sau một khoảng thời gian rất rất lâu cắm đầu vô công việc. . Thật may thay, Singapore khác những nơi tôi từng qua và còn làm tôi mất mấy tiếng để ngồi viết cái note này. . Chiều hôm qua tôi trở về Tân Sân Nhất sau hơn 1 tiếng 30 phút bay, đơn giản là cảm giác xót xa. Lạ nhỉ, lần trước bay ở Berlin về chả thấy gì, lần này tự nhiên lại xót xa. Tại người ta ở cái tầm cao hơn mình quá, tôi khen Singapore mà tôi xót cho Saigon, tôi nhìn anh xe ôm, nhìn những chị tay xách nách mang, nhìn cái cách người dân quê tôi băng qua đường chễnh chệ bất chấp đèn tín hiệu, tiếng la hét chèo kéo khách, tiếng bóp còi inh ỏi, xa xa là anh công an ngồi lướt điện thoại ở một xó cạnh lối ra sân bay, tất cả đều làm tôi xót xa. Hôm thứ năm trước khi bay đi chơi còn thấy bình thường mà nhỉ! . Changi đón chào vợ chồng tôi bằng cái thác nước trong nhà cao nhất thế giới, đó là cái woww đầu tiên khi tàu điện kết nối giữa những terminal đưa chúng tôi đi qua cái thác nước nhân tạo này, ngay bên trong sân bay. Chúng tôi về đến hostel trời cũng đã tối, hơn 8pm thì phải, nên cả 2 đều đi ngủ sớm để hôm sau có sức mà đi chơi. . . Ba ngày vi vu ở thành phố này là ba ngày làm tôi suy ngẫm rất nhiều, cái giả định về một thành phố hiện đại với những tòa nhà chọc trời chán phèo dần dần mất đi khi tôi lần lượt đi qua những tụ điểm nổi tiếng ở đây, chứng kiến cái cách họ nâng tầm công nghệ lên thành nghê thuật đã làm tôi thay đổi góc nhìn của mình. Xây cái nhà cao chót vót lên thì ở thành phố lớn nào cũng có nhưng để khoa học và nghệ thuật thật sự gặp nhau thì không phải nơi nào cũng làm được và cũng không phải người dân ở đâu cũng cảm thụ được. Sáng sớm hôm thứ bảy, trong lúc dạo bộ và ngồi nghỉ ngơi ở trạm xe bus gần khu Chinatown, tôi đọc được một bài chia sẻ của anh country manager bên Knorex, từng học tập và làm việc ở Singapore sau đó về quản lý branch ở Ho Chi Minh. Góc nhìn của anh ấy trong công việc thật ra không mới đối với tôi nhưng đơn giản là nó được đọc đúng lúc, đúng thời điểm. Đó là góc nhìn về việc học suốt đời, học ở bất kì hoàn cảnh nào mà không phải chờ có thầy dạy mới học được, về growth mindset, về thái độ làm việc cho đi trước để nhận lại sau, đặc biệt là về sự toàn cầu hóa nhân lực của các công ty, tập đoàn, đồng nghiệp của bạn ngày nay không còn là những anh em bạn dì nữa mà có thể đến từ bất kì đâu trên thế giới này, Ấn độ, Brazile, Chile, Sillicon Valley, etc. Bài chia sẻ ấy như là chất xúc tác cho chuyến đi của tôi, tôi bắt đầu để ý hơn tới con người nơi đây, từng chi tiết nhỏ trên đường, cách họ đào đường lên và lấp lại cẩn thận mà không để nhấp nhô, cách họ giữ gìn sạch sẽ không chỉ phía trước mà cả phía sau của nhà hàng cho đến cách họ quy hoạch bố trí nhà cửa, bố trí mảng xanh khắp nơi… tôi đã cảm thấy thật khó khăn trong việc tìm ra chỗ để chê. Mọi thứ được hoàn thiện một cách tuyệt vời ở tầm quốc gia, Singapore có lẽ là ví dụ điển hình nhất cho khái niệm quản lý “tự do trong khuôn khổ”, người dân có không gian để thể hiện cái riêng nhưng phải trong khuôn khổ nhất định để giữ gìn cái chung. . Trưa thứ bảy vợ chồng tôi đi dạo bộ ở khu Marina Bay thì tình cờ được dự giờ một buổi tập hát của mấy em tiểu học. Tụi nhỏ biểu diễn về nhạc kịch hay gì đó đại loại thế, nghe khá xa xỉ đối với đại đa số dân Việt Nam mình. Thật buồn cười khi tôi khen con nít ở Singapore nói tiếng anh hay quá, hát tự tin quá. Nhưng thật sự chúng đã được thừa hưởng di sản phi vật thể quá lớn từ ông Lý, tiếng anh vs tiếng trung, để giờ đây có thể tiếp cận dễ dàng hơn với tinh hoa thế giới, lại thêm cái kiểu giáo dục khai sáng, tự do thể hiện cái tôi cá nhân thế này nữa thì hỡi ơi, mấy đứa cháu ở quê đang đung đưa võng nghe thần tượng Hàn Quốc hát cả ngày hay đang tụ tập quán trà sữa để chém gió, thì rồi cơ may nào để chúng cạnh tranh trong cái thế giới toàn cầu hóa đây. Có thể nhiều người sẽ nghĩ làm gì tới nỗi, vẫn có rất nhiều nhân tài người Việt học trường làng những vẫn nổi danh thế giới đó thôi, nhưng khi đánh giá cái tầm quốc gia thì người ta không nói câu chuyện của một vài người xuất chúng, người ta nói đến sức mạnh của passport index (sức mạnh tấm hộ chiếu Việt Nam hình như đứng gần áp chót bảng xếp hạng), người ta đánh giá thành tích toàn đoàn, có đấy những ngôi sao vàng lẻ loi đoạt huy chương tầm thế giới nhưng có mấy bài báo đăng thành tích của toàn đoàn không! Và theo trải nghiệm cá nhân của tôi, bây giờ khi bạn hỏi một người nước ngoài biết gì về Việt Nam thì câu trả lời khó mà ngoài chiến tranh và gia công quần áo. . À nói một tí về khái niệm “Du lịch, quan sát và suy ngẫm” mà tôi đã từng đọc đâu đó, nó mang tới cho bạn một góc nhìn “thấm” hơn về nơi mình đã đi qua. Du lịch không dừng lại ở những tấm hình selffie khoe trên facebook mà còn cả ở cảm thụ cá nhân. Nhưng cũng có lẽ sự cảm thụ này không dễ mà có được, sau một thời gian dài liên tục học tập, đọc sách, tích lũy kiến thức thì mới may ra chấp chớm cảm nhận được điều này. Đó là lý do vì sao mà chúng ta rất hay thấy du khách nước ngoài tới Việt Nam du lịch thường tới bảo tàng, thường mua sách về Việt Nam để đọc, đơn giản vì họ đang cảm thụ một cách sâu hơn văn hóa, con người, đất nước chúng ta, chứ không chỉ dừng lại bề nổi ở những bức ảnh. Ngày nay tất nhiên không cần phải tới bảo tàng thì chúng ta mới biết về một nơi nào đó, mọi thứ có thể được đọc dễ dàng qua internet nhưng thật sự cảm giác của việc đi du lịch, cảm nhận bằng chính tất cả giác quan để rồi có những suy ngẫm sâu sắc hơn về sự vật sự việc xung quanh mang lại những lợi ích to lớn. Nó là cơn mưa mùa hè cho những bộ não đã bị lu mờ bởi những thứ lặp đi lặp lại hằng ngày, thậm chí nó có thể thay đổi hoàn toàn góc nhìn của mình từ trước tới nay về một việc nào đó. . Quay trở lại câu chuyện xót xa Singapore, có lẽ một dịp nào đó tôi sẽ trở lại đất nước này để xem cảm giác còn như xưa không. Như một lời nhắn nhủ cho bản thân, toàn cầu hóa là có thật và ở ngay đít rồi. Quên đi những xích mích, những câu chuyện vặt ở xung quanh, trong lúc mình ngồi nhiều chuyện thì thằng khác ở đâu đó vẫn đang miệt mài tu luyện. Kỷ luật hơn, học tập ở mọi nơi, học ở bất kì ai, đừng chờ có thầy dạy thì mới học được, đừng sợ học nhiều quá sẽ làm não mình bớt thông minh. Học kiến thức để tăng trí thông mình IQ, học văn hóa nghệ thuật, học cách ửng xử để tăng trí thông minh EQ. Làm việc với một thái độ sẵn sàng hy sinh, thân tôi đây nè, vứt việc cho tôi đi. Dù biết rằng một cánh én nhỏ khó làm nên mùa xuân, tôi có trở thành công dân toàn cầu cũng chưa chắc Việt Nam sẽ tốt hơn nhưng ít nhất tôi không muốn là người góp phần cho đất nước mà tôi yêu quý trở nên tệ hơn! . Tối qua về đến nhà con bạn người Malay đang sống ở Singapore, thấy hình đăng facebook, mới nhắn rủ cafe, cũng hơi tiếc không gặp được. Nó khoe mới có thằng bồ người Ấn, tụi nó vừa đi đám cưới bạn ở đảo Galapagos bên Ecuador về và rủ năm sau ra Hà Nội xem Vietnam F1 Race. . Chắc năm sau ra Hà Nội, quan sát và ngẫm tiếp nhỉ… .",
            "url": "https://phucnsp.github.io/blog/travel/2019/10/07/Singpore-trip-and-remaining.html",
            "relUrl": "/travel/2019/10/07/Singpore-trip-and-remaining.html",
            "date": " • Oct 7, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Data Science workflow recommendation",
            "content": "Repository of this workflow is stored here . Production data science template . The template of this repository follows production-data-science workflow, which focuses on productionizing data scientist’s work, make the analysis or research to be reusable, applicable to production. The workflow is separated into 2 phases: . exploration phase is where data scientist explores the project, mainly work with jupyter notebook. All the work in this phase will be stored in exploration folder. | production phase is where data scientists’ works are refactored into packages so it can be reuse, imported. All the work in this phase will be stored in your_package folder. | . How to setup a new repository - for maintainer . git clone https://gitlab.com/Phuc_Su/production_data_science_template.git git clone &lt;your_project_repository&gt; cd &lt;your_project_name&gt; git checkout -b product-initial-setup # open Finder, copy all content of production_data_science_template into your project repository, except .git and .idea folder conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; pip install git-lfs # in case you want to add some large file extension other than .jpg, .pdf, .csv, .xlsx git lfs track &lt;add large file path&gt; # rename &lt;your package&gt; folder and modify setup.py, most importance is require_packages. See example below # write something about your project in README.md pip install -e . pip freeze | grep -v &lt;package_name&gt; &gt; requirements.txt git add . git commit -m &quot;First commit&quot; git push -u origin HEAD . Example of setup.py . setup( name=&#39;your_project&#39;, version=&#39;v0.1&#39;, description=&#39;&#39;, long_description=readme(), classifiers=[ &#39;Programming Language :: Python :: 3&#39;, ], url=&#39;https://github.com/phucnsp/production_data_science_template&#39;, author=&#39;Phuc_Su&#39;, author_email=&#39;&#39;, license=&#39;&#39;, packages=[&#39;your_package&#39;], install_requires=[ &#39;pypandoc&gt;=1.4&#39;, &#39;watermark&gt;=1.5.0&#39;, &#39;pandas&gt;=0.20.3&#39;, &#39;scikit-learn&gt;=0.19.0&#39;, &#39;scipy&gt;=0.19.1&#39;, &#39;matplotlib&gt;=2.1.0&#39;, &#39;pytest&gt;=3.2.3&#39;, &#39;pytest-runner&gt;=2.12.1&#39;, &#39;click&gt;=6.7&#39; ], setup_requires=[&#39;pytest-runner&#39;], tests_require=[&#39;pytest&#39;], ) . and you are ready~! 🎉 . Note: if you want to setup notification on slack for merge request from gitlab, reference here . How to contribute - for developers . Setup first time . bash conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; git clone &lt;repository url&gt; cd to/the/project/directory pip install -r requirements.txt pip install -e . . For a private repository accessible only through an SSH authentication, substitute https://github.com/ with git@github.com:. . Returning to work . Some rules: 1 branch/1 exploration/1 folder | branch-name convention: explore-* for exploration, refactor-* for refactor | . | . git checkout master git pull --all # if you continue to work on old branch git checkout &lt;branch&gt; # if you want to start a new exploration git checkout -b &lt;new_branch&gt; # if your branch is far behind master and you want to merge git merge master ##################### Start working ##################### git add &lt;path_to_work_files/folder&gt; git commit -m &quot;some message&quot; git push -u origin HEAD . Notes . requirements.txt helps to setup your virtual environment, to make sure all contributors working on the same environments. So whenever you have a new libraries need to install, after installing you need to add it into requirements.txt by pip freeze | grep -v &lt;package_name&gt; &gt; requirements.txt | setup.py allows you to create packages that you can redistribute. This script is meant to install your package on the end user’s system, not to prepare the development environment. packages - in-house development packages. | install_requires - packages that our development packages dependence on. | py_modules=[&#39;new_module&#39;] - in-house development modules need to install (placed in root directory) | . | pip install -e . - to install packages/modules from setup.py, in the editable mode. | If you want to add large file into working repository: pip install git-lfs git lfs install # Tell LFS to track files with given path git lfs track &quot;path_to_large_file&quot; # Tell LFS to track files with format &quot;*.jpg&quot; git lfs track &quot;*.jpg&quot; # Tell LFS to track content of the whole directory git lfs track &quot;data/*&quot; . | . How to use the package - for users . Install the library . conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; pip install -e &#39;git+https://github.com/phucnsp/production_data_science_template.git&#39; . For a private repository accessible only through an SSH authentication, substitute git+https://github.com with git+ssh://git@github.com. Note that -e argument above to make the installation editable. . Leisure read . Production Data Science tutorial | Writing a setup script | Minimum structure | gitlab slack notification service | git strategy | .",
            "url": "https://phucnsp.github.io/blog/work/2019/05/10/data-science-template.html",
            "relUrl": "/work/2019/05/10/data-science-template.html",
            "date": " • May 10, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "Predict house price in America",
            "content": "Introduction . import pandas as pd pd.options.display.max_columns = 999 import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import KFold from sklearn.metrics import mean_squared_error from sklearn import linear_model from sklearn.model_selection import KFold . df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) . def transform_features(df): return df def select_features(df): return df[[&quot;Gr Liv Area&quot;, &quot;SalePrice&quot;]] def train_and_test(df): train = df[:1460] test = df[1460:] ## You can use `pd.DataFrame.select_dtypes()` to specify column types ## and return only those columns as a data frame. numeric_train = train.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) numeric_test = test.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) ## You can use `pd.Series.drop()` to drop a value. features = numeric_train.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df) rmse . 57088.251612639091 . Feature Engineering . Handle missing values: All columns: Drop any with 5% or more missing values for now. Text columns: Drop any with 1 or more missing values for now. Numerical columns: For columns with missing values, fill in with the most common value in that column . 1: All columns: Drop any with 5% or more missing values for now. . ## Series object: column name -&gt; number of missing values num_missing = df.isnull().sum() . # Filter Series to columns containing &gt;5% missing values drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() # Drop those columns from the data frame. Note the use of the .index accessor df = df.drop(drop_missing_cols.index, axis=1) . ## Series object: column name -&gt; number of missing values text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) ## Filter Series to columns containing *any* missing values drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) . ## Compute column-wise missing value counts num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() fixable_numeric_cols . BsmtFin SF 1 1 BsmtFin SF 2 1 Bsmt Unf SF 1 Total Bsmt SF 1 Garage Cars 1 Garage Area 1 Bsmt Full Bath 2 Bsmt Half Bath 2 Mas Vnr Area 23 dtype: int64 . ## Compute the most common value for each column in `fixable_nmeric_missing_cols`. replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] replacement_values_dict . {&#39;Bsmt Full Bath&#39;: 0.0, &#39;Bsmt Half Bath&#39;: 0.0, &#39;Bsmt Unf SF&#39;: 0.0, &#39;BsmtFin SF 1&#39;: 0.0, &#39;BsmtFin SF 2&#39;: 0.0, &#39;Garage Area&#39;: 0.0, &#39;Garage Cars&#39;: 2.0, &#39;Mas Vnr Area&#39;: 0.0, &#39;Total Bsmt SF&#39;: 0.0} . ## Use `pd.DataFrame.fillna()` to replace missing values. df = df.fillna(replacement_values_dict) . ## Verify that every column has 0 missing values df.isnull().sum().value_counts() . 0 64 dtype: int64 . years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_sold[years_sold &lt; 0] . 2180 -1 dtype: int64 . years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] years_since_remod[years_since_remod &lt; 0] . 1702 -1 2180 -2 2181 -1 dtype: int64 . ## Create new columns df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod ## Drop rows with negative values for both of these new features df = df.drop([1702, 2180, 2181], axis=0) ## No longer need original year columns df = df.drop([&quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis = 1) . Drop columns that: a. that aren&#39;t useful for ML b. leak data about the final sale . ## Drop columns that aren&#39;t useful for ML df = df.drop([&quot;PID&quot;, &quot;Order&quot;], axis=1) ## Drop columns that leak info about the final sale df = df.drop([&quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Yr Sold&quot;], axis=1) . Let&#39;s update transform_features() . def transform_features(df): num_missing = df.isnull().sum() drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() df = df.drop(drop_missing_cols.index, axis=1) text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] df = df.fillna(replacement_values_dict) years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod df = df.drop([1702, 2180, 2181], axis=0) df = df.drop([&quot;PID&quot;, &quot;Order&quot;, &quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis=1) return df def select_features(df): return df[[&quot;Gr Liv Area&quot;, &quot;SalePrice&quot;]] def train_and_test(df): train = df[:1460] test = df[1460:] ## You can use `pd.DataFrame.select_dtypes()` to specify column types ## and return only those columns as a data frame. numeric_train = train.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) numeric_test = test.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) ## You can use `pd.Series.drop()` to drop a value. features = numeric_train.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df) rmse . 55275.367312413066 . Feature Selection . numerical_df = transform_df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]) numerical_df . MS SubClass Lot Area Overall Qual Overall Cond Mas Vnr Area BsmtFin SF 1 BsmtFin SF 2 Bsmt Unf SF Total Bsmt SF 1st Flr SF 2nd Flr SF Low Qual Fin SF Gr Liv Area Bsmt Full Bath Bsmt Half Bath Full Bath Half Bath Bedroom AbvGr Kitchen AbvGr TotRms AbvGrd Fireplaces Garage Cars Garage Area Wood Deck SF Open Porch SF Enclosed Porch 3Ssn Porch Screen Porch Pool Area Misc Val Yr Sold SalePrice Years Before Sale Years Since Remod . 0 20 | 31770 | 6 | 5 | 112.0 | 639.0 | 0.0 | 441.0 | 1080.0 | 1656 | 0 | 0 | 1656 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 7 | 2 | 2.0 | 528.0 | 210 | 62 | 0 | 0 | 0 | 0 | 0 | 2010 | 215000 | 50 | 50 | . 1 20 | 11622 | 5 | 6 | 0.0 | 468.0 | 144.0 | 270.0 | 882.0 | 896 | 0 | 0 | 896 | 0.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 0 | 1.0 | 730.0 | 140 | 0 | 0 | 0 | 120 | 0 | 0 | 2010 | 105000 | 49 | 49 | . 2 20 | 14267 | 6 | 6 | 108.0 | 923.0 | 0.0 | 406.0 | 1329.0 | 1329 | 0 | 0 | 1329 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 312.0 | 393 | 36 | 0 | 0 | 0 | 0 | 12500 | 2010 | 172000 | 52 | 52 | . 3 20 | 11160 | 7 | 5 | 0.0 | 1065.0 | 0.0 | 1045.0 | 2110.0 | 2110 | 0 | 0 | 2110 | 1.0 | 0.0 | 2 | 1 | 3 | 1 | 8 | 2 | 2.0 | 522.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 244000 | 42 | 42 | . 4 60 | 13830 | 5 | 5 | 0.0 | 791.0 | 0.0 | 137.0 | 928.0 | 928 | 701 | 0 | 1629 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 6 | 1 | 2.0 | 482.0 | 212 | 34 | 0 | 0 | 0 | 0 | 0 | 2010 | 189900 | 13 | 12 | . 5 60 | 9978 | 6 | 6 | 20.0 | 602.0 | 0.0 | 324.0 | 926.0 | 926 | 678 | 0 | 1604 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 470.0 | 360 | 36 | 0 | 0 | 0 | 0 | 0 | 2010 | 195500 | 12 | 12 | . 6 120 | 4920 | 8 | 5 | 0.0 | 616.0 | 0.0 | 722.0 | 1338.0 | 1338 | 0 | 0 | 1338 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 6 | 0 | 2.0 | 582.0 | 0 | 0 | 170 | 0 | 0 | 0 | 0 | 2010 | 213500 | 9 | 9 | . 7 120 | 5005 | 8 | 5 | 0.0 | 263.0 | 0.0 | 1017.0 | 1280.0 | 1280 | 0 | 0 | 1280 | 0.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 506.0 | 0 | 82 | 0 | 0 | 144 | 0 | 0 | 2010 | 191500 | 18 | 18 | . 8 120 | 5389 | 8 | 5 | 0.0 | 1180.0 | 0.0 | 415.0 | 1595.0 | 1616 | 0 | 0 | 1616 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 1 | 2.0 | 608.0 | 237 | 152 | 0 | 0 | 0 | 0 | 0 | 2010 | 236500 | 15 | 14 | . 9 60 | 7500 | 7 | 5 | 0.0 | 0.0 | 0.0 | 994.0 | 994.0 | 1028 | 776 | 0 | 1804 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 442.0 | 140 | 60 | 0 | 0 | 0 | 0 | 0 | 2010 | 189000 | 11 | 11 | . 10 60 | 10000 | 6 | 5 | 0.0 | 0.0 | 0.0 | 763.0 | 763.0 | 763 | 892 | 0 | 1655 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 440.0 | 157 | 84 | 0 | 0 | 0 | 0 | 0 | 2010 | 175900 | 17 | 16 | . 11 20 | 7980 | 6 | 7 | 0.0 | 935.0 | 0.0 | 233.0 | 1168.0 | 1187 | 0 | 0 | 1187 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 6 | 0 | 2.0 | 420.0 | 483 | 21 | 0 | 0 | 0 | 0 | 500 | 2010 | 185000 | 18 | 3 | . 12 60 | 8402 | 6 | 5 | 0.0 | 0.0 | 0.0 | 789.0 | 789.0 | 789 | 676 | 0 | 1465 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 393.0 | 0 | 75 | 0 | 0 | 0 | 0 | 0 | 2010 | 180400 | 12 | 12 | . 13 20 | 10176 | 7 | 5 | 0.0 | 637.0 | 0.0 | 663.0 | 1300.0 | 1341 | 0 | 0 | 1341 | 1.0 | 0.0 | 1 | 1 | 2 | 1 | 5 | 1 | 2.0 | 506.0 | 192 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 171500 | 20 | 20 | . 14 120 | 6820 | 8 | 5 | 0.0 | 368.0 | 1120.0 | 0.0 | 1488.0 | 1502 | 0 | 0 | 1502 | 1.0 | 0.0 | 1 | 1 | 1 | 1 | 4 | 0 | 2.0 | 528.0 | 0 | 54 | 0 | 0 | 140 | 0 | 0 | 2010 | 212000 | 25 | 25 | . 15 60 | 53504 | 8 | 5 | 603.0 | 1416.0 | 0.0 | 234.0 | 1650.0 | 1690 | 1589 | 0 | 3279 | 1.0 | 0.0 | 3 | 1 | 4 | 1 | 12 | 1 | 3.0 | 841.0 | 503 | 36 | 0 | 0 | 210 | 0 | 0 | 2010 | 538000 | 7 | 7 | . 16 50 | 12134 | 8 | 7 | 0.0 | 427.0 | 0.0 | 132.0 | 559.0 | 1080 | 672 | 0 | 1752 | 0.0 | 0.0 | 2 | 0 | 4 | 1 | 8 | 0 | 2.0 | 492.0 | 325 | 12 | 0 | 0 | 0 | 0 | 0 | 2010 | 164000 | 22 | 5 | . 17 20 | 11394 | 9 | 2 | 350.0 | 1445.0 | 0.0 | 411.0 | 1856.0 | 1856 | 0 | 0 | 1856 | 1.0 | 0.0 | 1 | 1 | 1 | 1 | 8 | 1 | 3.0 | 834.0 | 113 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 394432 | 0 | 0 | . 18 20 | 19138 | 4 | 5 | 0.0 | 120.0 | 0.0 | 744.0 | 864.0 | 864 | 0 | 0 | 864 | 0.0 | 0.0 | 1 | 0 | 2 | 1 | 4 | 0 | 2.0 | 400.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 141000 | 59 | 59 | . 19 20 | 13175 | 6 | 6 | 119.0 | 790.0 | 163.0 | 589.0 | 1542.0 | 2073 | 0 | 0 | 2073 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 7 | 2 | 2.0 | 500.0 | 349 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 210000 | 32 | 22 | . 20 20 | 11751 | 6 | 6 | 480.0 | 705.0 | 0.0 | 1139.0 | 1844.0 | 1844 | 0 | 0 | 1844 | 0.0 | 0.0 | 2 | 0 | 3 | 1 | 7 | 1 | 2.0 | 546.0 | 0 | 122 | 0 | 0 | 0 | 0 | 0 | 2010 | 190000 | 33 | 33 | . 21 85 | 10625 | 7 | 6 | 81.0 | 885.0 | 168.0 | 0.0 | 1053.0 | 1173 | 0 | 0 | 1173 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 6 | 2 | 2.0 | 528.0 | 0 | 120 | 0 | 0 | 0 | 0 | 0 | 2010 | 170000 | 36 | 36 | . 22 60 | 7500 | 7 | 5 | 0.0 | 533.0 | 0.0 | 281.0 | 814.0 | 814 | 860 | 0 | 1674 | 1.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 0 | 2.0 | 663.0 | 0 | 96 | 0 | 0 | 0 | 0 | 0 | 2010 | 216000 | 10 | 10 | . 23 20 | 11241 | 6 | 7 | 180.0 | 578.0 | 0.0 | 426.0 | 1004.0 | 1004 | 0 | 0 | 1004 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 1 | 2.0 | 480.0 | 0 | 0 | 0 | 0 | 0 | 0 | 700 | 2010 | 149000 | 40 | 40 | . 24 20 | 12537 | 5 | 6 | 0.0 | 734.0 | 0.0 | 344.0 | 1078.0 | 1078 | 0 | 0 | 1078 | 1.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 1 | 2.0 | 500.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 149900 | 39 | 2 | . 25 20 | 8450 | 5 | 6 | 0.0 | 775.0 | 0.0 | 281.0 | 1056.0 | 1056 | 0 | 0 | 1056 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 1 | 1.0 | 304.0 | 0 | 85 | 184 | 0 | 0 | 0 | 0 | 2010 | 142000 | 42 | 42 | . 26 20 | 8400 | 4 | 5 | 0.0 | 804.0 | 78.0 | 0.0 | 882.0 | 882 | 0 | 0 | 882 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 4 | 0 | 2.0 | 525.0 | 240 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 126000 | 40 | 40 | . 27 20 | 10500 | 4 | 5 | 0.0 | 432.0 | 0.0 | 432.0 | 864.0 | 864 | 0 | 0 | 864 | 0.0 | 0.0 | 1 | 0 | 3 | 1 | 5 | 1 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 115000 | 39 | 39 | . 28 120 | 5858 | 7 | 5 | 0.0 | 1051.0 | 0.0 | 354.0 | 1405.0 | 1337 | 0 | 0 | 1337 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 1 | 2.0 | 511.0 | 203 | 68 | 0 | 0 | 0 | 0 | 0 | 2010 | 184000 | 11 | 11 | . 29 160 | 1680 | 6 | 5 | 504.0 | 156.0 | 0.0 | 327.0 | 483.0 | 483 | 504 | 0 | 987 | 0.0 | 0.0 | 1 | 1 | 2 | 1 | 5 | 0 | 1.0 | 264.0 | 275 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 96000 | 39 | 39 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2900 20 | 13618 | 8 | 5 | 198.0 | 1350.0 | 0.0 | 378.0 | 1728.0 | 1960 | 0 | 0 | 1960 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 8 | 2 | 3.0 | 714.0 | 172 | 38 | 0 | 0 | 0 | 0 | 0 | 2006 | 320000 | 1 | 0 | . 2901 20 | 11443 | 8 | 5 | 208.0 | 1460.0 | 0.0 | 408.0 | 1868.0 | 2028 | 0 | 0 | 2028 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 7 | 2 | 3.0 | 880.0 | 326 | 66 | 0 | 0 | 0 | 0 | 0 | 2006 | 369900 | 1 | 0 | . 2902 20 | 11577 | 9 | 5 | 382.0 | 1455.0 | 0.0 | 383.0 | 1838.0 | 1838 | 0 | 0 | 1838 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 9 | 1 | 3.0 | 682.0 | 161 | 225 | 0 | 0 | 0 | 0 | 0 | 2006 | 359900 | 1 | 0 | . 2903 20 | 31250 | 1 | 3 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1600 | 0 | 0 | 1600 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 270.0 | 0 | 0 | 135 | 0 | 0 | 0 | 0 | 2006 | 81500 | 55 | 55 | . 2904 90 | 7020 | 7 | 5 | 200.0 | 1243.0 | 0.0 | 45.0 | 1288.0 | 1368 | 0 | 0 | 1368 | 2.0 | 0.0 | 2 | 0 | 2 | 2 | 8 | 0 | 4.0 | 784.0 | 0 | 48 | 0 | 0 | 0 | 0 | 0 | 2006 | 215000 | 9 | 9 | . 2905 120 | 4500 | 6 | 5 | 116.0 | 897.0 | 0.0 | 319.0 | 1216.0 | 1216 | 0 | 0 | 1216 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 402.0 | 0 | 125 | 0 | 0 | 0 | 0 | 0 | 2006 | 164000 | 8 | 8 | . 2906 120 | 4500 | 6 | 5 | 443.0 | 1201.0 | 0.0 | 36.0 | 1237.0 | 1337 | 0 | 0 | 1337 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 405.0 | 0 | 199 | 0 | 0 | 0 | 0 | 0 | 2006 | 153500 | 8 | 8 | . 2907 20 | 17217 | 5 | 5 | 0.0 | 0.0 | 0.0 | 1140.0 | 1140.0 | 1140 | 0 | 0 | 1140 | 0.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 0 | 0.0 | 0.0 | 36 | 56 | 0 | 0 | 0 | 0 | 0 | 2006 | 84500 | 0 | 0 | . 2908 160 | 2665 | 5 | 6 | 0.0 | 0.0 | 0.0 | 264.0 | 264.0 | 616 | 688 | 0 | 1304 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 1 | 1.0 | 336.0 | 141 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 104500 | 29 | 29 | . 2909 160 | 2665 | 5 | 6 | 0.0 | 548.0 | 173.0 | 36.0 | 757.0 | 925 | 550 | 0 | 1475 | 0.0 | 0.0 | 2 | 0 | 4 | 1 | 6 | 1 | 1.0 | 336.0 | 104 | 26 | 0 | 0 | 0 | 0 | 0 | 2006 | 127000 | 29 | 29 | . 2910 160 | 3964 | 6 | 4 | 0.0 | 837.0 | 0.0 | 105.0 | 942.0 | 1291 | 1230 | 0 | 2521 | 1.0 | 0.0 | 2 | 1 | 5 | 1 | 10 | 1 | 2.0 | 576.0 | 728 | 20 | 0 | 0 | 0 | 0 | 0 | 2006 | 151400 | 33 | 33 | . 2911 20 | 10172 | 5 | 7 | 0.0 | 441.0 | 0.0 | 423.0 | 864.0 | 874 | 0 | 0 | 874 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 5 | 0 | 1.0 | 288.0 | 0 | 120 | 0 | 0 | 0 | 0 | 0 | 2006 | 126500 | 38 | 3 | . 2912 90 | 11836 | 5 | 5 | 0.0 | 149.0 | 0.0 | 1503.0 | 1652.0 | 1652 | 0 | 0 | 1652 | 0.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 3.0 | 928.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 146500 | 36 | 36 | . 2913 180 | 1470 | 4 | 6 | 0.0 | 522.0 | 0.0 | 108.0 | 630.0 | 630 | 0 | 0 | 630 | 1.0 | 0.0 | 1 | 0 | 1 | 1 | 3 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 73000 | 36 | 36 | . 2914 160 | 1484 | 4 | 4 | 0.0 | 252.0 | 0.0 | 294.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 1.0 | 253.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 79400 | 34 | 34 | . 2915 20 | 13384 | 5 | 5 | 194.0 | 119.0 | 344.0 | 641.0 | 1104.0 | 1360 | 0 | 0 | 1360 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 8 | 1 | 1.0 | 336.0 | 160 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 140000 | 37 | 27 | . 2916 180 | 1533 | 5 | 7 | 0.0 | 553.0 | 0.0 | 77.0 | 630.0 | 630 | 0 | 0 | 630 | 1.0 | 0.0 | 1 | 0 | 1 | 1 | 3 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 92000 | 36 | 36 | . 2917 160 | 1533 | 4 | 5 | 0.0 | 408.0 | 0.0 | 138.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 1.0 | 286.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 87550 | 36 | 36 | . 2918 160 | 1526 | 4 | 5 | 0.0 | 0.0 | 0.0 | 546.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 0.0 | 0.0 | 0 | 34 | 0 | 0 | 0 | 0 | 0 | 2006 | 79500 | 36 | 36 | . 2919 160 | 1936 | 4 | 7 | 0.0 | 0.0 | 0.0 | 546.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 90500 | 36 | 36 | . 2920 160 | 1894 | 4 | 5 | 0.0 | 252.0 | 0.0 | 294.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 286.0 | 0 | 24 | 0 | 0 | 0 | 0 | 0 | 2006 | 71000 | 36 | 36 | . 2921 90 | 12640 | 6 | 5 | 0.0 | 936.0 | 396.0 | 396.0 | 1728.0 | 1728 | 0 | 0 | 1728 | 0.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 2.0 | 574.0 | 40 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 150900 | 30 | 30 | . 2922 90 | 9297 | 5 | 5 | 0.0 | 1606.0 | 0.0 | 122.0 | 1728.0 | 1728 | 0 | 0 | 1728 | 2.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 2.0 | 560.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 188000 | 30 | 30 | . 2923 20 | 17400 | 5 | 5 | 0.0 | 936.0 | 0.0 | 190.0 | 1126.0 | 1126 | 0 | 0 | 1126 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 5 | 1 | 2.0 | 484.0 | 295 | 41 | 0 | 0 | 0 | 0 | 0 | 2006 | 160000 | 29 | 29 | . 2924 20 | 20000 | 5 | 7 | 0.0 | 1224.0 | 0.0 | 0.0 | 1224.0 | 1224 | 0 | 0 | 1224 | 1.0 | 0.0 | 1 | 0 | 4 | 1 | 7 | 1 | 2.0 | 576.0 | 474 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 131000 | 46 | 10 | . 2925 80 | 7937 | 6 | 6 | 0.0 | 819.0 | 0.0 | 184.0 | 1003.0 | 1003 | 0 | 0 | 1003 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 0 | 2.0 | 588.0 | 120 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 142500 | 22 | 22 | . 2926 20 | 8885 | 5 | 5 | 0.0 | 301.0 | 324.0 | 239.0 | 864.0 | 902 | 0 | 0 | 902 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 0 | 2.0 | 484.0 | 164 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 131000 | 23 | 23 | . 2927 85 | 10441 | 5 | 5 | 0.0 | 337.0 | 0.0 | 575.0 | 912.0 | 970 | 0 | 0 | 970 | 0.0 | 1.0 | 1 | 0 | 3 | 1 | 6 | 0 | 0.0 | 0.0 | 80 | 32 | 0 | 0 | 0 | 0 | 700 | 2006 | 132000 | 14 | 14 | . 2928 20 | 10010 | 5 | 5 | 0.0 | 1071.0 | 123.0 | 195.0 | 1389.0 | 1389 | 0 | 0 | 1389 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 6 | 1 | 2.0 | 418.0 | 240 | 38 | 0 | 0 | 0 | 0 | 0 | 2006 | 170000 | 32 | 31 | . 2929 60 | 9627 | 7 | 5 | 94.0 | 758.0 | 0.0 | 238.0 | 996.0 | 996 | 1004 | 0 | 2000 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 9 | 1 | 3.0 | 650.0 | 190 | 48 | 0 | 0 | 0 | 0 | 0 | 2006 | 188000 | 13 | 12 | . 2927 rows × 34 columns . abs_corr_coeffs = numerical_df.corr()[&#39;SalePrice&#39;].abs().sort_values() abs_corr_coeffs . BsmtFin SF 2 0.006127 Misc Val 0.019273 Yr Sold 0.030358 3Ssn Porch 0.032268 Bsmt Half Bath 0.035875 Low Qual Fin SF 0.037629 Pool Area 0.068438 MS SubClass 0.085128 Overall Cond 0.101540 Screen Porch 0.112280 Kitchen AbvGr 0.119760 Enclosed Porch 0.128685 Bedroom AbvGr 0.143916 Bsmt Unf SF 0.182751 Lot Area 0.267520 2nd Flr SF 0.269601 Bsmt Full Bath 0.276258 Half Bath 0.284871 Open Porch SF 0.316262 Wood Deck SF 0.328183 BsmtFin SF 1 0.439284 Fireplaces 0.474831 TotRms AbvGrd 0.498574 Mas Vnr Area 0.506983 Years Since Remod 0.534985 Full Bath 0.546118 Years Before Sale 0.558979 1st Flr SF 0.635185 Garage Area 0.641425 Total Bsmt SF 0.644012 Garage Cars 0.648361 Gr Liv Area 0.717596 Overall Qual 0.801206 SalePrice 1.000000 Name: SalePrice, dtype: float64 . ## Let&#39;s only keep columns with a correlation coefficient of larger than 0.4 (arbitrary, worth experimenting later!) abs_corr_coeffs[abs_corr_coeffs &gt; 0.4] . BsmtFin SF 1 0.439284 Fireplaces 0.474831 TotRms AbvGrd 0.498574 Mas Vnr Area 0.506983 Years Since Remod 0.534985 Full Bath 0.546118 Years Before Sale 0.558979 1st Flr SF 0.635185 Garage Area 0.641425 Total Bsmt SF 0.644012 Garage Cars 0.648361 Gr Liv Area 0.717596 Overall Qual 0.801206 SalePrice 1.000000 Name: SalePrice, dtype: float64 . ## Drop columns with less than 0.4 correlation with SalePrice transform_df = transform_df.drop(abs_corr_coeffs[abs_corr_coeffs &lt; 0.4].index, axis=1) . Which categorical columns should we keep? . ## Create a list of column names from documentation that are *meant* to be categorical nominal_features = [&quot;PID&quot;, &quot;MS SubClass&quot;, &quot;MS Zoning&quot;, &quot;Street&quot;, &quot;Alley&quot;, &quot;Land Contour&quot;, &quot;Lot Config&quot;, &quot;Neighborhood&quot;, &quot;Condition 1&quot;, &quot;Condition 2&quot;, &quot;Bldg Type&quot;, &quot;House Style&quot;, &quot;Roof Style&quot;, &quot;Roof Matl&quot;, &quot;Exterior 1st&quot;, &quot;Exterior 2nd&quot;, &quot;Mas Vnr Type&quot;, &quot;Foundation&quot;, &quot;Heating&quot;, &quot;Central Air&quot;, &quot;Garage Type&quot;, &quot;Misc Feature&quot;, &quot;Sale Type&quot;, &quot;Sale Condition&quot;] . Which columns are currently numerical but need to be encoded as categorical instead (because the numbers don&#39;t have any semantic meaning)? If a categorical column has hundreds of unique values (or categories), should we keep it? When we dummy code this column, hundreds of columns will need to be added back to the data frame. . ## Which categorical columns have we still carried with us? We&#39;ll test tehse transform_cat_cols = [] for col in nominal_features: if col in transform_df.columns: transform_cat_cols.append(col) ## How many unique values in each categorical column? uniqueness_counts = transform_df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values() ## Aribtrary cutoff of 10 unique values (worth experimenting) drop_nonuniq_cols = uniqueness_counts[uniqueness_counts &gt; 10].index transform_df = transform_df.drop(drop_nonuniq_cols, axis=1) . ## Select just the remaining text columns and convert to categorical text_cols = transform_df.select_dtypes(include=[&#39;object&#39;]) for col in text_cols: transform_df[col] = transform_df[col].astype(&#39;category&#39;) ## Create dummy columns and add back to the dataframe! transform_df = pd.concat([ transform_df, pd.get_dummies(transform_df.select_dtypes(include=[&#39;category&#39;])) ], axis=1) . Update select_features() . def transform_features(df): num_missing = df.isnull().sum() drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() df = df.drop(drop_missing_cols.index, axis=1) text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] df = df.fillna(replacement_values_dict) years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod df = df.drop([1702, 2180, 2181], axis=0) df = df.drop([&quot;PID&quot;, &quot;Order&quot;, &quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis=1) return df def select_features(df, coeff_threshold=0.4, uniq_threshold=10): numerical_df = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]) abs_corr_coeffs = numerical_df.corr()[&#39;SalePrice&#39;].abs().sort_values() df = df.drop(abs_corr_coeffs[abs_corr_coeffs &lt; coeff_threshold].index, axis=1) nominal_features = [&quot;PID&quot;, &quot;MS SubClass&quot;, &quot;MS Zoning&quot;, &quot;Street&quot;, &quot;Alley&quot;, &quot;Land Contour&quot;, &quot;Lot Config&quot;, &quot;Neighborhood&quot;, &quot;Condition 1&quot;, &quot;Condition 2&quot;, &quot;Bldg Type&quot;, &quot;House Style&quot;, &quot;Roof Style&quot;, &quot;Roof Matl&quot;, &quot;Exterior 1st&quot;, &quot;Exterior 2nd&quot;, &quot;Mas Vnr Type&quot;, &quot;Foundation&quot;, &quot;Heating&quot;, &quot;Central Air&quot;, &quot;Garage Type&quot;, &quot;Misc Feature&quot;, &quot;Sale Type&quot;, &quot;Sale Condition&quot;] transform_cat_cols = [] for col in nominal_features: if col in df.columns: transform_cat_cols.append(col) uniqueness_counts = df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values() drop_nonuniq_cols = uniqueness_counts[uniqueness_counts &gt; 10].index df = df.drop(drop_nonuniq_cols, axis=1) text_cols = df.select_dtypes(include=[&#39;object&#39;]) for col in text_cols: df[col] = df[col].astype(&#39;category&#39;) df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=[&#39;category&#39;]))], axis=1) return df def train_and_test(df, k=0): numeric_df = df.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) features = numeric_df.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() if k == 0: train = df[:1460] test = df[1460:] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse if k == 1: # Randomize *all* rows (frac=1) from `df` and return shuffled_df = df.sample(frac=1, ) train = df[:1460] test = df[1460:] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions_one = lr.predict(test[features]) mse_one = mean_squared_error(test[&quot;SalePrice&quot;], predictions_one) rmse_one = np.sqrt(mse_one) lr.fit(test[features], test[&quot;SalePrice&quot;]) predictions_two = lr.predict(train[features]) mse_two = mean_squared_error(train[&quot;SalePrice&quot;], predictions_two) rmse_two = np.sqrt(mse_two) avg_rmse = np.mean([rmse_one, rmse_two]) print(rmse_one) print(rmse_two) return avg_rmse else: kf = KFold(n_splits=k, shuffle=True) rmse_values = [] for train_index, test_index, in kf.split(df): train = df.iloc[train_index] test = df.iloc[test_index] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) rmse_values.append(rmse) print(rmse_values) avg_rmse = np.mean(rmse_values) return avg_rmse df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df, k=4) rmse . [25761.875549560471, 36527.812968130842, 24956.485193881424, 28486.738135675929] . 28933.227961812168 .",
            "url": "https://phucnsp.github.io/blog/self-taught/2017/09/15/predict-house-price.html",
            "relUrl": "/self-taught/2017/09/15/predict-house-price.html",
            "date": " • Sep 15, 2017"
        }
        
    
  
    
        ,"post10": {
            "title": "Kaggle competition - titanic machine learning from disaster",
            "content": "import pandas as pd train = pd.read_csv(&quot;train.csv&quot;) holdout = pd.read_csv(&quot;test.csv&quot;) print(holdout.head()) . PassengerId Pclass Name Sex 0 892 3 Kelly, Mr. James male 1 893 3 Wilkes, Mrs. James (Ellen Needs) female 2 894 2 Myles, Mr. Thomas Francis male 3 895 3 Wirz, Mr. Albert male 4 896 3 Hirvonen, Mrs. Alexander (Helga E Lindqvist) female Age SibSp Parch Ticket Fare Cabin Embarked 0 34.5 0 0 330911 7.8292 NaN Q 1 47.0 1 0 363272 7.0000 NaN S 2 62.0 0 0 240276 9.6875 NaN Q 3 27.0 0 0 315154 8.6625 NaN S 4 22.0 1 1 3101298 12.2875 NaN S . # %load functions.py def process_missing(df): &quot;&quot;&quot;Handle various missing values from the data set Usage holdout = process_missing(holdout) &quot;&quot;&quot; df[&quot;Fare&quot;] = df[&quot;Fare&quot;].fillna(train[&quot;Fare&quot;].mean()) df[&quot;Embarked&quot;] = df[&quot;Embarked&quot;].fillna(&quot;S&quot;) return df def process_age(df): &quot;&quot;&quot;Process the Age column into pre-defined &#39;bins&#39; Usage train = process_age(train) &quot;&quot;&quot; df[&quot;Age&quot;] = df[&quot;Age&quot;].fillna(-0.5) cut_points = [-1,0,5,12,18,35,60,100] label_names = [&quot;Missing&quot;,&quot;Infant&quot;,&quot;Child&quot;,&quot;Teenager&quot;,&quot;Young Adult&quot;,&quot;Adult&quot;,&quot;Senior&quot;] df[&quot;Age_categories&quot;] = pd.cut(df[&quot;Age&quot;],cut_points,labels=label_names) return df def process_fare(df): &quot;&quot;&quot;Process the Fare column into pre-defined &#39;bins&#39; Usage train = process_fare(train) &quot;&quot;&quot; cut_points = [-1,12,50,100,1000] label_names = [&quot;0-12&quot;,&quot;12-50&quot;,&quot;50-100&quot;,&quot;100+&quot;] df[&quot;Fare_categories&quot;] = pd.cut(df[&quot;Fare&quot;],cut_points,labels=label_names) return df def process_cabin(df): &quot;&quot;&quot;Process the Cabin column into pre-defined &#39;bins&#39; Usage train process_cabin(train) &quot;&quot;&quot; df[&quot;Cabin_type&quot;] = df[&quot;Cabin&quot;].str[0] df[&quot;Cabin_type&quot;] = df[&quot;Cabin_type&quot;].fillna(&quot;Unknown&quot;) df = df.drop(&#39;Cabin&#39;,axis=1) return df def process_titles(df): &quot;&quot;&quot;Extract and categorize the title from the name column Usage train = process_titles(train) &quot;&quot;&quot; titles = { &quot;Mr&quot; : &quot;Mr&quot;, &quot;Mme&quot;: &quot;Mrs&quot;, &quot;Ms&quot;: &quot;Mrs&quot;, &quot;Mrs&quot; : &quot;Mrs&quot;, &quot;Master&quot; : &quot;Master&quot;, &quot;Mlle&quot;: &quot;Miss&quot;, &quot;Miss&quot; : &quot;Miss&quot;, &quot;Capt&quot;: &quot;Officer&quot;, &quot;Col&quot;: &quot;Officer&quot;, &quot;Major&quot;: &quot;Officer&quot;, &quot;Dr&quot;: &quot;Officer&quot;, &quot;Rev&quot;: &quot;Officer&quot;, &quot;Jonkheer&quot;: &quot;Royalty&quot;, &quot;Don&quot;: &quot;Royalty&quot;, &quot;Sir&quot; : &quot;Royalty&quot;, &quot;Countess&quot;: &quot;Royalty&quot;, &quot;Dona&quot;: &quot;Royalty&quot;, &quot;Lady&quot; : &quot;Royalty&quot; } extracted_titles = df[&quot;Name&quot;].str.extract(&#39; ([A-Za-z]+) .&#39;,expand=False) df[&quot;Title&quot;] = extracted_titles.map(titles) return df def create_dummies(df,column_name): &quot;&quot;&quot;Create Dummy Columns (One Hot Encoding) from a single Column Usage train = create_dummies(train,&quot;Age&quot;) &quot;&quot;&quot; dummies = pd.get_dummies(df[column_name],prefix=column_name) df = pd.concat([df,dummies],axis=1) return df . #preprocess the data def pre_process(df): df = process_missing(df) df = process_age(df) df = process_fare(df) df = process_titles(df) df = process_cabin(df) for col in [&quot;Age_categories&quot;,&quot;Fare_categories&quot;, &quot;Title&quot;,&quot;Cabin_type&quot;,&quot;Sex&quot;]: df = create_dummies(df,col) return df train = pre_process(train) holdout = pre_process(holdout) . Data exploration . #Inspect data type of column explore_cols = [&quot;SibSp&quot;,&quot;Parch&quot;,&quot;Survived&quot;] explore = train[explore_cols].copy() explore.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 3 columns): SibSp 891 non-null int64 Parch 891 non-null int64 Survived 891 non-null int64 dtypes: int64(3) memory usage: 21.0 KB . # Histogram to view the distribution of 2 columns: SibSp and Parch import matplotlib.pyplot as plt %matplotlib inline explore.drop(&quot;Survived&quot;,axis=1).plot.hist(alpha=0.5,bins=8) plt.xticks(range(11)) plt.show() . explore[&quot;familysize&quot;] = explore[[&quot;SibSp&quot;,&quot;Parch&quot;]].sum(axis=1) explore.drop(&quot;Survived&quot;,axis=1).plot.hist(alpha=0.5,bins=10) plt.xticks(range(11)) plt.show() . # Use pivot tables to look at the survival rate for different values of the columns import numpy as np for col in explore.columns.drop(&quot;Survived&quot;): pivot = explore.pivot_table(index=col,values=&quot;Survived&quot;) pivot.plot.bar(ylim=(0,1),yticks=np.arange(0,1,.1)) plt.axhspan(.3, .6, alpha=0.2, color=&#39;red&#39;) plt.show() . The SibSp column shows the number of siblings and/or spouses each passenger had on board, while the Parch columns shows the number of parents or children each passenger had onboard. Neither column has any missing values. . The distribution of values in both columns is skewed right, with the majority of values being zero. . You can sum these two columns to explore the total number of family members each passenger had onboard. The shape of the distribution of values in this case is similar, however there are less values at zero, and the quantity tapers off less rapidly as the values increase. . Looking at the survival rates of the the combined family members, you can see that few of the over 500 passengers with no family members survived, while greater numbers of passengers with family members survived. . Engineering new features . # Based on the observation about few surviver with no family group, let&#39;s create a binary value column where 1 is with # family and 0 is without family def feature_alone(df): df[&quot;familysize&quot;] = df[[&quot;SibSp&quot;,&quot;Parch&quot;]].sum(axis=1) df[&quot;isalone&quot;] = 0 df.loc[(df[&quot;familysize&quot;] == 0),&quot;isalone&quot;] = 1 df.drop(&quot;familysize&quot;, axis = 1) return df train = feature_alone(train) holdout = feature_alone(holdout) . Feature selection/preparation . # Select the best-performing features from sklearn.ensemble import RandomForestClassifier from sklearn.feature_selection import RFECV def select_features(df): # Remove non-numeric columns, columns that have null values df = df.select_dtypes([np.number]).dropna(axis=1) all_X = df.drop([&quot;Survived&quot;,&quot;PassengerId&quot;],axis=1) all_y = df[&quot;Survived&quot;] clf = RandomForestClassifier(random_state=1) selector = RFECV(clf,cv=10) selector.fit(all_X,all_y) best_columns = list(all_X.columns[selector.support_]) print(&quot;Best Columns n&quot;+&quot;-&quot;*12+&quot; n{}&quot;.format(best_columns)) return best_columns cols = select_features(train) . Best Columns [&#39;Pclass&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Age_categories_Adult&#39;, &#39;Age_categories_Infant&#39;, &#39;Age_categories_Missing&#39;, &#39;Age_categories_Senior&#39;, &#39;Age_categories_Teenager&#39;, &#39;Age_categories_Young Adult&#39;, &#39;Fare_categories_0-12&#39;, &#39;Fare_categories_100+&#39;, &#39;Fare_categories_12-50&#39;, &#39;Fare_categories_50-100&#39;, &#39;Title_Master&#39;, &#39;Title_Miss&#39;, &#39;Title_Mr&#39;, &#39;Title_Mrs&#39;, &#39;Title_Officer&#39;, &#39;Cabin_type_C&#39;, &#39;Cabin_type_D&#39;, &#39;Cabin_type_E&#39;, &#39;Cabin_type_Unknown&#39;, &#39;Sex_female&#39;, &#39;Sex_male&#39;, &#39;familysize&#39;, &#39;isalone&#39;] . Model selection/Tuning . # Write a function to train 3 different models. # Using grid search to train using different combinations of hyperparameters to find best performing models. from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import GridSearchCV def select_model(df,features): all_X = df[features] all_y = df[&quot;Survived&quot;] # List of dictionaries, each containing a model name, # it&#39;s estimator and a dict of hyperparameters models = [ { &quot;name&quot;: &quot;LogisticRegression&quot;, &quot;estimator&quot;: LogisticRegression(), &quot;hyperparameters&quot;: { &quot;solver&quot;: [&quot;newton-cg&quot;, &quot;lbfgs&quot;, &quot;liblinear&quot;] } }, { &quot;name&quot;: &quot;KNeighborsClassifier&quot;, &quot;estimator&quot;: KNeighborsClassifier(), &quot;hyperparameters&quot;: { &quot;n_neighbors&quot;: range(1,20,2), &quot;weights&quot;: [&quot;distance&quot;, &quot;uniform&quot;], &quot;algorithm&quot;: [&quot;ball_tree&quot;, &quot;kd_tree&quot;, &quot;brute&quot;], &quot;p&quot;: [1,2] } }, { &quot;name&quot;: &quot;RandomForestClassifier&quot;, &quot;estimator&quot;: RandomForestClassifier(random_state=1), &quot;hyperparameters&quot;: { &quot;n_estimators&quot;: [4, 6, 9], &quot;criterion&quot;: [&quot;entropy&quot;, &quot;gini&quot;], &quot;max_depth&quot;: [2, 5, 10], &quot;max_features&quot;: [&quot;log2&quot;, &quot;sqrt&quot;], &quot;min_samples_leaf&quot;: [1, 5, 8], &quot;min_samples_split&quot;: [2, 3, 5] } } ] for model in models: print(model[&#39;name&#39;]) print(&#39;-&#39;*len(model[&#39;name&#39;])) grid = GridSearchCV(model[&quot;estimator&quot;], param_grid=model[&quot;hyperparameters&quot;], cv=10) grid.fit(all_X,all_y) model[&quot;best_params&quot;] = grid.best_params_ model[&quot;best_score&quot;] = grid.best_score_ model[&quot;best_model&quot;] = grid.best_estimator_ print(&quot;Best Score: {}&quot;.format(model[&quot;best_score&quot;])) print(&quot;Best Parameters: {} n&quot;.format(model[&quot;best_params&quot;])) return models result = select_model(train,cols) . LogisticRegression Best Score: 0.8226711560044894 Best Parameters: {&#39;solver&#39;: &#39;liblinear&#39;} KNeighborsClassifier -- Best Score: 0.7833894500561167 Best Parameters: {&#39;algorithm&#39;: &#39;kd_tree&#39;, &#39;n_neighbors&#39;: 3, &#39;p&#39;: 1, &#39;weights&#39;: &#39;uniform&#39;} RandomForestClassifier - Best Score: 0.8451178451178452 Best Parameters: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 10, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 3, &#39;n_estimators&#39;: 9} . Submit to Kaggle . def save_submission_file(model,cols,filename=&quot;submission.csv&quot;): holdout_data = holdout[cols] predictions = model.predict(holdout_data) holdout_ids = holdout[&quot;PassengerId&quot;] submission_df = {&quot;PassengerId&quot;: holdout_ids, &quot;Survived&quot;: predictions} submission = pd.DataFrame(submission_df) submission.to_csv(filename,index=False) best_rf_model = result[2][&quot;best_model&quot;] save_submission_file(best_rf_model,cols) .",
            "url": "https://phucnsp.github.io/blog/kaggle/2017/08/20/kaggle-titanic-machine-learning-from-disaster.html",
            "relUrl": "/kaggle/2017/08/20/kaggle-titanic-machine-learning-from-disaster.html",
            "date": " • Aug 20, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I have been working as Data Scientist at MTI Technology Vietnam since 2018 and my journey in AI field started since 2017. In here, I mainly work with OCR (optical charcter recognition) projects where we not only have to extract texts from documents but also classify it into some specific fields defined by clients. .",
          "url": "https://phucnsp.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}