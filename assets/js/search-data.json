{
  
    
        "post0": {
            "title": "",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](data/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ . Useful links for jupyter notebook . http://blog.juliusschulz.de/blog/ultimate-ipython-notebook#document-metadata | .",
            "url": "https://phucnsp.github.io/blog/2020/04/10/2020-02-20-tutorial-notebook-fastpage.html",
            "relUrl": "/2020/04/10/2020-02-20-tutorial-notebook-fastpage.html",
            "date": " • Apr 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Pytorch part 2 - basic training and debugging neural network",
            "content": "Computer programs in general consist of two primary components, code and data. With traditional programming, the programmer’s job is to explicitly write the software or code to perform computations. But with deep learning and neural networks, this job explicitly belongs to the optimization algorithm. It will compile our data into code which is actually neural net&#39;s weights. The programmer’s job is to oversee and guide the learning process though training. We can think of this as an indirect way of writing software or code. . In this part, we will go through a basic pipeline of training and debugging neural network which contains 4 main steps: . Prepare the data for training | Build the model | Train the model | Analyze the model&#39;s results | In reality there are 2 very improtant steps that we are ignoring here: . Data collection and labeling: this step comes before step 1 and is the most tricky and time consuming. | Deployment and testing: after finishing training, we need to deploy it to production. We will try to cover this step later. To be simple and pytorch oritented, we will use the well-prepared and clean dataset FASHION MNIST which is available in torchvision package. | . So now, let&#39;s get started! . Data and Data processing . Prepare the data for training - we are here | Build the model | Train the model | Analyze the model&#39;s results | Data is the primary ingredient of deep learning. Before feeding data into our network, we need to consider many aspects such as: . Who created the dataset? | How was the dataset created? | What transformations were used? | What intent does the dataset have? | Possible unintentional consequences? | Is the dataset biased? | Are there ethical issues with the dataset? | . In this tutorial, we will use the well-prepared Fashion-MNIST dataset which was created by research lab of Zalando - a German based multi-national fashion commerce company. The dataset was designed to mirror the original MNIST dataset as closely as possible while introducing higher difficulty in training due to simply having more complex data than hand written images. The abstract from its paper: . We present Fashion-MNIST, a new dataset comprising of 28 × 28 grayscale images of 70, 000 fashion products from 10 categories, with 7, 000 images per category. The training set has 60, 000 images and the test set has 10, 000 images. Fashion-MNIST is intended to serve as a direct dropin replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist. . The Fashion-MNIST was built, unlike the hand-drawn MNIST dataset, from actual images on Zalando’s website. However, they have been transformed to more closely correspond to the MNIST specifications. This is the general conversion process that each image from the site went through: . Converted to PNG | Trimmed | Resized | Sharpened | Extended | Negated | Gray-scaled | . The dataset has the following ten classes of fashion items: . Index 0 1 2 3 4 5 6 7 8 9 . Label | T-shirt/top | Trouser | Pullover | Dress | Coat | Sandal | Shirt | Sneaker | Bag | Ankle boot | . A sample of the items look like this: . . That&#39;s enough information for the dataset. Now we will go to step 1. prepare data for training. . Firstly we need to import all the necessary packages. . # import common Pytorch packages import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchvision import torchvision.transforms as transforms # import other common packages used for data science import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix import pdb # python debugger torch.set_printoptions(linewidth=120) # set print options for pytorch printe statements . Along the way forward we will frequently use the following 7 most common Pytorch libraries: . Package Description . torch | The top-level PyTorch package and tensor library. | . torch.nn | A subpackage that contains modules and extensible classes for building neural networks. | . torch.nn.functional | A functional interface that contains typical operations used for building neural networks like loss functions and convolutions. | . torch.autograd | handle the automatic differentiation of arbitrary scalar valued functions. | . torch.optim | A subpackage that contains standard optimization operations like SGD and Adam. | . torchvision | A package that provides access to popular datasets, model architectures, and image transformations for computer vision. | . torchvision.transforms | An interface that contains common transforms for image processing. | . We&#39;ll follow ETL process using torchvision: . Extract: Get the Fashion-MNIST image data from the source. | Transform: Put our data into tensor form. | Load: Put our data into an object to make it easily accessible. | . For these purposes, PyTorch provides us with two classes: . torch.utils.data.Dataset: an abstract class for representing a dataset. An abstract class is a Python class that has methods we must implement, in our case are __getitem__ and __len__. In order to create a custom dataset, we need to subclass the Dataset class and override __len__, that provides the size of the dataset, and __getitem__, supporting integer indexing in range from 0 to len(self) exclusive. Upon doing this, our new subclass can then be passed to the a PyTorch DataLoader object. | torch.utils.data.DataLoader: wraps a dataset and provides access to the underlying data. | . The fashion-MNIST dataset that comes built-in with the torchvision package, so we won’t have to do the steps mentioned above for creatign a new dataset. Just know that the Fashion-MNIST built-in dataset class is doing this behind the scenes. When we run the code below for the first time, the Fashion-MNIST dataset will be downloaded locally. Subsequent calls check for the data before downloading it. Thus, we don&#39;t have to worry about double downloads or repeated network calls. . train_set = torchvision.datasets.FashionMNIST( root=&#39;./data&#39;, # The location on disk where the data is located. train=True, # If the dataset is the training set download=True, # If the data should be downloaded. transform=transforms.Compose([ # A composition of transformations performed on dataset elements. transforms.ToTensor() ]) ) . Wrap the dataset into dataloader and load our first batch of sammple. . train_dl = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True) # the batch will be different each time a call next occurs. images, labels = next(iter(train_dl)) images.shape, labels.shape . (torch.Size([4, 1, 28, 28]), torch.Size([4])) . The size of each dimention in the image tensor is defined as (batch size, number of color channels, image height, image width). So now, we have our desired dataset and dataloader in order to feed into the neural net. We can play around with the dataset to better understand it. . # the number of samples in our dataset len(train_set) . 60000 . # label of all 6000 images in train_set train_set.targets . tensor([9, 0, 0, ..., 3, 0, 5]) . # classes to predict train_set.classes . [&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;] . # number of samples per class train_set.targets.bincount() . tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]) . # take a look on a single item ys[0].item() plt.imshow(xs[0].squeeze(), cmap=&quot;gray&quot;) . &lt;matplotlib.image.AxesImage at 0x1a2a0ee790&gt; . # take a look on a batch grid = torchvision.utils.make_grid(xs, nrow=10) plt.figure(figsize=(15,15)) plt.imshow(grid.permute(1,2,0)) # plt.imshow(np.transpose(grid, (1,2,0))) . &lt;matplotlib.image.AxesImage at 0x1a2a383e90&gt; . Neural Net and Pytorch design . Prepare the data for training | Build the model - we are here | Train the model | Analyze the model&#39;s results | What we want our network to ultimately do is model or approximate a function that maps image inputs to the correct output class. . To build neural networks in PyTorch, we use the torch.nn package, which is PyTorch’s neural network (nn) library. We typically import the package like so: . The primary component we&#39;ll need to build a neural network is a layer, and so, as we might expect, PyTorch&#39;s neural network library contains classes that aid us in constructing layers. . PyTorch&#39;s nn.Module Class . Build PyTorch CNN - Object Oriented Neural Net- | CNN Layers - Deep Neural Network Archite- | CNN Weights - Learnable Parameters in Neural Net- | Callable Neural Networks - Linear Layers in - | CNN Forward Method - Deep Learning Implement- | Forward Propagation Explained - Pass Image to PyTorch Neural Network | Neural Network Batch Processing - Pass Image Batch to PyTorch CNN | CNN Output Size Formula - Bonus Neural Network Debugging Session | . import torch.nn as nn import torch.nn.functional as F class Network(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self, x): # (1) input layer x = x # (2) hidden conv layer x = self.conv1(x) x = F.relu(x) x = F.max_pool2d(x, kernel_size=2, stride=2) # out shape - bs x 6 x 12 x 12 # (3) hidden conv layer x = self.conv2(x) x = F.relu(x) x = F.max_pool2d(x, kernel_size=2, stride=2) # out shape - bs x 12 x 4 x 4 # (4) hidden linear layer # import pdb; pdb.set_trace() x = x.flatten(start_dim=1) # x.reshape(-1, 12*4*4) x = self.fc1(x) x = F.relu(x) # (5) hidden linear layer x = self.fc2(x) x = F.relu(x) # (6) output layer x = self.out(x) # x = F.softmax(x, dim=1) return x . model = Network() model . Network( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=192, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=60, bias=True) (out): Linear(in_features=60, out_features=10, bias=True) ) . pred = model(xs) print(pred.shape) pred . torch.Size([4, 10]) . tensor([[-0.0071, 0.0308, 0.1355, 0.1304, -0.1290, 0.0521, 0.0140, -0.0722, 0.0914, -0.0045], [-0.0082, 0.0273, 0.1325, 0.1348, -0.1298, 0.0527, 0.0120, -0.0763, 0.0880, -0.0087], [-0.0101, 0.0328, 0.1360, 0.1372, -0.1294, 0.0470, 0.0095, -0.0750, 0.0896, -0.0034], [-0.0076, 0.0324, 0.1334, 0.1366, -0.1288, 0.0475, 0.0084, -0.0755, 0.0876, -0.0043]], grad_fn=&lt;AddmmBackward&gt;) . F.softmax(pred) . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. &#34;&#34;&#34;Entry point for launching an IPython kernel. . tensor([[0.1184, 0.1070, 0.0941, 0.0892, 0.0883, 0.1071, 0.0939, 0.0983, 0.1142, 0.0895], [0.1184, 0.1071, 0.0940, 0.0894, 0.0889, 0.1064, 0.0931, 0.0982, 0.1145, 0.0901], [0.1180, 0.1080, 0.0937, 0.0892, 0.0888, 0.1061, 0.0936, 0.0980, 0.1144, 0.0902], [0.1183, 0.1069, 0.0939, 0.0892, 0.0880, 0.1064, 0.0939, 0.0986, 0.1145, 0.0902]], grad_fn=&lt;SoftmaxBackward&gt;) . torch.argmax(F.softmax(pred)) . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. &#34;&#34;&#34;Entry point for launching an IPython kernel. . tensor(0) . Training NN . We are now ready to begin the training process. The training process is an iterative process which including following steps: . Get batch from the training set. . Since we have 60,000 samples in our training set, we will have 60,000 / 100 = 600 iterations. Something to notice, batch_size will directly impact to the number of times the weights updated. In our case, the weights will be updated 600 times by the end of each loop. So far, there is no rule-of-thump for selecting the value of batch size so we still need to do trial and error to figure out the best value. | . | Pass batch to network. . | Calculate the loss (difference between the predicted values and the true values). . To be notice, the function cross_entropy has a parameter called reduction which has 3 option - none, mean, sum. By default, this variable is set to mean which will average the loss value by batch_size. None and sum means there is no reduction applied and the output will be summed, respectively. | . | Calculate the gradient of the loss function w.r.t the network&#39;s weights. . Before calculate the gradient, we need to zero out these gradients, by calling optimizer.zero_grad(), because after we call the loss.backward() method, the gradients will be calculated and added to the grad attributes of our network&#39;s parameters. | Calculating the gradients is very easy using PyTorch. Since PyTorch has created a computation graph under the hood. As our tensor flowed forward through our network, all of the computations where added to the graph. The computation graph is then used by PyTorch to calculate the gradients of the loss function with respect to the network&#39;s weights. | . | Update the weights using the gradients to reduce the loss. . The gradients calculated from step 4 are used by the optimizer to update the respective weights. | . | Repeat steps 1-5 until one epoch is completed. . | Repeat steps 1-6 for as many epochs required to reach the minimum loss. | To create our optimizer, we use the torch.optim package that has many optimization algorithm implementations that we can use, for example: Adam. The model&#39;s parameters and learning rate are passed to the optimizer. lr tells optimizer how far to step in the direction of minimizing loss fct while Parameters provide Adam the ability to access gradients. . model = Network() train_dl = torch.utils.data.DataLoader(train_set, batch_size=100) optimizer = torch.optim.Adam(model.parameters(), lr=0.01) accuracy = lambda preds, lbs: (F.softmax(preds).argmax(dim=1) == lbs).sum().item() for epoch in range(3): total_loss = 0 total_acc = 0 for batch in train_dl: images, labels = batch # 1. Get batch from the training set. preds = model(images) # 2. Pass batch to network. loss = F.cross_entropy(preds, labels) # 3. Calculate the loss optimizer.zero_grad() loss.backward() # 4. Calculate the gradient optimizer.step() # 5. Update the weights total_loss += loss.item() total_acc += accuracy(preds, labels) nr_iters_per_epoch = len(train_dl) print(f&quot;epoch: {epoch} navg_loss: {total_loss/nr_iters_per_epoch} navg_acc : {total_acc/nr_iters_per_epoch}&quot;) . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. after removing the cwd from sys.path. . epoch: 0 avg_loss: 0.6262958686550458 avg_acc : 76.13 epoch: 1 avg_loss: 0.4225387443850438 avg_acc : 84.28333333333333 epoch: 2 avg_loss: 0.37975253333648046 avg_acc : 85.75166666666667 . Analyze the model&#39;s results . Analyse classification result using a Confusion Matrix . The confusion matrix will show us which categories the model is predicting correctly and which categories the model is predicting incorrectly. For the incorrect predictions, this will show us which categories are confusing the model. . Now, if we compare the two tensors element-wise and count the number of predicted labels vs the target labels, the values inside the two tensors act as coordinates for our matrix. Let&#39;s stack these two tensors along the second dimension so we can have 60,000 ordered pairs. . len(train_set), len(train_set.targets) . (60000, 60000) . # Get predictions for the entire training set @torch.no_grad() def get_all_preds(model, loader): all_preds = torch.tensor([]) for batch in loader: imgs, lbs = batch preds = model(imgs) all_preds = torch.cat((all_preds, preds), dim=0) return all_preds with torch.no_grad(): prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000) train_preds = get_all_preds(model, prediction_loader) stacked = torch.stack( ( train_set.targets, train_preds.argmax(dim=1) ), dim=1) stacked.shape . torch.Size([60000, 2]) . # method 1: manually calculate confusion matrix cmt = torch.zeros(10,10, dtype=torch.int64) for p in stacked: tl, pl = p.tolist() cmt[tl,pl] += 1 cmt . tensor([[5523, 1, 46, 59, 15, 2, 306, 0, 48, 0], [ 36, 5829, 3, 77, 15, 0, 32, 0, 8, 0], [ 144, 6, 3704, 51, 1359, 3, 603, 0, 129, 1], [ 554, 26, 2, 5077, 189, 0, 135, 0, 15, 2], [ 49, 3, 99, 161, 5298, 1, 316, 0, 72, 1], [ 13, 0, 0, 0, 0, 5671, 0, 178, 81, 57], [1443, 10, 244, 75, 693, 0, 3414, 0, 121, 0], [ 1, 0, 0, 0, 0, 38, 0, 5823, 19, 119], [ 45, 0, 4, 8, 17, 1, 34, 4, 5884, 3], [ 0, 0, 0, 0, 0, 23, 0, 261, 6, 5710]]) . # method 2: use sklearn import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix cm = confusion_matrix(train_set.targets, train_preds.argmax(dim=1)) cm . array([[5523, 1, 46, 59, 15, 2, 306, 0, 48, 0], [ 36, 5829, 3, 77, 15, 0, 32, 0, 8, 0], [ 144, 6, 3704, 51, 1359, 3, 603, 0, 129, 1], [ 554, 26, 2, 5077, 189, 0, 135, 0, 15, 2], [ 49, 3, 99, 161, 5298, 1, 316, 0, 72, 1], [ 13, 0, 0, 0, 0, 5671, 0, 178, 81, 57], [1443, 10, 244, 75, 693, 0, 3414, 0, 121, 0], [ 1, 0, 0, 0, 0, 38, 0, 5823, 19, 119], [ 45, 0, 4, 8, 17, 1, 34, 4, 5884, 3], [ 0, 0, 0, 0, 0, 23, 0, 261, 6, 5710]]) . To nicely plot the confusion matrix, we can use below util function. . import itertools import numpy as np import matplotlib.pyplot as plt def plot_confusion_matrix(cm, classes, normalize=False, title=&#39;Confusion matrix&#39;, cmap=plt.cm.Blues): if normalize: cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] print(&quot;Normalized confusion matrix&quot;) else: print(&#39;Confusion matrix, without normalization&#39;) print(cm) plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = &#39;.2f&#39; if normalize else &#39;d&#39; thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) . names = ( &#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39; ) plt.figure(figsize=(10,10)) plot_confusion_matrix(cm, names) . Confusion matrix, without normalization [[5523 1 46 59 15 2 306 0 48 0] [ 36 5829 3 77 15 0 32 0 8 0] [ 144 6 3704 51 1359 3 603 0 129 1] [ 554 26 2 5077 189 0 135 0 15 2] [ 49 3 99 161 5298 1 316 0 72 1] [ 13 0 0 0 0 5671 0 178 81 57] [1443 10 244 75 693 0 3414 0 121 0] [ 1 0 0 0 0 38 0 5823 19 119] [ 45 0 4 8 17 1 34 4 5884 3] [ 0 0 0 0 0 23 0 261 6 5710]] . The confusion matrix has three axes: . Prediction label (class) | True label | Heat map value (color) | . The prediction label and true labels show us which prediction class we are dealing with. The matrix diagonal represents locations in the matrix where the prediction and the truth are the same, so this is where we want the heat map to be darker. . Any values that are not on the diagonal are incorrect predictions because the prediction and the true label don&#39;t match. To read the plot, we can use these steps: . Choose a prediction label on the horizontal axis. | Check the diagonal location for this label to see the total number correct. | Check the other non-diagonal locations to see where the network is confused. | For example, the network is confusing a T-shirt/top with a shirt, but is not confusing the T-shirt/top with things like: Ankle boot, Sneaker, Sandal. If we think about it, this makes pretty good sense. As our model learns, we will see the numbers that lie outside the diagonal become smaller and smaller. . Analyse training loop by using TensorBoard with PyTorch . Tensorboard provides visualization and tooling needed for machine learning experimentation. With tensorboard, we can: . Tracking and visualizing metrics such as loss and accuracy | Visualizing the model graph (ops and layers) | Viewing histograms of weights, biases, or other tensors as they change over time | Projecting embeddings to a lower dimensional space | Displaying images, text, and audio data | Profiling TensorFlow programs | And much more | . It is a font-end web interface that essentially reads data from a file and displays it. PyTorch has created a utility class called SummaryWriter which will help us get the data out of our program, save on disk so as to tensorboard can read. To use tensorboard, PyTorch version 1.1.0 is required. . import torch print(f&quot; torch version: {torch.__version__}&quot;) . torch version: 1.4.0 . !pip install tensorboard==1.15.0; from torch.utils.tensorboard import SummaryWriter . Requirement already satisfied: tensorboard==1.15.0 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (1.15.0) Requirement already satisfied: six&gt;=1.10.0 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (1.14.0) Requirement already satisfied: markdown&gt;=2.6.8 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (3.2.1) Requirement already satisfied: absl-py&gt;=0.4 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (0.9.0) Requirement already satisfied: protobuf&gt;=3.6.0 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (3.11.3) Requirement already satisfied: numpy&gt;=1.12.0 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (1.18.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (45.2.0.post20200210) Requirement already satisfied: grpcio&gt;=1.6.3 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (1.28.1) Requirement already satisfied: werkzeug&gt;=0.11.15 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (1.0.1) Requirement already satisfied: wheel&gt;=0.26; python_version &gt;= &#34;3&#34; in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (0.34.2) . The SummaryWriter class comes with a bunch of method that we can call to selectively pick and choose which data we want to be available to TensorBoard. We will add 4 types of data to tensorboard for visualization: . images | graph | scalar value | histogram | These added values are even updated and showed in real-time on tensorboard as the network trains. . It is helpful to see the loss and accuracy values over time. However, the real power of TensorBoard is its out-of-the-box capability of comparing multiple runs. This allows us to rapidly experiment by varying the hyperparameter values and comparing runs to see which parameters are working best. . One more thing to be notices, with PyTorch&#39;s SummaryWriter a run starts when the writer object instance is created and ends when the writer instance is closed or goes out of scope. . model = Network() train_dl = torch.utils.data.DataLoader(train_set, batch_size=10) optimizer = torch.optim.Adam(model.parameters(), lr=0.01) accuracy = lambda preds, lbs: (F.softmax(preds).argmax(dim=1) == lbs).sum().item() imgs, lbls = next(iter(train_dl)) grid = torchvision.utils.make_grid(imgs) tb = SummaryWriter() tb.add_image(&#39;image&#39;, grid) #1. Add a batch of images to the writer tb.add_graph(model, imgs) #2. Add graph of our model to the writer for epoch in range(3): total_loss = 0 total_acc = 0 for batch in train_dl: imgs, lbls = batch preds = model(imgs) loss = F.cross_entropy(preds, lbls) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss total_acc += accuracy(preds, lbls) nr_batch = len(train_set) #3. Add scalar values, average loss and average accuracy, to display on tensorboard over time or over epoch. tb.add_scalar(&#39;Average_Loss&#39;, total_loss/nr_batch, epoch) tb.add_scalar(&#39;Average_Accuracy&#39;, total_acc/nr_batch, epoch) #4. Add values to histograms to see its frequency distributions. tb.add_histogram(&#39;conv1.bias&#39;, model.conv1.bias, epoch) tb.add_histogram(&#39;conv1.weight&#39;, model.conv1.weight, epoch) tb.add_histogram(&#39;conv1.weight.grad&#39;, model.conv1.weight.grad, epoch) print(f&quot;epoch: {epoch} navg_loss: {total_loss/nr_batch} navg_acc : {total_acc/nr_batch}&quot;) tb.close() . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. after removing the cwd from sys.path. . epoch: 0 avg_loss: 0.06133313849568367 avg_acc : 0.7699666666666667 epoch: 1 avg_loss: 0.05296260863542557 avg_acc : 0.80595 epoch: 2 avg_loss: 0.051954615861177444 avg_acc : 0.8122833333333334 . By default, the PyTorch SummaryWriter object writes the data to disk in a directory called ./runs that is created in the current working directory. To launch TensorBoard, we have 2 ways: . run and show directly on jupyter notebook | run the tensorboard command at our terminal and open http://localhost:6006 on browser. | To be noticed, the defaut save path and open port are able to change if you want. . # 1. run and show directly on jupyter notebook %load_ext tensorboard %tensorboard --logdir=runs --port=6006 . # 2. run the tensorboard command at our terminal and open `http://localhost:6006` on browser. !tensorboard --logdir=runs . TensorFlow installation not found - running with reduced feature set. TensorBoard 1.15.0 at http://VN0130.local:6006/ (Press CTRL+C to quit) ^C . Hyperparameter Experimenting - Training Neural Networks . The best part about TensorBoard is its out-of-the-box capability of tracking our hyperparameters over time and across runs. We can use TensorBoard to rapidly experiment with different training hyperparameters comparing the results. . To uniquely identify each run, we can either set the file name of the run directly, or pass a comment string to the constructor that will be appended to the auto-generated file name. . Note the cross_entropy function with reduction=mean has averaged the loss over batch size and our total_loss is then summed up. It is correct if we have a fix batch size and its value is divisible by number of sample (if not, last batch will has less sample than others). However, in the case of batch size vary, we have to change reduction=sum. . If we have a list of parameters, we can package them up into a set for each of our runs using the Cartesian product. The Cartesian product takes multiple sets as arguments and return a set of all ordered pairs. Note that this is equivalent to nested for-loops. . from itertools import product # itertools&#39; Cartesian product implementation # define the list of parameters and their value list that we want to tolerate parameters = dict( lr = [0.1, 0.01], batch_size = [10, 100], ) param_values = [v for v in parameters.values()] # use the Cartesian product to iterate over the pairs of parameters&#39;values for lr, batch_size in product(*param_values): print(lr, batch_size) . 0.1 10 0.1 100 0.01 10 0.01 100 . accuracy = lambda preds, lbs: (F.softmax(preds).argmax(dim=1) == lbs).sum().item() model = Network() for lr, batch_size in product(*param_values): train_dl = torch.utils.data.DataLoader(train_set, batch_size=batch_size) optimizer = torch.optim.Adam(model.parameters(), lr=lr) imgs, lbls = next(iter(train_dl)) grid = torchvision.utils.make_grid(imgs) comment = f&quot; batch_size = {batch_size}, lr = {lr}&quot; tb = SummaryWriter(comment=comment) tb.add_image(&#39;images&#39;, grid) tb.add_graph(model, imgs) for epoch in range(3): total_loss = 0 total_acc = 0 for batch in train_dl: imgs, lbls = batch preds = model(imgs) loss = F.cross_entropy(preds, lbls, reduction=&#39;sum&#39;) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() total_acc += accuracy(preds, lbls) tb.add_scalar(&#39;average_loss&#39;, total_loss/len(train_set), epoch) tb.add_scalar(&#39;average_acc&#39;, total_acc/len(train_set), epoch) for name,param in model.named_parameters(): tb.add_histogram(name, param, epoch) tb.add_histogram(f&quot;{name}.grad&quot;, param.grad, epoch) print(f&quot;epoch: {epoch} navg_loss: {total_loss/len(train_set)} navg_acc : {total_acc/len(train_set)}&quot;) tb.close() . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. &#34;&#34;&#34;Entry point for launching an IPython kernel. . epoch: 0 avg_loss: 2.3510418080329893 avg_acc : 0.10213333333333334 epoch: 1 avg_loss: 2.3241496319452923 avg_acc : 0.10206666666666667 epoch: 2 avg_loss: 2.3241496464411417 avg_acc : 0.10208333333333333 epoch: 0 avg_loss: 2.31076997756958 avg_acc : 0.09978333333333333 epoch: 1 avg_loss: 2.3104461242675782 avg_acc : 0.0994 epoch: 2 avg_loss: 2.3104453076680502 avg_acc : 0.09963333333333334 epoch: 0 avg_loss: 2.3050063188870746 avg_acc : 0.10093333333333333 epoch: 1 avg_loss: 2.3049483956654866 avg_acc : 0.1014 epoch: 2 avg_loss: 2.3049483927408856 avg_acc : 0.1014 epoch: 0 avg_loss: 2.3035935297648114 avg_acc : 0.10016666666666667 epoch: 1 avg_loss: 2.303564562733968 avg_acc : 0.09968333333333333 epoch: 2 avg_loss: 2.303563890838623 avg_acc : 0.09968333333333333 . !tensorboard --logdir=runs . TensorFlow installation not found - running with reduced feature set. TensorBoard 1.15.0 at http://VN0130.local:6007/ (Press CTRL+C to quit) ^C . We can refactor the way we manage parameters by using the following RunBuilder Class. The method get_runs() will get the runs for us that it builds based on the parameters we pass in. . from collections import OrderedDict from collections import namedtuple from itertools import product class RunBuilder(): @staticmethod def get_runs(params): # create new tuple subclass called Run with named fields got from params.keys Run = namedtuple(&#39;Run&#39;, params.keys()) runs = [] # use Cartesian product to pair parameters&#39;values and pass to Run. for v in product(*params.values()): runs.append(Run(*v)) return runs params = OrderedDict( lr = [.01, .001], batch_size = [10, 100] ) runs = RunBuilder.get_runs(params) runs . [Run(lr=0.01, batch_size=10), Run(lr=0.01, batch_size=100), Run(lr=0.001, batch_size=10), Run(lr=0.001, batch_size=100)] . We can see that the RunBuilder class has built and returned a list of four runs. Each of these runs has a learning rate and a batch size that defines the run. Notice the string representation of the run output. This string representation was automatically generated for us by the Run tuple class, and this string can be used to uniquely identify the run if we want to write out run statistics to disk for TensorBoard or any other visualization program. Additionally, because the run is object is a tuple with named attributes, we can access the values using dot notation like so: . run = runs[0] run.lr, run.batch_size . (0.01, 10) . All we have to do to add additional values is to add them to the original parameter list, and if we want to add an additional type of parameter, all we have to do is add it. The new parameter and its values will automatically become available to be consumed inside the run. The string output for the run also updates as well. . This functionality will allow us to have greater control as we experiment with different values during training. . accuracy = lambda preds, lbs: (F.softmax(preds).argmax(dim=1) == lbs).sum().item() model = Network() # for lr, batch_size in product(*param_values): # old code for run in RunBuilder.get_runs(params): # Changes for RunBuilder train_dl = torch.utils.data.DataLoader(train_set, batch_size=run.batch_size) # Changes for RunBuilder optimizer = torch.optim.Adam(model.parameters(), lr=run.lr) # Changes for RunBuilder imgs, lbls = next(iter(train_dl)) grid = torchvision.utils.make_grid(imgs) # comment = f&quot; batch_size = {batch_size}, lr = {lr}&quot; # old code comment = f&#39;-{run}&#39; # Changes for RunBuilder tb = SummaryWriter(comment=comment) tb.add_image(&#39;images&#39;, grid) tb.add_graph(model, imgs) for epoch in range(3): total_loss = 0 total_acc = 0 for batch in train_dl: imgs, lbls = batch preds = model(imgs) loss = F.cross_entropy(preds, lbls, reduction=&#39;sum&#39;) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() total_acc += accuracy(preds, lbls) tb.add_scalar(&#39;average_loss&#39;, total_loss/len(train_set), epoch) tb.add_scalar(&#39;average_acc&#39;, total_acc/len(train_set), epoch) for name,param in model.named_parameters(): tb.add_histogram(name, param, epoch) tb.add_histogram(f&quot;{name}.grad&quot;, param.grad, epoch) print(f&quot;epoch: {epoch} navg_loss: {total_loss/len(train_set)} navg_acc : {total_acc/len(train_set)}&quot;) tb.close() . Speeding up the training process by increasing num_workers . To speed up the training process, we will make use of the num_workers optional attribute of the DataLoader class. By setting this number to 1, default set to 0, the speep can increase up to 20% and does not improve more or even worse with setting more than 1. . The num_workers attribute tells the data loader instance how many sub-processes to use for data loading. By default, the num_workers value is set to zero, the training process will work sequentially inside the main process. After a batch is used during the training process and another one is needed, we read the batch data from disk. Now, if we have a worker process, we can make use of the fact that our machine has multiple cores. This means that the next batch can already be loaded and ready to go by the time the main process is ready for another batch. This is where the speed up comes from. If we add more batches to the queue doesn&#39;t mean the batches are being processes faster because we are bounded by the time it takes to forward and backward propagate a given batch. . Some random tricks . Object Oriented Programming and Why Pytorch select it. . When we’re writing programs or building software, there are two key components, code and data. With object oriented programming, we orient our program design and structure around objects. Objects are defined in code using classes. A class defines the object&#39;s specification or spec, which specifies what data and code each object of the class should have. When we create an object of a class, we call the object an instance of the class, and all instances of a given class have two core components: . Methods(code) | Attributes(data) | . In a given program, many objects, a.k.a instances of a given class have the same available attributes and the same available methods. The difference between objects of the same class is the values contained within the object for each attribute. Each object has its own attribute values. These values determine the internal state of the object. The code and data of each object is said to be encapsulated within the object. . Let’s build a simple class to demonstrate how classes encapsulate data and code: . class Sample: #class declaration def __init__(self, name): #class constructor (code) self.name = name #attribute (data) def set_name(self, name): #method declaration (code) self.name = name #method implementation (code) . Let&#39;s switch gears now and look at how object oriented programming fits in with PyTorch. . The primary component we&#39;ll need to build a neural network is a layer, and so, as we might expect, PyTorch&#39;s neural network library contains classes that aid us in constructing layers. As we know, deep neural networks are built using multiple layers. This is what makes the network deep. Each layer in a neural network has two primary components: . A transformation (code) | A collection of weights (data) | . Like many things in life, this fact makes layers great candidates to be represented as objects using Object Oriented Programming - OOP. . Stack vs Concat . Concatenating joins a sequence of tensors along an existing axis, and stacking joins a sequence of tensors along a new axis. . import torch t1 = torch.tensor([1,1,1]) t2 = torch.tensor([2,2,2]) t3 = torch.tensor([3,3,3]) . # Now, let’s concatenate these with one another. Notice that each of these tensors have a single axis. This # means that the result of the cat function will also have a single axis. This is because when we concatenate, # we do it along an existing axis. Notice that in this example, the only existing axis is the first axis. # Alright, so we took three single axis tensors each having an axis length of three, and now we have a single # tensor with an axis length of nine. torch.cat((t1,t2,t3),dim=0).shape . torch.Size([9]) . # Now, let’s stack these tensors along a new axis that we’ll insert. We’ll insert an axis at the first index. # Note that this insertion will be happening implicitly under the hood by the stack function. # This gives us a new tensor that has a shape of 3 x 3. Notice how the three tensors are concatenated along # the first axis of this tensor. Note that we can also insert the new axis explicitly, and preform the # concatenation directly. torch.stack((t1,t2,t3), dim=0).shape . torch.Size([3, 3]) . # example of combining stack and concat # Joining Images With An Existing Batch # Suppose we have the same three separate image tensors. Only, this time, we already have a batch tensor. # Assume our task is to join these three separate images with the batch. import torch batch = torch.zeros(3,3,28,28) t1 = torch.zeros(3,28,28) t2 = torch.zeros(3,28,28) t3 = torch.zeros(3,28,28) torch.cat( ( batch ,torch.stack( (t1,t2,t3) ,dim=0 ) ) ,dim=0 ).shape . # Joining Batches Into A Single Batch import torch t1 = torch.zeros(1,3,28,28) t2 = torch.zeros(1,3,28,28) t3 = torch.zeros(1,3,28,28) torch.cat( (t1,t2,t3) ,dim=0 ).shape . torch.Size([3, 3, 28, 28]) . # Joining Images Into A Single Batch import torch t1 = torch.zeros(3,28,28) t2 = torch.zeros(3,28,28) t3 = torch.zeros(3,28,28) torch.stack( (t1,t2,t3) ,dim=0 ).shape . torch.Size([3, 3, 28, 28]) . Disabling PyTorch Gradient Tracking . # Get predictions for the entire training set # Note at the top, we have annotated the function using the @torch.no_grad() PyTorch decoration. # This is because we want this functions execution to omit gradient tracking. This is because gradient # tracking uses memory, and during inference (getting predictions while not training) there is no need to # keep track of the computational graph. The decoration is one way of locally turning off the gradient tracking # feature while executing specific functions. We specifically need the gradient calculation feature anytime we # are going to calculate gradients using the backward() function. Otherwise, it is a good idea to turn it off # because having it off will reduce memory consumption for computations, e.g. when we are using networks for # predicting (inference). # As another example, we can use Python&#39;s with context manger keyword to specify that a specify block of code # should exclude gradient computations. # Both of these options are valid @torch.no_grad() def get_all_preds(model, loader): all_preds = torch.tensor([]) for batch in loader: imgs, lbs = batch preds = model(imgs) all_preds = torch.cat((all_preds, preds), dim=0) return all_preds # Locally Disabling PyTorch Gradient Tracking with torch.no_grad(): prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000) train_preds = get_all_preds(model, prediction_loader) . Effective Pytorch . https://github.com/vahidk/EffectivePyTorch?fbclid=IwAR1MhsjnjccWy6dIVtibFOCZbWhLtAj5pSTobnkUDxw_gHgfEswnVzqrKQ0#torchscript . PyTorch is similar to NumPy, with the additional benefit that PyTorch allows you to perform your computations on CPUs, GPUs, and TPUs without any material change to your code. PyTorch also makes it easy to distribute your computation across multiple devices or machines. One of the most important features of PyTorch is automatic differentiation. It allows computing the gradients of your functions analytically in an efficient manner which is crucial for training machine learning models using gradient descent method | . The first thing to learn about PyTorch is the concept of Tensors. Tensors are simply multidimensional arrays. A PyTorch Tensor is very similar to a NumPy array with some magical additional functionality. | . # One of the most commonly used operations in machine learning applications is matrix multiplication import torch x = torch.randn([3,4]) y = torch.randn([4,5]) z = x@y z . tensor([[-1.9095e+00, 2.7157e+00, 3.0564e+00, -2.6597e+00, -3.5740e+00], [ 5.7195e-01, 1.1266e+00, -2.7287e+00, 1.9540e+00, 2.5402e-03], [ 2.6661e+00, 3.8373e+00, -3.7645e+00, 2.2918e-02, -1.5655e+00]]) . # add 2 tensors z = x + y . # convert between torch and numpy x.numpy() x = torch.tensor(x) . # Automatic Differentiation # The most important advantage of PyTorch over NumPy is its automatic differentiation functionality which is # very useful in optimization applications such as optimizing parameters of a neural network # Say you have a composite function which is a chain of two functions: g(u(x)). To compute the derivative of g # with respect to x we can use the chain rule which states that: dg/dx = dg/du * du/dx. PyTorch can analytically # compute the derivatives for us. # To compute the derivatives in PyTorch first we create a tensor and set its requires_grad to true. We can use # tensor operations to define our functions. We assume u is a quadratic function and g is a simple linear # function # In this case our composite function is g(u(x)) = -x*x. So its derivative with respect to x is -2x. At point # x=1, this is equal to -2. x = torch.tensor(1.0, requires_grad = True) def u(x): return x*x def g(u): return -u dgdx = torch.autograd.grad(g(u(x)), x) dgdx . (tensor(-2.),) . torch.arange(100, dtype=torch.float32) . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90., 91., 92., 93., 94., 95., 96., 97., 98., 99.]) . # There are some predefined modules that act as a container for other modules. The most commonly used container # module is torch.nn.Sequential. As its name implies it&#39;s used to to stack multiple modules (or layers) on top # of each other. For example to stack two Linear layers with a ReLU nonlinearity in between you can do: torch.nn.Sequential( torch.nn.Linear(1,2), torch.nn.ReLU(), torch.nn.Linear(2,3), ) . # optimizing runtime with TorchScript # PyTorch is optimized to perform operations on large tensors. Doing many operations on small tensors is quite # inefficient in PyTorch. So, whenever possible you should rewrite your computations in batch form to reduce # overhead and improve performance. If there&#39;s no way you can manually batch your operations, using TorchScript # may improve your code&#39;s performance. TorchScript is simply a subset of Python functions that are recognized by # PyTorch. PyTorch can automatically optimize your TorchScript code using its just in time (jit) compiler and # reduce some overheads. . idx = torch.tensor([0,1,2], dtype=torch.long) t = torch.tensor([ [1,2,3], [2,3,4], [3,4,5]], dtype=torch.float32); @torch.jit.script def batch_gather_jit(tensor, indices): # import pdb; pdb.set_trace() output = [] for i in range(tensor.size(0)): output += [tensor[i][indices[i]]] return torch.stack(output) # %%timeit # %time %time batch_gather_jit(t, idx) . CPU times: user 4.27 ms, sys: 739 µs, total: 5.01 ms Wall time: 3.99 ms . tensor([1., 3., 5.]) . idx = torch.tensor([0,1,2], dtype=torch.long) t = torch.tensor([ [1,2,3], [2,3,4], [3,4,5]], dtype=torch.float32); # @torch.jit.script def batch_gather_jit(tensor, indices): # import pdb; pdb.set_trace() output = [] for i in range(tensor.size(0)): output += [tensor[i][indices[i]]] return torch.stack(output) # %%timeit # %time %time batch_gather_jit(t, idx) . CPU times: user 237 µs, sys: 462 µs, total: 699 µs Wall time: 3.16 ms . tensor([1., 3., 5.]) . idx = torch.tensor([0,1,2], dtype=torch.long) t = torch.tensor([ [1,2,3], [2,3,4], [3,4,5]], dtype=torch.float32); def batch_gather_vec(tensor, indices): shape = list(tensor.shape) flat_first = torch.reshape( tensor, [shape[0] * shape[1]] + shape[2:]) offset = torch.reshape( torch.arange(shape[0]).cuda() * shape[1], [shape[0]] + [1] * (len(indices.shape) - 1)) output = flat_first[indices + offset] return output %time batch_gather_vec(t, idx) . CPU times: user 866 µs, sys: 2.32 ms, total: 3.19 ms Wall time: 8.97 ms . tensor([1., 3., 5.]) . # broadcasting: the good and the ugly . Reference . Some good sources: . deeplizard : https://deeplizard.com/learn/video/v5cngxo4mIg | effective pytorch - vahidk https://github.com/vahidk/EffectivePyTorch?fbclid=IwAR1MhsjnjccWy6dIVtibFOCZbWhLtAj5pSTobnkUDxw_gHgfEswnVzqrKQ0#torchscript | recommend walk with pytorch: https://forums.fast.ai/t/getting-comfortable-with-pytorch-projects/28371 | official tutorial: https://pytorch.org/tutorials/ | DL(with Pytorch): https://github.com/Atcold/pytorch-Deep-Learning | Pytorch project template: https://github.com/moemen95/PyTorch-Project-Template | nlp turorial with pytorch : https://github.com/graykode/nlp-tutorial | UDACITY course https://www.udacity.com/course/deep-learning-pytorch--ud188 | awesome pytorch list: https://github.com/bharathgs/Awesome-pytorch-list | deep learning with pytorch https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf | pytorch zero to all: https://github.com/hunkim/PyTorchZeroToAll | others: https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc | grokking book | . | .",
            "url": "https://phucnsp.github.io/blog/self-taught/tutorial/2020/03/22/self-taught-pytorch-part2-training-nn.html",
            "relUrl": "/self-taught/tutorial/2020/03/22/self-taught-pytorch-part2-training-nn.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Pytorch part 1 - basic tensor and Pytorch tensor",
            "content": "Some good sources: . deeplizard : https://deeplizard.com/learn/video/v5cngxo4mIg | effective pytorch - vahidk https://github.com/vahidk/EffectivePyTorch?fbclid=IwAR1MhsjnjccWy6dIVtibFOCZbWhLtAj5pSTobnkUDxw_gHgfEswnVzqrKQ0#torchscript | recommend walk with pytorch: https://forums.fast.ai/t/getting-comfortable-with-pytorch-projects/28371 | official tutorial: https://pytorch.org/tutorials/ | DL(with Pytorch): https://github.com/Atcold/pytorch-Deep-Learning | Pytorch project template: https://github.com/moemen95/PyTorch-Project-Template | nlp turorial with pytorch : https://github.com/graykode/nlp-tutorial | UDACITY course https://www.udacity.com/course/deep-learning-pytorch--ud188 | awesome pytorch list: https://github.com/bharathgs/Awesome-pytorch-list | deep learning with pytorch https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf | pytorch zero to all: https://github.com/hunkim/PyTorchZeroToAll | others: https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc | grokking book | . | . Neural network programming - deep learning with pytorch - deepilzard . Part 1: Pytorch and Tensors . Section 1: Introducing Pytorch . PyTorch Explained - Python Deep Learning Neural Network API | PyTorch Install - Quick and Easy | CUDA Explained - Why Deep Learning Uses GPUs | . Section 2: Introducing Tensors . Tensors Explained - Data Structures of Deep Learning | Rank, Axes, and Shape Explained - Tensors for Deep Learning | CNN Tensor Shape Explained - CNNs and Feature Maps | . Section 3: Pytorch Tensors . PyTorch Tensors Explained - Neural Network Programming | Creating PyTorch Tensors for Deep Learning - Best Options | . Section 4: Tensor Operations . Flatten, Reshape, and Squeeze Explained - Tensors for Deep Learning | CNN Flatten Operation Visualized - Tensor Batch Processing | Tensors for Deep Learning - Broadcasting and Element-wise Operations | ArgMax and Reduction Ops - Tensors for Deep Learning | . Part 2: Neural Networks and Deep learning with Pytorch . Prepare the data | Build the model | Train the model | Analyze the model&#39;s results | . Section 1: Data and Data processing . Importance of Data in Deep Learning - Fashion MNIST for AI | Extract, Transform, Load (ETL) - Deep Learning Data Preparation | PyTorch Datasets and DataLoaders - Training Set Exploration | . import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchvision import torchvision.transforms as transforms import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix #from plotcm import plot_confusion_matrix import pdb torch.set_printoptions(linewidth=120) . train_set = torchvision.datasets.FashionMNIST( root=&#39;./data&#39; ,train=True ,download=True ,transform=transforms.Compose([ transforms.ToTensor() ]) ) . train_dl = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True) xs,ys = next(iter(train_dl)) xs.shape, ys.shape . (torch.Size([4, 1, 28, 28]), torch.Size([4])) . train_set.train_labels, train_set.targets . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/torchvision/datasets/mnist.py:45: UserWarning: train_labels has been renamed targets warnings.warn(&#34;train_labels has been renamed targets&#34;) . (tensor([9, 0, 0, ..., 3, 0, 5]), tensor([9, 0, 0, ..., 3, 0, 5])) . train_set.classes . [&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;] . import matplotlib.pyplot as plt ys[0].item() plt.imshow(xs[0].squeeze(), cmap=&quot;gray&quot;) . &lt;matplotlib.image.AxesImage at 0x1a2a0ee790&gt; . grid = torchvision.utils.make_grid(xs, nrow=10) plt.figure(figsize=(15,15)) plt.imshow(grid.permute(1,2,0)) # plt.imshow(np.transpose(grid, (1,2,0))) . &lt;matplotlib.image.AxesImage at 0x1a2a383e90&gt; . Section 2: Neural Net and Pytorch design . Build PyTorch CNN - Object Oriented Neural Net- | CNN Layers - Deep Neural Network Archite- | CNN Weights - Learnable Parameters in Neural Net- | Callable Neural Networks - Linear Layers in - | CNN Forward Method - Deep Learning Implement- | Forward Propagation Explained - Pass Image to PyTorch Neural Network | Neural Network Batch Processing - Pass Image Batch to PyTorch CNN | CNN Output Size Formula - Bonus Neural Network Debugging Session | . import torch.nn as nn import torch.nn.functional as F class Network(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) self.fc2 = nn.Linear(in_features=120, out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self, x): # (1) input layer x = x # (2) hidden conv layer x = self.conv1(x) x = F.relu(x) x = F.max_pool2d(x, kernel_size=2, stride=2) # out shape - bs x 6 x 12 x 12 # (3) hidden conv layer x = self.conv2(x) x = F.relu(x) x = F.max_pool2d(x, kernel_size=2, stride=2) # out shape - bs x 12 x 4 x 4 # (4) hidden linear layer # import pdb; pdb.set_trace() x = x.flatten(start_dim=1) # x.reshape(-1, 12*4*4) x = self.fc1(x) x = F.relu(x) # (5) hidden linear layer x = self.fc2(x) x = F.relu(x) # (6) output layer x = self.out(x) # x = F.softmax(x, dim=1) return x . model = Network() model . Network( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=192, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=60, bias=True) (out): Linear(in_features=60, out_features=10, bias=True) ) . pred = model(xs) print(pred.shape) pred . torch.Size([4, 10]) . tensor([[-0.0071, 0.0308, 0.1355, 0.1304, -0.1290, 0.0521, 0.0140, -0.0722, 0.0914, -0.0045], [-0.0082, 0.0273, 0.1325, 0.1348, -0.1298, 0.0527, 0.0120, -0.0763, 0.0880, -0.0087], [-0.0101, 0.0328, 0.1360, 0.1372, -0.1294, 0.0470, 0.0095, -0.0750, 0.0896, -0.0034], [-0.0076, 0.0324, 0.1334, 0.1366, -0.1288, 0.0475, 0.0084, -0.0755, 0.0876, -0.0043]], grad_fn=&lt;AddmmBackward&gt;) . F.softmax(pred) . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. &#34;&#34;&#34;Entry point for launching an IPython kernel. . tensor([[0.1184, 0.1070, 0.0941, 0.0892, 0.0883, 0.1071, 0.0939, 0.0983, 0.1142, 0.0895], [0.1184, 0.1071, 0.0940, 0.0894, 0.0889, 0.1064, 0.0931, 0.0982, 0.1145, 0.0901], [0.1180, 0.1080, 0.0937, 0.0892, 0.0888, 0.1061, 0.0936, 0.0980, 0.1144, 0.0902], [0.1183, 0.1069, 0.0939, 0.0892, 0.0880, 0.1064, 0.0939, 0.0986, 0.1145, 0.0902]], grad_fn=&lt;SoftmaxBackward&gt;) . torch.argmax(F.softmax(pred)) . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. &#34;&#34;&#34;Entry point for launching an IPython kernel. . tensor(0) . Section 3: Training NN . We are now ready to begin the training process. The training process is an iterative process which including following steps: . Get batch from the training set. . Since we have 60,000 samples in our training set, we will have 60,000 / 100 = 600 iterations. Something to notice, batch_size will directly impact to the number of times the weights updated. In our case, the weights will be updated 600 times by the end of each loop. So far, there is no rule-of-thump for selecting the value of batch size so we still need to do trial and error to figure out the best value. | . | Pass batch to network. . | Calculate the loss (difference between the predicted values and the true values). . To be notice, the function cross_entropy has a parameter called reduction which has 3 option - none, mean, sum. By default, this variable is set to mean which will average the loss value by batch_size. None and sum means there is no reduction applied and the output will be summed, respectively. | . | Calculate the gradient of the loss function w.r.t the network&#39;s weights. . Before calculate the gradient, we need to zero out these gradients, by calling optimizer.zero_grad(), because after we call the loss.backward() method, the gradients will be calculated and added to the grad attributes of our network&#39;s parameters. | Calculating the gradients is very easy using PyTorch. Since PyTorch has created a computation graph under the hood. As our tensor flowed forward through our network, all of the computations where added to the graph. The computation graph is then used by PyTorch to calculate the gradients of the loss function with respect to the network&#39;s weights. | . | Update the weights using the gradients to reduce the loss. . The gradients calculated from step 4 are used by the optimizer to update the respective weights. | . | Repeat steps 1-5 until one epoch is completed. . | Repeat steps 1-6 for as many epochs required to reach the minimum loss. | To create our optimizer, we use the torch.optim package that has many optimization algorithm implementations that we can use, for example: Adam. The model&#39;s parameters and learning rate are passed to the optimizer. lr tells optimizer how far to step in the direction of minimizing loss fct while Parameters provide Adam the ability to access gradients. . model = Network() train_dl = torch.utils.data.DataLoader(train_set, batch_size=100) optimizer = torch.optim.Adam(model.parameters(), lr=0.01) accuracy = lambda preds, lbs: (F.softmax(preds).argmax(dim=1) == lbs).sum().item() for epoch in range(3): total_loss = 0 total_acc = 0 for batch in train_dl: images, labels = batch # 1. Get batch from the training set. preds = model(images) # 2. Pass batch to network. loss = F.cross_entropy(preds, labels) # 3. Calculate the loss optimizer.zero_grad() loss.backward() # 4. Calculate the gradient optimizer.step() # 5. Update the weights total_loss += loss.item() total_acc += accuracy(preds, labels) nr_iters_per_epoch = len(train_dl) print(f&quot;epoch: {epoch} navg_loss: {total_loss/nr_iters_per_epoch} navg_acc : {total_acc/nr_iters_per_epoch}&quot;) . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. after removing the cwd from sys.path. . epoch: 0 avg_loss: 0.6262958686550458 avg_acc : 76.13 epoch: 1 avg_loss: 0.4225387443850438 avg_acc : 84.28333333333333 epoch: 2 avg_loss: 0.37975253333648046 avg_acc : 85.75166666666667 . Section 4: Analyze the model&#39;s results . Analyse classification result using a Confusion Matrix . The confusion matrix will show us which categories the model is predicting correctly and which categories the model is predicting incorrectly. For the incorrect predictions, this will show us which categories are confusing the model. . Now, if we compare the two tensors element-wise and count the number of predicted labels vs the target labels, the values inside the two tensors act as coordinates for our matrix. Let&#39;s stack these two tensors along the second dimension so we can have 60,000 ordered pairs. . len(train_set), len(train_set.targets) . (60000, 60000) . # Get predictions for the entire training set @torch.no_grad() def get_all_preds(model, loader): all_preds = torch.tensor([]) for batch in loader: imgs, lbs = batch preds = model(imgs) all_preds = torch.cat((all_preds, preds), dim=0) return all_preds with torch.no_grad(): prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000) train_preds = get_all_preds(model, prediction_loader) stacked = torch.stack( ( train_set.targets, train_preds.argmax(dim=1) ), dim=1) stacked.shape . torch.Size([60000, 2]) . # method 1: manually calculate confusion matrix cmt = torch.zeros(10,10, dtype=torch.int64) for p in stacked: tl, pl = p.tolist() cmt[tl,pl] += 1 cmt . tensor([[5523, 1, 46, 59, 15, 2, 306, 0, 48, 0], [ 36, 5829, 3, 77, 15, 0, 32, 0, 8, 0], [ 144, 6, 3704, 51, 1359, 3, 603, 0, 129, 1], [ 554, 26, 2, 5077, 189, 0, 135, 0, 15, 2], [ 49, 3, 99, 161, 5298, 1, 316, 0, 72, 1], [ 13, 0, 0, 0, 0, 5671, 0, 178, 81, 57], [1443, 10, 244, 75, 693, 0, 3414, 0, 121, 0], [ 1, 0, 0, 0, 0, 38, 0, 5823, 19, 119], [ 45, 0, 4, 8, 17, 1, 34, 4, 5884, 3], [ 0, 0, 0, 0, 0, 23, 0, 261, 6, 5710]]) . # method 2: use sklearn import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix cm = confusion_matrix(train_set.targets, train_preds.argmax(dim=1)) cm . array([[5523, 1, 46, 59, 15, 2, 306, 0, 48, 0], [ 36, 5829, 3, 77, 15, 0, 32, 0, 8, 0], [ 144, 6, 3704, 51, 1359, 3, 603, 0, 129, 1], [ 554, 26, 2, 5077, 189, 0, 135, 0, 15, 2], [ 49, 3, 99, 161, 5298, 1, 316, 0, 72, 1], [ 13, 0, 0, 0, 0, 5671, 0, 178, 81, 57], [1443, 10, 244, 75, 693, 0, 3414, 0, 121, 0], [ 1, 0, 0, 0, 0, 38, 0, 5823, 19, 119], [ 45, 0, 4, 8, 17, 1, 34, 4, 5884, 3], [ 0, 0, 0, 0, 0, 23, 0, 261, 6, 5710]]) . To nicely plot the confusion matrix, we can use below util function. . import itertools import numpy as np import matplotlib.pyplot as plt def plot_confusion_matrix(cm, classes, normalize=False, title=&#39;Confusion matrix&#39;, cmap=plt.cm.Blues): if normalize: cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] print(&quot;Normalized confusion matrix&quot;) else: print(&#39;Confusion matrix, without normalization&#39;) print(cm) plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = &#39;.2f&#39; if normalize else &#39;d&#39; thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) . names = ( &#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39; ) plt.figure(figsize=(10,10)) plot_confusion_matrix(cm, names) . Confusion matrix, without normalization [[5523 1 46 59 15 2 306 0 48 0] [ 36 5829 3 77 15 0 32 0 8 0] [ 144 6 3704 51 1359 3 603 0 129 1] [ 554 26 2 5077 189 0 135 0 15 2] [ 49 3 99 161 5298 1 316 0 72 1] [ 13 0 0 0 0 5671 0 178 81 57] [1443 10 244 75 693 0 3414 0 121 0] [ 1 0 0 0 0 38 0 5823 19 119] [ 45 0 4 8 17 1 34 4 5884 3] [ 0 0 0 0 0 23 0 261 6 5710]] . The confusion matrix has three axes: . Prediction label (class) | True label | Heat map value (color) | . The prediction label and true labels show us which prediction class we are dealing with. The matrix diagonal represents locations in the matrix where the prediction and the truth are the same, so this is where we want the heat map to be darker. . Any values that are not on the diagonal are incorrect predictions because the prediction and the true label don&#39;t match. To read the plot, we can use these steps: . Choose a prediction label on the horizontal axis. | Check the diagonal location for this label to see the total number correct. | Check the other non-diagonal locations to see where the network is confused. | For example, the network is confusing a T-shirt/top with a shirt, but is not confusing the T-shirt/top with things like: Ankle boot, Sneaker, Sandal. If we think about it, this makes pretty good sense. As our model learns, we will see the numbers that lie outside the diagonal become smaller and smaller. . Analyse training loop by using TensorBoard with PyTorch . Tensorboard provides visualization and tooling needed for machine learning experimentation. With tensorboard, we can: . Tracking and visualizing metrics such as loss and accuracy | Visualizing the model graph (ops and layers) | Viewing histograms of weights, biases, or other tensors as they change over time | Projecting embeddings to a lower dimensional space | Displaying images, text, and audio data | Profiling TensorFlow programs | And much more | . It is a font-end web interface that essentially reads data from a file and displays it. PyTorch has created a utility class called SummaryWriter which will help us get the data out of our program, save on disk so as to tensorboard can read. To use tensorboard, PyTorch version 1.1.0 is required. . import torch print(f&quot; torch version: {torch.__version__}&quot;) . torch version: 1.4.0 . !pip install tensorboard==1.15.0; from torch.utils.tensorboard import SummaryWriter . Requirement already satisfied: tensorboard==1.15.0 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (1.15.0) Requirement already satisfied: six&gt;=1.10.0 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (1.14.0) Requirement already satisfied: markdown&gt;=2.6.8 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (3.2.1) Requirement already satisfied: absl-py&gt;=0.4 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (0.9.0) Requirement already satisfied: protobuf&gt;=3.6.0 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (3.11.3) Requirement already satisfied: numpy&gt;=1.12.0 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (1.18.1) Requirement already satisfied: setuptools&gt;=41.0.0 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (45.2.0.post20200210) Requirement already satisfied: grpcio&gt;=1.6.3 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (1.28.1) Requirement already satisfied: werkzeug&gt;=0.11.15 in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (1.0.1) Requirement already satisfied: wheel&gt;=0.26; python_version &gt;= &#34;3&#34; in /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages (from tensorboard==1.15.0) (0.34.2) . The SummaryWriter class comes with a bunch of method that we can call to selectively pick and choose which data we want to be available to TensorBoard. We will add 4 types of data to tensorboard for visualization: . images | graph | scalar value | histogram | These added values are even updated and showed in real-time on tensorboard as the network trains. . It is helpful to see the loss and accuracy values over time. However, the real power of TensorBoard is its out-of-the-box capability of comparing multiple runs. This allows us to rapidly experiment by varying the hyperparameter values and comparing runs to see which parameters are working best. . One more thing to be notices, with PyTorch&#39;s SummaryWriter a run starts when the writer object instance is created and ends when the writer instance is closed or goes out of scope. . model = Network() train_dl = torch.utils.data.DataLoader(train_set, batch_size=10) optimizer = torch.optim.Adam(model.parameters(), lr=0.01) accuracy = lambda preds, lbs: (F.softmax(preds).argmax(dim=1) == lbs).sum().item() imgs, lbls = next(iter(train_dl)) grid = torchvision.utils.make_grid(imgs) tb = SummaryWriter() tb.add_image(&#39;image&#39;, grid) #1. Add a batch of images to the writer tb.add_graph(model, imgs) #2. Add graph of our model to the writer for epoch in range(3): total_loss = 0 total_acc = 0 for batch in train_dl: imgs, lbls = batch preds = model(imgs) loss = F.cross_entropy(preds, lbls) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss total_acc += accuracy(preds, lbls) nr_batch = len(train_set) #3. Add scalar values, average loss and average accuracy, to display on tensorboard over time or over epoch. tb.add_scalar(&#39;Average_Loss&#39;, total_loss/nr_batch, epoch) tb.add_scalar(&#39;Average_Accuracy&#39;, total_acc/nr_batch, epoch) #4. Add values to histograms to see its frequency distributions. tb.add_histogram(&#39;conv1.bias&#39;, model.conv1.bias, epoch) tb.add_histogram(&#39;conv1.weight&#39;, model.conv1.weight, epoch) tb.add_histogram(&#39;conv1.weight.grad&#39;, model.conv1.weight.grad, epoch) print(f&quot;epoch: {epoch} navg_loss: {total_loss/nr_batch} navg_acc : {total_acc/nr_batch}&quot;) tb.close() . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. after removing the cwd from sys.path. . epoch: 0 avg_loss: 0.06133313849568367 avg_acc : 0.7699666666666667 epoch: 1 avg_loss: 0.05296260863542557 avg_acc : 0.80595 epoch: 2 avg_loss: 0.051954615861177444 avg_acc : 0.8122833333333334 . By default, the PyTorch SummaryWriter object writes the data to disk in a directory called ./runs that is created in the current working directory. To launch TensorBoard, we have 2 ways: . run and show directly on jupyter notebook | run the tensorboard command at our terminal and open http://localhost:6006 on browser. | To be noticed, the defaut save path and open port are able to change if you want. . # 1. run and show directly on jupyter notebook %load_ext tensorboard %tensorboard --logdir=runs --port=6006 . # 2. run the tensorboard command at our terminal and open `http://localhost:6006` on browser. !tensorboard --logdir=runs . TensorFlow installation not found - running with reduced feature set. TensorBoard 1.15.0 at http://VN0130.local:6006/ (Press CTRL+C to quit) ^C . Hyperparameter Experimenting - Training Neural Networks . The best part about TensorBoard is its out-of-the-box capability of tracking our hyperparameters over time and across runs. We can use TensorBoard to rapidly experiment with different training hyperparameters comparing the results. . To uniquely identify each run, we can either set the file name of the run directly, or pass a comment string to the constructor that will be appended to the auto-generated file name. . Note the cross_entropy function with reduction=mean has averaged the loss over batch size and our total_loss is then summed up. It is correct if we have a fix batch size and its value is divisible by number of sample (if not, last batch will has less sample than others). However, in the case of batch size vary, we have to change reduction=sum. . If we have a list of parameters, we can package them up into a set for each of our runs using the Cartesian product. The Cartesian product takes multiple sets as arguments and return a set of all ordered pairs. Note that this is equivalent to nested for-loops. . from itertools import product # itertools&#39; Cartesian product implementation # define the list of parameters and their value list that we want to tolerate parameters = dict( lr = [0.1, 0.01], batch_size = [10, 100], ) param_values = [v for v in parameters.values()] # use the Cartesian product to iterate over the pairs of parameters&#39;values for lr, batch_size in product(*param_values): print(lr, batch_size) . 0.1 10 0.1 100 0.01 10 0.01 100 . accuracy = lambda preds, lbs: (F.softmax(preds).argmax(dim=1) == lbs).sum().item() model = Network() for lr, batch_size in product(*param_values): train_dl = torch.utils.data.DataLoader(train_set, batch_size=batch_size) optimizer = torch.optim.Adam(model.parameters(), lr=lr) imgs, lbls = next(iter(train_dl)) grid = torchvision.utils.make_grid(imgs) comment = f&quot; batch_size = {batch_size}, lr = {lr}&quot; tb = SummaryWriter(comment=comment) tb.add_image(&#39;images&#39;, grid) tb.add_graph(model, imgs) for epoch in range(3): total_loss = 0 total_acc = 0 for batch in train_dl: imgs, lbls = batch preds = model(imgs) loss = F.cross_entropy(preds, lbls, reduction=&#39;sum&#39;) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() total_acc += accuracy(preds, lbls) tb.add_scalar(&#39;average_loss&#39;, total_loss/len(train_set), epoch) tb.add_scalar(&#39;average_acc&#39;, total_acc/len(train_set), epoch) for name,param in model.named_parameters(): tb.add_histogram(name, param, epoch) tb.add_histogram(f&quot;{name}.grad&quot;, param.grad, epoch) print(f&quot;epoch: {epoch} navg_loss: {total_loss/len(train_set)} navg_acc : {total_acc/len(train_set)}&quot;) tb.close() . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. &#34;&#34;&#34;Entry point for launching an IPython kernel. . epoch: 0 avg_loss: 2.3510418080329893 avg_acc : 0.10213333333333334 epoch: 1 avg_loss: 2.3241496319452923 avg_acc : 0.10206666666666667 epoch: 2 avg_loss: 2.3241496464411417 avg_acc : 0.10208333333333333 epoch: 0 avg_loss: 2.31076997756958 avg_acc : 0.09978333333333333 epoch: 1 avg_loss: 2.3104461242675782 avg_acc : 0.0994 epoch: 2 avg_loss: 2.3104453076680502 avg_acc : 0.09963333333333334 epoch: 0 avg_loss: 2.3050063188870746 avg_acc : 0.10093333333333333 epoch: 1 avg_loss: 2.3049483956654866 avg_acc : 0.1014 epoch: 2 avg_loss: 2.3049483927408856 avg_acc : 0.1014 epoch: 0 avg_loss: 2.3035935297648114 avg_acc : 0.10016666666666667 epoch: 1 avg_loss: 2.303564562733968 avg_acc : 0.09968333333333333 epoch: 2 avg_loss: 2.303563890838623 avg_acc : 0.09968333333333333 . !tensorboard --logdir=runs . TensorFlow installation not found - running with reduced feature set. TensorBoard 1.15.0 at http://VN0130.local:6007/ (Press CTRL+C to quit) ^C . We can refactor the way we manage parameters by using the following RunBuilder Class. The method get_runs() will get the runs for us that it builds based on the parameters we pass in. . from collections import OrderedDict from collections import namedtuple from itertools import product class RunBuilder(): @staticmethod def get_runs(params): # create new tuple subclass called Run with named fields got from params.keys Run = namedtuple(&#39;Run&#39;, params.keys()) runs = [] # use Cartesian product to pair parameters&#39;values and pass to Run. for v in product(*params.values()): runs.append(Run(*v)) return runs params = OrderedDict( lr = [.01, .001], batch_size = [10, 100] ) runs = RunBuilder.get_runs(params) runs . [Run(lr=0.01, batch_size=10), Run(lr=0.01, batch_size=100), Run(lr=0.001, batch_size=10), Run(lr=0.001, batch_size=100)] . We can see that the RunBuilder class has built and returned a list of four runs. Each of these runs has a learning rate and a batch size that defines the run. Notice the string representation of the run output. This string representation was automatically generated for us by the Run tuple class, and this string can be used to uniquely identify the run if we want to write out run statistics to disk for TensorBoard or any other visualization program. Additionally, because the run is object is a tuple with named attributes, we can access the values using dot notation like so: . run = runs[0] run.lr, run.batch_size . (0.01, 10) . All we have to do to add additional values is to add them to the original parameter list, and if we want to add an additional type of parameter, all we have to do is add it. The new parameter and its values will automatically become available to be consumed inside the run. The string output for the run also updates as well. . This functionality will allow us to have greater control as we experiment with different values during training. . accuracy = lambda preds, lbs: (F.softmax(preds).argmax(dim=1) == lbs).sum().item() model = Network() # for lr, batch_size in product(*param_values): # old code for run in RunBuilder.get_runs(params): # Changes for RunBuilder train_dl = torch.utils.data.DataLoader(train_set, batch_size=run.batch_size) # Changes for RunBuilder optimizer = torch.optim.Adam(model.parameters(), lr=run.lr) # Changes for RunBuilder imgs, lbls = next(iter(train_dl)) grid = torchvision.utils.make_grid(imgs) # comment = f&quot; batch_size = {batch_size}, lr = {lr}&quot; # old code comment = f&#39;-{run}&#39; # Changes for RunBuilder tb = SummaryWriter(comment=comment) tb.add_image(&#39;images&#39;, grid) tb.add_graph(model, imgs) for epoch in range(3): total_loss = 0 total_acc = 0 for batch in train_dl: imgs, lbls = batch preds = model(imgs) loss = F.cross_entropy(preds, lbls, reduction=&#39;sum&#39;) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() total_acc += accuracy(preds, lbls) tb.add_scalar(&#39;average_loss&#39;, total_loss/len(train_set), epoch) tb.add_scalar(&#39;average_acc&#39;, total_acc/len(train_set), epoch) for name,param in model.named_parameters(): tb.add_histogram(name, param, epoch) tb.add_histogram(f&quot;{name}.grad&quot;, param.grad, epoch) print(f&quot;epoch: {epoch} navg_loss: {total_loss/len(train_set)} navg_acc : {total_acc/len(train_set)}&quot;) tb.close() . Speeding up the training process by increasing num_workers . To speed up the training process, we will make use of the num_workers optional attribute of the DataLoader class. By setting this number to 1, default set to 0, the speep can increase up to 20% and does not improve more or even worse with setting more than 1. . The num_workers attribute tells the data loader instance how many sub-processes to use for data loading. By default, the num_workers value is set to zero, the training process will work sequentially inside the main process. After a batch is used during the training process and another one is needed, we read the batch data from disk. Now, if we have a worker process, we can make use of the fact that our machine has multiple cores. This means that the next batch can already be loaded and ready to go by the time the main process is ready for another batch. This is where the speed up comes from. If we add more batches to the queue doesn&#39;t mean the batches are being processes faster because we are bounded by the time it takes to forward and backward propagate a given batch. . Stack vs Concat . Concatenating joins a sequence of tensors along an existing axis, and stacking joins a sequence of tensors along a new axis. . import torch t1 = torch.tensor([1,1,1]) t2 = torch.tensor([2,2,2]) t3 = torch.tensor([3,3,3]) . # Now, let’s concatenate these with one another. Notice that each of these tensors have a single axis. This # means that the result of the cat function will also have a single axis. This is because when we concatenate, # we do it along an existing axis. Notice that in this example, the only existing axis is the first axis. # Alright, so we took three single axis tensors each having an axis length of three, and now we have a single # tensor with an axis length of nine. torch.cat((t1,t2,t3),dim=0).shape . torch.Size([9]) . # Now, let’s stack these tensors along a new axis that we’ll insert. We’ll insert an axis at the first index. # Note that this insertion will be happening implicitly under the hood by the stack function. # This gives us a new tensor that has a shape of 3 x 3. Notice how the three tensors are concatenated along # the first axis of this tensor. Note that we can also insert the new axis explicitly, and preform the # concatenation directly. torch.stack((t1,t2,t3), dim=0).shape . torch.Size([3, 3]) . # example of combining stack and concat # Joining Images With An Existing Batch # Suppose we have the same three separate image tensors. Only, this time, we already have a batch tensor. # Assume our task is to join these three separate images with the batch. import torch batch = torch.zeros(3,3,28,28) t1 = torch.zeros(3,28,28) t2 = torch.zeros(3,28,28) t3 = torch.zeros(3,28,28) torch.cat( ( batch ,torch.stack( (t1,t2,t3) ,dim=0 ) ) ,dim=0 ).shape . # Joining Batches Into A Single Batch import torch t1 = torch.zeros(1,3,28,28) t2 = torch.zeros(1,3,28,28) t3 = torch.zeros(1,3,28,28) torch.cat( (t1,t2,t3) ,dim=0 ).shape . torch.Size([3, 3, 28, 28]) . # Joining Images Into A Single Batch import torch t1 = torch.zeros(3,28,28) t2 = torch.zeros(3,28,28) t3 = torch.zeros(3,28,28) torch.stack( (t1,t2,t3) ,dim=0 ).shape . torch.Size([3, 3, 28, 28]) . Disabling PyTorch Gradient Tracking . # Get predictions for the entire training set # Note at the top, we have annotated the function using the @torch.no_grad() PyTorch decoration. # This is because we want this functions execution to omit gradient tracking. This is because gradient # tracking uses memory, and during inference (getting predictions while not training) there is no need to # keep track of the computational graph. The decoration is one way of locally turning off the gradient tracking # feature while executing specific functions. We specifically need the gradient calculation feature anytime we # are going to calculate gradients using the backward() function. Otherwise, it is a good idea to turn it off # because having it off will reduce memory consumption for computations, e.g. when we are using networks for # predicting (inference). # As another example, we can use Python&#39;s with context manger keyword to specify that a specify block of code # should exclude gradient computations. # Both of these options are valid @torch.no_grad() def get_all_preds(model, loader): all_preds = torch.tensor([]) for batch in loader: imgs, lbs = batch preds = model(imgs) all_preds = torch.cat((all_preds, preds), dim=0) return all_preds # Locally Disabling PyTorch Gradient Tracking with torch.no_grad(): prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000) train_preds = get_all_preds(model, prediction_loader) . Effective Pytorch . https://github.com/vahidk/EffectivePyTorch?fbclid=IwAR1MhsjnjccWy6dIVtibFOCZbWhLtAj5pSTobnkUDxw_gHgfEswnVzqrKQ0#torchscript . PyTorch is similar to NumPy, with the additional benefit that PyTorch allows you to perform your computations on CPUs, GPUs, and TPUs without any material change to your code. PyTorch also makes it easy to distribute your computation across multiple devices or machines. One of the most important features of PyTorch is automatic differentiation. It allows computing the gradients of your functions analytically in an efficient manner which is crucial for training machine learning models using gradient descent method | . The first thing to learn about PyTorch is the concept of Tensors. Tensors are simply multidimensional arrays. A PyTorch Tensor is very similar to a NumPy array with some magical additional functionality. | . # One of the most commonly used operations in machine learning applications is matrix multiplication import torch x = torch.randn([3,4]) y = torch.randn([4,5]) z = x@y z . tensor([[-1.9095e+00, 2.7157e+00, 3.0564e+00, -2.6597e+00, -3.5740e+00], [ 5.7195e-01, 1.1266e+00, -2.7287e+00, 1.9540e+00, 2.5402e-03], [ 2.6661e+00, 3.8373e+00, -3.7645e+00, 2.2918e-02, -1.5655e+00]]) . # add 2 tensors z = x + y . # convert between torch and numpy x.numpy() x = torch.tensor(x) . # Automatic Differentiation # The most important advantage of PyTorch over NumPy is its automatic differentiation functionality which is # very useful in optimization applications such as optimizing parameters of a neural network # Say you have a composite function which is a chain of two functions: g(u(x)). To compute the derivative of g # with respect to x we can use the chain rule which states that: dg/dx = dg/du * du/dx. PyTorch can analytically # compute the derivatives for us. # To compute the derivatives in PyTorch first we create a tensor and set its requires_grad to true. We can use # tensor operations to define our functions. We assume u is a quadratic function and g is a simple linear # function # In this case our composite function is g(u(x)) = -x*x. So its derivative with respect to x is -2x. At point # x=1, this is equal to -2. x = torch.tensor(1.0, requires_grad = True) def u(x): return x*x def g(u): return -u dgdx = torch.autograd.grad(g(u(x)), x) dgdx . (tensor(-2.),) . torch.arange(100, dtype=torch.float32) . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90., 91., 92., 93., 94., 95., 96., 97., 98., 99.]) . # There are some predefined modules that act as a container for other modules. The most commonly used container # module is torch.nn.Sequential. As its name implies it&#39;s used to to stack multiple modules (or layers) on top # of each other. For example to stack two Linear layers with a ReLU nonlinearity in between you can do: torch.nn.Sequential( torch.nn.Linear(1,2), torch.nn.ReLU(), torch.nn.Linear(2,3), ) . # optimizing runtime with TorchScript # PyTorch is optimized to perform operations on large tensors. Doing many operations on small tensors is quite # inefficient in PyTorch. So, whenever possible you should rewrite your computations in batch form to reduce # overhead and improve performance. If there&#39;s no way you can manually batch your operations, using TorchScript # may improve your code&#39;s performance. TorchScript is simply a subset of Python functions that are recognized by # PyTorch. PyTorch can automatically optimize your TorchScript code using its just in time (jit) compiler and # reduce some overheads. . idx = torch.tensor([0,1,2], dtype=torch.long) t = torch.tensor([ [1,2,3], [2,3,4], [3,4,5]], dtype=torch.float32); @torch.jit.script def batch_gather_jit(tensor, indices): # import pdb; pdb.set_trace() output = [] for i in range(tensor.size(0)): output += [tensor[i][indices[i]]] return torch.stack(output) # %%timeit # %time %time batch_gather_jit(t, idx) . CPU times: user 4.27 ms, sys: 739 µs, total: 5.01 ms Wall time: 3.99 ms . tensor([1., 3., 5.]) . idx = torch.tensor([0,1,2], dtype=torch.long) t = torch.tensor([ [1,2,3], [2,3,4], [3,4,5]], dtype=torch.float32); # @torch.jit.script def batch_gather_jit(tensor, indices): # import pdb; pdb.set_trace() output = [] for i in range(tensor.size(0)): output += [tensor[i][indices[i]]] return torch.stack(output) # %%timeit # %time %time batch_gather_jit(t, idx) . CPU times: user 237 µs, sys: 462 µs, total: 699 µs Wall time: 3.16 ms . tensor([1., 3., 5.]) . idx = torch.tensor([0,1,2], dtype=torch.long) t = torch.tensor([ [1,2,3], [2,3,4], [3,4,5]], dtype=torch.float32); def batch_gather_vec(tensor, indices): shape = list(tensor.shape) flat_first = torch.reshape( tensor, [shape[0] * shape[1]] + shape[2:]) offset = torch.reshape( torch.arange(shape[0]).cuda() * shape[1], [shape[0]] + [1] * (len(indices.shape) - 1)) output = flat_first[indices + offset] return output %time batch_gather_vec(t, idx) . CPU times: user 866 µs, sys: 2.32 ms, total: 3.19 ms Wall time: 8.97 ms . tensor([1., 3., 5.]) . # broadcasting: the good and the ugly .",
            "url": "https://phucnsp.github.io/blog/self-taught/2020/03/22/self-taught-pytorch-part1-tensor.html",
            "relUrl": "/self-taught/2020/03/22/self-taught-pytorch-part1-tensor.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "This notebook tracks my progress in learning nlp",
            "content": "fastai nlp course . video 11 . nn.Embedding(len_vocab, len_vector_embedding) . like a matrix with nr_row = nr of words in vocab and nr_column = len_vector_represent_each_word = nr_features. This matrix can be imported from other network or can be learned from training. It is used to placed at the start of language-related networks to represent words input. | references: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/word_embeddings_tutorial.ipynb#scrollTo=rFc2HSo8SKo9 | . | nn.Linear . Parameter(tensor) | . | nn.init.kaiminguniform this funciton implements the initialization recommendation from the sound paper Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | https://towardsdatascience.com/understand-kaiming-initialization-and-implementation-detail-in-pytorch-f7aa967e9138 | | . | . Why do we need a good initialization? . . https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/ https://medium.com/a-paper-a-day-will-have-you-screaming-hurray/day-8-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification-f449a886e604 . trace of matrix: deep learning page 44 | expectation, variance and covariance: deep learning page 58 | .",
            "url": "https://phucnsp.github.io/blog/self-learning/2020/03/22/self-taught-nlp.html",
            "relUrl": "/self-learning/2020/03/22/self-taught-nlp.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Self-taught NLP",
            "content": "fastai nlp course . video 11 . nn.Embedding(len_vocab, len_vector_embedding) . like a matrix with nr_row = nr of words in vocab and nr_column = len_vector_represent_each_word = nr_features. This matrix can be imported from other network or can be learned from training. It is used to placed at the start of language-related networks to represent words input. | references: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/word_embeddings_tutorial.ipynb#scrollTo=rFc2HSo8SKo9 | . | nn.Linear . Parameter(tensor) | . | nn.init.kaiminguniform this funciton implements the initialization recommendation from the sound paper Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | https://towardsdatascience.com/understand-kaiming-initialization-and-implementation-detail-in-pytorch-f7aa967e9138 | | . | . Exlain Kaiming initialization . please see the link .",
            "url": "https://phucnsp.github.io/blog/self-learning/2020/03/22/nlp-course.html",
            "relUrl": "/self-learning/2020/03/22/nlp-course.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Explain Kaiming Initialization",
            "content": "nn.init.kaiminguniform this funciton implements the initialization recommendation from the sound paper Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | https://towardsdatascience.com/understand-kaiming-initialization-and-implementation-detail-in-pytorch-f7aa967e9138 | . | . Firstly, we take a brief look on notation of a standard neural network. The image below shows notation of 4 layer neural network. . The initialialization method proposed in this paper was to tackle the problem of hard convergence with randomly initialized weight drawn from Gaussian distribution. Earlier, there was another paper ,Xavier initialization, which also tackled this problem but they only considered linear layer and did not consider non-linear layer. . I will go along part 2.2 in the original paper,Delving Deep in Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, with some explanations in detail either in math or code. . The central idea is to investigate the impact of initialization in the variance of responses in each layer. . Forward Propagation Case . For each layer, the response is: begin{equation} mathbf{y}_{l}= mathrm{W}_{l} mathbf{x}_{l}+ mathbf{b}_{l} label{eq1} tag{1} end{equation} . begin{equation} mathbf{x}_{l}=f left( mathbf{y}_{l-1} right) label{eq2} tag{2} end{equation} To be consistent with the paper, `x` is used instead of `a` - response after activation fucntion. . A few assumption about the initialization: . Initialized elements of $ mathrm{W}_{l}$ and $ mathrm{x}_{l}$ are mutually independent and share the same distribution. | $ mathrm{W}_{l}$ and $ mathrm{x}_{l}$ are independent each other. | . Now, let do some transformation from the function ref{eq1}, ref{eq2} . Given $ mathrm{y&#39;}_{l}$, $ mathrm{x&#39;}_{l}$, $ mathrm{w&#39;}_{l}$ is the random variables of each element in $ mathrm{y}_{l}$, $ mathrm{x}_{l}$ and $ mathrm{W}_{l}$ respectively. Then we have: . begin{equation} begin{aligned} operatorname{Var} left[y&#39;_{l} right] &amp;= operatorname{Var} sum_{1}^{n_{l}} left(w&#39;_{l} x&#39;_{l} right) &amp;= sum_{1}^{n_{l}} operatorname{Var} left[w&#39;_{l} x&#39;_{l} right] &amp;= n_{l} operatorname{Var} left[w&#39;_{l} x&#39;_{l} right] end{aligned} label{eq3} tag{3} end{equation}Intuitively explaination: . for each node, such as node 1 in layer 2, its value will be sum of product of x and w. Therefore, the variance of y will be variance of sum of those products. Because all the w and x follow the same distribution (respectively), I am using a common notation $ mathrm{w&#39;}_{l}$$ mathrm{x&#39;}_{l}$ which represent those products. | $ mathrm{n}_{l-1}$ represent the number of product between $ mathrm{w&#39;}_{l}$ and $ mathrm{x&#39;}_{l}$ | Bias term $ mathrm{b}$ is ignore because it usually is initialized with a constant value, so it variance is 0. | Because w and x are independent, so equation 3 can be transformed to equation 4. | . begin{equation} begin{aligned} operatorname{Var} left[y&#39;_{l} right] &amp;= n_{l} operatorname{Var} left[w&#39;_{l} x&#39;_{l} right] &amp;= n_{l}( underbrace{ mathbb{E} left[{w&#39;}_{l}^{2} right]}_{= operatorname{Var} left[w&#39;_{l} right]} mathbb{E} left[{x&#39;}_{l}^{2} right]- underbrace{ mathbb{E} left[w&#39;_{l} right]^{2}}_{=0} mathbb{E} left[x&#39;_{l} right]^{2}) &amp;=n_{l} operatorname{Var} left[w&#39;_{l} right] mathbb{E} left[{x&#39;}_{l}^{2} right] end{aligned} label{eq4} tag{4} end{equation}Intuitively explaination: . assuming 2 random variables are independent, we can derive line 1 into line 2 by applying formular wiki formular variance . | By assuming random variable $ mathrm{w}_{l}$ has zero mean, we have: begin{equation} begin{aligned} operatorname{Var} left[w&#39;_{l} right] &amp;= mathbb{E} left[w&#39;^{2} right]- mathbb{E}[w&#39;]^{2} &amp;= mathbb{E} left[w&#39;^{2} right] end{aligned} label{eq5} tag{5} end{equation} | . But $ mathbb{E}[x&#39;]^{2} neq operatorname{Var} left[x&#39; right]$ because $ mathrm{E}[x&#39;]$ does not have zero mean, it is the result of ReLU function, $x_{l}= max left(0, y_{l-1} right)$, from previous layer. . begin{equation} begin{aligned} mathbb{E} left[{x&#39;}_{l}^{2} right] &amp;= mathbb{E} left[ max left(0, y&#39;_{l-1} right)^{2} right] &amp;= frac{1}{2} mathbb{E} left[{y&#39;}_{l-1}^{2} right] &amp;= frac{1}{2} operatorname{Var} left[y&#39;_{l-1} right] end{aligned} label{eq6} tag{6} end{equation}Intuitively explaination: . Assuming $ mathrm{w}_{l-1}$ has a symmetric distribution around 0 and $ mathrm{b}_{l-1}$ = 0 then $ mathrm{y}_{l-1}$ has zero mean and symmetric distribution around 0 =&gt; that&#39;s why we can derive equation ref{eq6}. | . begin{equation} begin{aligned} mathbb{E} left(y_{l-1} right) &amp;= mathbb{E} left(w_{l-1} x_{l-1} right) &amp;= mathbb{E} left(w_{l-1} right) mathbb{E} left(x_{l-1} right) &amp;=0 end{aligned} label{eq7} tag{7} end{equation} begin{equation} begin{aligned} mathbb{P} left(y_{l-1}&gt;0 right) &amp;= mathbb{P} left(w_{l-1} x_{l-1}&gt;0 right) &amp;= mathbb{P} left( left(w_{l-1}&gt;0 text { and } x_{l-1}&gt;0 right) text { or } left(w_{l-1}&lt;0 text { and } x_{l-1}&lt;0 right) right) &amp;= mathbb{P} left(w_{l-1}&gt;0 right) mathbb{P} left(x_{l-1}&gt;0 right)+ mathbb{P} left(w_{l-1}&lt;0 right) mathbb{P} left(x_{l-1}&lt;0 right) &amp;= frac{1}{2} mathbb{P} left(x_{l-1}&gt;0 right)+ frac{1}{2} mathbb{P} left(x_{l-1}&lt;0 right) &amp;= frac{1}{2} end{aligned} label{eq8} tag{8} end{equation}Plugging back to equation ref{eq4} we have: begin{equation} operatorname{Var} left[y_{l} right]= frac{1}{2} n_{l} operatorname{Var} left[w_{l} right] operatorname{Var} left[y_{l-1} right] label{eq9} tag{9} end{equation} . With L layers put together, we have: begin{equation} operatorname{Var} left[y_{L} right]= operatorname{Var} left[y_{1} right] left( prod_{l=2}^{L} frac{1}{2} n_{l} operatorname{Var} left[w_{l} right] right) label{eq10} tag{10} end{equation} . From here, the layer part on paper is quite clear, equation ref{eq10} is the key to the initialization design. A proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. So we expect the above product to take a proper scalar, eg 1. A sufficient condition is: begin{equation} frac{1}{2} n_{l} operatorname{Var} left[w_{l} right]=1, quad forall l label{eq11} tag{11} end{equation} . begin{equation} begin{aligned} Rightarrow operatorname{Var} left[w_{l} right] = frac{2}{n_{l}} Rightarrow mathbb{E} left[w_{l} right] = sqrt{ frac{2}{n_{l}}} end{aligned} label{eq12} tag{12} end{equation}Equation ref{12} is the He. initialilzation, together with b=0. . Note: from equation ref{eq6} we can see that if previous layer is not ReLU-kind, such as the first layer, we will have $ mathbb{E} left[{x&#39;}_{l}^{2} right] = operatorname{Var} left[y&#39;_{l-1} right]$ then $ mathbb{E} left[w_{l} right] = sqrt{ frac{1}{n_{l}}}$. But the factor 1/2 here does not matter if it just exists on one layer. So we adopt equation ref . Backward Propagation Case . Code implementation . https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/ https://medium.com/a-paper-a-day-will-have-you-screaming-hurray/day-8-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification-f449a886e604 . trace of matrix: deep learning page 44 | expectation, variance and covariance: deep learning page 58 | .",
            "url": "https://phucnsp.github.io/blog/self-learning/2020/03/22/explain-kaiming-initialization.html",
            "relUrl": "/self-learning/2020/03/22/explain-kaiming-initialization.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Tutorial jupyter notebook and Fastpages",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](data/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ . Useful links for jupyter notebook . http://blog.juliusschulz.de/blog/ultimate-ipython-notebook#document-metadata | .",
            "url": "https://phucnsp.github.io/blog/tutorial/2020/02/20/tutorial-notebook-and-fastpage.html",
            "relUrl": "/tutorial/2020/02/20/tutorial-notebook-and-fastpage.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Giới thiệu phương pháp học Leitner",
            "content": "Gi&#7899;i thi&#7879;u . Những thứ không hiệu quả cho việc học như: bài giảng, nhồi nhét và đọc lại. . Phương pháp này dựa trên việc học tập một kiến thức mới lặp đi lặp lại, chúng ta kiểm tra trí nhớ của mình về một kiến thức nào đó lặp đi lặp lại và trong những khoảng thời gian khác nhau. Phương pháp này không phải là một study trick hay life hack mà là một cách để điều khiển bộ não của mình, giúp não bộ nhớ lâu hơn. Phương pháp này được phát minh bở nhà tâm lý học người Đức Hermann Ebbinghaus. Ông đã tiến hành một thí nghiệm, theo dõi khả năng nhớ hàng nghìn từ vựng vô nghĩa của mình và ghi chép việc quên của mình. Ông khám phá ra rằng ông đã quên hầu hết mọi thứ đã học trong vòng 24h đầu tiên và những thứ còn xót lại tiếp tục bị quên dần những ngày sau đó. Tuy nhiên, nhìn chung tốc độ quên những thứ đã học giảm dần khi chúng ta được chủ động xem lại kiến thức đó (không phải bị động) - mặc dù khi cyahúng ta dừng thực hành, bộ nhớ tiếp tục giảm. Do vậy, để học bất cứ thứ gì, bạn cần phải xem lại nó ngay lúc bộ não bắt đầu quên và thời gian giữa các lần review phải tăng dần. . forgeting_curve . Điều thú vị trong cách học này là bạn không phải học 1 thứ lặp đi lặp lại mà bạn chỉ học thứ mới hoặc thứ bạn cứ quên quài. . H&#7897;p Leitner . Đầu tiên, chúng ta chia hộp thành 7 cấp độ, bạn có thể có nhiều hoặc ít hơn nếu thích. Một card mới sẽ được bỏ vào hộp thứ 1, khuyến khich bắt đầu với 5 card mơi mỗi ngày. Khi bạn review một card, nếu bạn nhớ đúng kiến thức trong đó, chuyển card đó up một level. Nếu card của bạn ở level cuối cùng, chúc mừng bạn, bây giờ bạn có thể vứt chiệc card đó đi, kiến thức bên trong chiếc card đó sẽ theo bạn suốt đời. Tuy nhiên nếu bạn quên một card nào đó, bạn phải chuyển nó về hộp ban đầu, hộp đầu tiên. Thời gian review hộp leitner: . level 1: hằng ngày | level 2: 2 ngày một lần | level 3: 4 ngày một lần ... cứ như vậy, mỗi lần lên một bậc thì gấp đôi thời gian review Mỗi lần review, chúng ta sẽ review level từ trên xuống, như vậy level 1 sẽ được review cuối cùng. Như vậy chúng ta sẽ biết được những card mình bị quên và những card mới bỏ vào. Hằng ngày, cố gắng không để sót lại bất kì card nào ở level 1, học đi học lại đến khi nào mình nhớ nó và move nó lên level 2. | . review first =&gt; add new card later . mỗi ngày chúng ta cố dành khoảng 20-30 phút để học thay vì xem tivi - bạn có thể nhớ mọi thứ trên đời. Tuy nhiên để xây dựng được thới quen mới khá khó, nếu bạn khởi đầu lớn, có thể bạn sẽ kết thúc nó ngay ngày hôm sau. Nếu bạn khởi đầu nhỏ và lấy cảm hứng dần dần, bạn có thể học nhiều hơn mỗi ngày, vì vậy mà mình khuyến khích học 5 card mỗi ngày. . Sau khi chúng ta đã quen với cách học này, chúng ta có thể có 10,15,20,25,30 card/ngày. Nếu bạn học 30 card mỗi ngày, một năm bạn sẽ học được 10.000+ điều mới . Nh&#7919;ng vi&#7879;c c&#243; th&#7875; d&#7851;n t&#7899;i h&#7885;c sai . Việc học sẽ fail nếu những card của bạn cồng kềnh, không liên quan tới nhau và không có nghĩa. Mặc khác nếu những card của bạn là những mảng nhỏ, kết nôi vơi nhau thì sẽ tốt hơn. Đó cũng là cách nao bộ hoạt động, lots of small and connected things. Vấn đề không phải ở chỗ collection mà ở chỗ connection. | . =&gt; card của bạn phải nhỏ, kết nối, có ý nghĩa. . Nhỏ: quá nhiều thông tin trên một card =&gt; hãy cắt nhỏ nó ra thành nhiều card, smaller and connected pieces. =&gt; rule of thumb: mỗi card chỉ nếu có một và chỉ một idea. | connected: nếu bạn vẽ hình, ghi hoàn cảnh hoặc thông tin cá nhân lên card, sẽ gợi nhớ tốt hơn có card. | ý nghĩa: hãy chọn một topic nào đó mà bạn đang theo đuổi, học piano, đọc truyện, chơi game và bắt đầu dùng leitner box để học mọi thứ về bộ môn đó. Mình tin rằng cách tốt nhất để giữ motivation cho việc học là mình đọc học thứ gì đó mình quan tâm . | không quan trọng bạn học buổi sáng hay chiều, quan trọng là hằng ngày đều phải học (đôi khi có thể skip 1 ngày) . | . you cheat: correct answer, time, | you dont shuffle | you dont cart your victories | you dont educate yourself | your flashcard are wrong | . https://www.youtube.com/watch?v=HN0OUnLxFeU&amp;list=PLdddsM1tHEe-I7QpcFmoNsmwP3dWH_3s_ https://www.youtube.com/watch?v=hs5qmTKBSU0&amp;list=PLdddsM1tHEe-I7QpcFmoNsmwP3dWH_3s_&amp;index=3 https://www.youtube.com/watch?v=ad1nHS-3stg https://ncase.me/remember/ https://en.wikipedia.org/wiki/Leitner_system https://www.ankiapp.com/ https://tinycards.duolingo.com/ .",
            "url": "https://phucnsp.github.io/blog/tutorial/2020/02/20/phuong-phap-hoc-tap-leitner.html",
            "relUrl": "/tutorial/2020/02/20/phuong-phap-hoc-tap-leitner.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Tutorial markdown and Fastpages",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://phucnsp.github.io/blog/tutorial/2020/01/14/tutorial-markdown-and-fastpage.html",
            "relUrl": "/tutorial/2020/01/14/tutorial-markdown-and-fastpage.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Singapore, quan sát và suy ngẫm",
            "content": "Đợt tháng mười vừa rồi tôi có dịp đi dạo quanh anh hàng xóm Singapore. Cũng lâu rồi tôi mới có dịp đi ra khỏi Vietnam, tôi thích cái cảm giác được bay, được transit ở một sân bay nào đó, ngồi ngắm những người xa lạ rảo bước qua lại, cô đơn nhưng thú vị. Singapore nổi tiếng bởi sự hiện đại, bởi những tòa nhà chọc trời, bởi Universal Studio…nhưng đối với tôi thì những cái đó chả có ý nghĩa gì cả. Khi bạn đã từng ngồi cafe ở đại lộ danh vọng Hollywood, đã đi loanh quanh ở quảng trường thời đại NewYork, đã xem biểu tình của người dân Syria trước nhà trắng Mỹ thì liệu những tòa nhà chọc trời ở Singapore có gì để hấp dẫn. Tôi đi Singapore chơi đơn giản vì tôi muốn dẫn vợ đi nước ngoài cho biết và cũng vì tôi cần đi du lịch sau một khoảng thời gian rất rất lâu cắm đầu vô công việc. . Thật may thay, Singapore khác những nơi tôi từng qua và còn làm tôi mất mấy tiếng để ngồi viết cái note này. . Chiều hôm qua tôi trở về Tân Sân Nhất sau hơn 1 tiếng 30 phút bay, đơn giản là cảm giác xót xa. Lạ nhỉ, lần trước bay ở Berlin về chả thấy gì, lần này tự nhiên lại xót xa. Tại người ta ở cái tầm cao hơn mình quá, tôi khen Singapore mà tôi xót cho Saigon, tôi nhìn anh xe ôm, nhìn những chị tay xách nách mang, nhìn cái cách người dân quê tôi băng qua đường chễnh chệ bất chấp đèn tín hiệu, tiếng la hét chèo kéo khách, tiếng bóp còi inh ỏi, xa xa là anh công an ngồi lướt điện thoại ở một xó cạnh lối ra sân bay, tất cả đều làm tôi xót xa. Hôm thứ năm trước khi bay đi chơi còn thấy bình thường mà nhỉ! . Changi đón chào vợ chồng tôi bằng cái thác nước trong nhà cao nhất thế giới, đó là cái woww đầu tiên khi tàu điện kết nối giữa những terminal đưa chúng tôi đi qua cái thác nước nhân tạo này, ngay bên trong sân bay. Chúng tôi về đến hostel trời cũng đã tối, hơn 8pm thì phải, nên cả 2 đều đi ngủ sớm để hôm sau có sức mà đi chơi. . . Ba ngày vi vu ở thành phố này là ba ngày làm tôi suy ngẫm rất nhiều, cái giả định về một thành phố hiện đại với những tòa nhà chọc trời chán phèo dần dần mất đi khi tôi lần lượt đi qua những tụ điểm nổi tiếng ở đây, chứng kiến cái cách họ nâng tầm công nghệ lên thành nghê thuật đã làm tôi thay đổi góc nhìn của mình. Xây cái nhà cao chót vót lên thì ở thành phố lớn nào cũng có nhưng để khoa học và nghệ thuật thật sự gặp nhau thì không phải nơi nào cũng làm được và cũng không phải người dân ở đâu cũng cảm thụ được. Sáng sớm hôm thứ bảy, trong lúc dạo bộ và ngồi nghỉ ngơi ở trạm xe bus gần khu Chinatown, tôi đọc được một bài chia sẻ của anh country manager bên Knorex, từng học tập và làm việc ở Singapore sau đó về quản lý branch ở Ho Chi Minh. Góc nhìn của anh ấy trong công việc thật ra không mới đối với tôi nhưng đơn giản là nó được đọc đúng lúc, đúng thời điểm. Đó là góc nhìn về việc học suốt đời, học ở bất kì hoàn cảnh nào mà không phải chờ có thầy dạy mới học được, về growth mindset, về thái độ làm việc cho đi trước để nhận lại sau, đặc biệt là về sự toàn cầu hóa nhân lực của các công ty, tập đoàn, đồng nghiệp của bạn ngày nay không còn là những anh em bạn dì nữa mà có thể đến từ bất kì đâu trên thế giới này, Ấn độ, Brazile, Chile, Sillicon Valley, etc. Bài chia sẻ ấy như là chất xúc tác cho chuyến đi của tôi, tôi bắt đầu để ý hơn tới con người nơi đây, từng chi tiết nhỏ trên đường, cách họ đào đường lên và lấp lại cẩn thận mà không để nhấp nhô, cách họ giữ gìn sạch sẽ không chỉ phía trước mà cả phía sau của nhà hàng cho đến cách họ quy hoạch bố trí nhà cửa, bố trí mảng xanh khắp nơi… tôi đã cảm thấy thật khó khăn trong việc tìm ra chỗ để bĩu môi chê cười. Mọi thứ được hoàn thiện một cách tuyệt vời ở tầm quốc gia, Singapore có lẽ là ví dụ điển hình nhất cho khái niệm quản lý “tự do trong khuôn khổ”, người dân có không gian để thể hiện cái riêng nhưng phải trong khuôn khổ nhất định để giữ gìn cái chung. . Trưa thứ bảy vợ chồng tôi đi dạo bộ ở khu Marina Bay thì tình cờ được dự giờ một buổi tập hát của mấy em tiểu học. Tụi nhỏ biểu diễn về nhạc kịch hay gì đó đại loại thế, nghe khá xa xỉ đối với đại đa số dân Việt Nam mình. Thật buồn cười khi tôi khen con nít ở Singapore nói tiếng anh hay quá, hát tự tin quá. Nhưng thật sự chúng đã được thừa hưởng di sản phi vật thể quá lớn từ ông Lý, tiếng anh vs tiếng trung, để giờ đây có thể tiếp cận dễ dàng hơn với tinh hoa thế giới, lại thêm cái kiểu giáo dục khai sáng, tự do thể hiện cái tôi cá nhân thế này nữa thì hỡi ơi, mấy đứa cháu ở quê đang đung đưa võng nghe thần tượng Hàn Quốc hát cả ngày hay đang tụ tập quán trà sữa để chém gió, thì rồi cơ may nào để chúng cạnh tranh trong cái thế giới toàn cầu hóa đây. Có thể nhiều người sẽ nghĩ làm gì tới nỗi, vẫn có rất nhiều nhân tài người Việt học trường làng những vẫn nổi danh thế giới đó thôi, nhưng khi đánh giá cái tầm quốc gia thì người ta không nói câu chuyện của một vài người xuất chúng, người ta nói đến sức mạnh của passport index (sức mạnh tấm hộ chiếu Việt Nam hình như đứng gần áp chót bảng xếp hạng), người ta đánh giá thành tích toàn đoàn, có đấy những ngôi sao vàng lẻ loi đoạt huy chương tầm thế giới nhưng có mấy bài báo đăng thành tích của toàn đoàn không! Và theo trải nghiệm cá nhân của tôi, bây giờ khi bạn hỏi một người nước ngoài biết gì về Việt Nam thì câu trả lời khó mà ngoài chiến tranh và gia công quần áo. . À nói một tí về khái niệm “Du lịch, quan sát và suy ngẫm” mà tôi đã từng đọc đâu đó, nó mang tới cho bạn một góc nhìn “thấm” hơn về nơi mình đã đi qua. Du lịch không dừng lại ở những tấm hình selffie khoe trên facebook mà còn cả ở cảm thụ cá nhân. Nhưng cũng có lẽ sự cảm thụ này không dễ mà có được, sau một thời gian dài liên tục học tập, đọc sách, tích lũy kiến thức thì mới may ra chấp chớm cảm nhận được điều này. Đó là lý do vì sao mà chúng ta rất hay thấy du khách nước ngoài tới Việt Nam du lịch thường tới bảo tàng, thường mua sách về Việt Nam để đọc, đơn giản vì họ đang cảm thụ một cách sâu hơn văn hóa, con người, đất nước chúng ta, chứ không chỉ dừng lại bề nổi ở những bức ảnh. Ngày nay tất nhiên không cần phải tới bảo tàng thì chúng ta mới biết về một nơi nào đó, mọi thứ có thể được đọc dễ dàng qua internet nhưng thật sự cảm giác của việc đi du lịch, cảm nhận bằng chính tất cả giác quan để rồi có những suy ngẫm sâu sắc hơn về sự vật sự việc xung quanh mang lại những lợi ích to lớn. Nó là cơn mưa mùa hè cho những bộ não đã bị lu mờ bởi những thứ lặp đi lặp lại hằng ngày, thậm chí nó có thể thay đổi hoàn toàn góc nhìn của mình từ trước tới nay về một việc nào đó. . Quay trở lại câu chuyện xót xa Singapore, có lẽ một dịp nào đó tôi sẽ trở lại đất nước này để xem cảm giác còn như xưa không. Như một lời nhắn nhủ cho bản thân, toàn cầu hóa là có thật và ở ngay đít rồi. Quên đi những xích mích, những câu chuyện vặt ở xung quanh, trong lúc mình ngồi nhiều chuyện thì thằng khác ở đâu đó vẫn đang miệt mài tu luyện. Kỷ luật hơn, học tập ở mọi nơi, học ở bất kì ai, đừng chờ có thầy dạy thì mới học được, đừng sợ học nhiều quá sẽ làm não mình bớt thông minh. Học kiến thức để tăng trí thông mình IQ, học văn hóa nghệ thuật, học cách ửng xử để tăng trí thông minh EQ. Làm việc với một thái độ sẵn sàng hy sinh, thân tôi đây nè, vứt việc cho tôi đi. Dù biết rằng một cánh én nhỏ khó làm nên mùa xuân, tôi có trở thành công dân toàn cầu cũng chưa chắc Việt Nam sẽ tốt hơn nhưng ít nhất tôi không muốn là người góp phần cho đất nước mà tôi yêu quý trở nên tệ hơn! . Tối qua về đến nhà con bạn người Malay đang sống ở Singapore, thấy hình đăng facebook, mới nhắn rủ cafe, cũng hơi tiếc không gặp được. Nó khoe mới có thằng bồ người Ấn, tụi nó vừa đi đám cưới bạn ở đảo Galapagos bên Ecuador về và rủ năm sau ra Hà Nội xem Vietnam F1 Race. . Chắc năm sau ra Hà Nội, quan sát và ngẫm tiếp nhỉ… .",
            "url": "https://phucnsp.github.io/blog/travel/2019/10/07/Singpore-trip-and-remaining.html",
            "relUrl": "/travel/2019/10/07/Singpore-trip-and-remaining.html",
            "date": " • Oct 7, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "Data Science workflow recommendation",
            "content": "Repository of this workflow is stored here . Production data science template . The template of this repository follows production-data-science workflow, which focuses on productionizing data scientist’s work, make the analysis or research to be reusable, applicable to production. The workflow is separated into 2 phases: . exploration phase is where data scientist explores the project, mainly work with jupyter notebook. All the work in this phase will be stored in exploration folder. | production phase is where data scientists’ works are refactored into packages so it can be reuse, imported. All the work in this phase will be stored in your_package folder. | . How to setup a new repository - for maintainer . git clone https://gitlab.com/Phuc_Su/production_data_science_template.git git clone &lt;your_project_repository&gt; cd &lt;your_project_name&gt; git checkout -b product-initial-setup # open Finder, copy all content of production_data_science_template into your project repository, except .git and .idea folder conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; pip install git-lfs # in case you want to add some large file extension other than .jpg, .pdf, .csv, .xlsx git lfs track &lt;add large file path&gt; # rename &lt;your package&gt; folder and modify setup.py, most importance is require_packages. See example below # write something about your project in README.md pip install -e . pip freeze | grep -v &lt;package_name&gt; &gt; requirements.txt git add . git commit -m &quot;First commit&quot; git push -u origin HEAD . Example of setup.py . setup( name=&#39;your_project&#39;, version=&#39;v0.1&#39;, description=&#39;&#39;, long_description=readme(), classifiers=[ &#39;Programming Language :: Python :: 3&#39;, ], url=&#39;https://github.com/phucnsp/production_data_science_template&#39;, author=&#39;Phuc_Su&#39;, author_email=&#39;&#39;, license=&#39;&#39;, packages=[&#39;your_package&#39;], install_requires=[ &#39;pypandoc&gt;=1.4&#39;, &#39;watermark&gt;=1.5.0&#39;, &#39;pandas&gt;=0.20.3&#39;, &#39;scikit-learn&gt;=0.19.0&#39;, &#39;scipy&gt;=0.19.1&#39;, &#39;matplotlib&gt;=2.1.0&#39;, &#39;pytest&gt;=3.2.3&#39;, &#39;pytest-runner&gt;=2.12.1&#39;, &#39;click&gt;=6.7&#39; ], setup_requires=[&#39;pytest-runner&#39;], tests_require=[&#39;pytest&#39;], ) . and you are ready~! 🎉 . Note: if you want to setup notification on slack for merge request from gitlab, reference here . How to contribute - for developers . Setup first time . bash conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; git clone &lt;repository url&gt; cd to/the/project/directory pip install -r requirements.txt pip install -e . . For a private repository accessible only through an SSH authentication, substitute https://github.com/ with git@github.com:. . Returning to work . Some rules: 1 branch/1 exploration/1 folder | branch-name convention: explore-* for exploration, refactor-* for refactor | . | . git checkout master git pull --all # if you continue to work on old branch git checkout &lt;branch&gt; # if you want to start a new exploration git checkout -b &lt;new_branch&gt; # if your branch is far behind master and you want to merge git merge master ##################### Start working ##################### git add &lt;path_to_work_files/folder&gt; git commit -m &quot;some message&quot; git push -u origin HEAD . Notes . requirements.txt helps to setup your virtual environment, to make sure all contributors working on the same environments. So whenever you have a new libraries need to install, after installing you need to add it into requirements.txt by pip freeze | grep -v &lt;package_name&gt; &gt; requirements.txt | setup.py allows you to create packages that you can redistribute. This script is meant to install your package on the end user’s system, not to prepare the development environment. packages - in-house development packages. | install_requires - packages that our development packages dependence on. | py_modules=[&#39;new_module&#39;] - in-house development modules need to install (placed in root directory) | . | pip install -e . - to install packages/modules from setup.py, in the editable mode. | If you want to add large file into working repository: pip install git-lfs git lfs install # Tell LFS to track files with given path git lfs track &quot;path_to_large_file&quot; # Tell LFS to track files with format &quot;*.jpg&quot; git lfs track &quot;*.jpg&quot; # Tell LFS to track content of the whole directory git lfs track &quot;data/*&quot; . | . How to use the package - for users . Install the library . conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; pip install -e &#39;git+https://github.com/phucnsp/production_data_science_template.git&#39; . For a private repository accessible only through an SSH authentication, substitute git+https://github.com with git+ssh://git@github.com. Note that -e argument above to make the installation editable. . Leisure read . Production Data Science tutorial | Writing a setup script | Minimum structure | gitlab slack notification service | git strategy | .",
            "url": "https://phucnsp.github.io/blog/work/2019/05/10/data-science-template.html",
            "relUrl": "/work/2019/05/10/data-science-template.html",
            "date": " • May 10, 2019"
        }
        
    
  
    
        ,"post11": {
            "title": "Predict house price in America",
            "content": "Introduction . import pandas as pd pd.options.display.max_columns = 999 import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import KFold from sklearn.metrics import mean_squared_error from sklearn import linear_model from sklearn.model_selection import KFold . df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) . def transform_features(df): return df def select_features(df): return df[[&quot;Gr Liv Area&quot;, &quot;SalePrice&quot;]] def train_and_test(df): train = df[:1460] test = df[1460:] ## You can use `pd.DataFrame.select_dtypes()` to specify column types ## and return only those columns as a data frame. numeric_train = train.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) numeric_test = test.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) ## You can use `pd.Series.drop()` to drop a value. features = numeric_train.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df) rmse . 57088.251612639091 . Feature Engineering . Handle missing values: All columns: Drop any with 5% or more missing values for now. Text columns: Drop any with 1 or more missing values for now. Numerical columns: For columns with missing values, fill in with the most common value in that column . 1: All columns: Drop any with 5% or more missing values for now. . ## Series object: column name -&gt; number of missing values num_missing = df.isnull().sum() . # Filter Series to columns containing &gt;5% missing values drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() # Drop those columns from the data frame. Note the use of the .index accessor df = df.drop(drop_missing_cols.index, axis=1) . ## Series object: column name -&gt; number of missing values text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) ## Filter Series to columns containing *any* missing values drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) . ## Compute column-wise missing value counts num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() fixable_numeric_cols . BsmtFin SF 1 1 BsmtFin SF 2 1 Bsmt Unf SF 1 Total Bsmt SF 1 Garage Cars 1 Garage Area 1 Bsmt Full Bath 2 Bsmt Half Bath 2 Mas Vnr Area 23 dtype: int64 . ## Compute the most common value for each column in `fixable_nmeric_missing_cols`. replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] replacement_values_dict . {&#39;Bsmt Full Bath&#39;: 0.0, &#39;Bsmt Half Bath&#39;: 0.0, &#39;Bsmt Unf SF&#39;: 0.0, &#39;BsmtFin SF 1&#39;: 0.0, &#39;BsmtFin SF 2&#39;: 0.0, &#39;Garage Area&#39;: 0.0, &#39;Garage Cars&#39;: 2.0, &#39;Mas Vnr Area&#39;: 0.0, &#39;Total Bsmt SF&#39;: 0.0} . ## Use `pd.DataFrame.fillna()` to replace missing values. df = df.fillna(replacement_values_dict) . ## Verify that every column has 0 missing values df.isnull().sum().value_counts() . 0 64 dtype: int64 . years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_sold[years_sold &lt; 0] . 2180 -1 dtype: int64 . years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] years_since_remod[years_since_remod &lt; 0] . 1702 -1 2180 -2 2181 -1 dtype: int64 . ## Create new columns df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod ## Drop rows with negative values for both of these new features df = df.drop([1702, 2180, 2181], axis=0) ## No longer need original year columns df = df.drop([&quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis = 1) . Drop columns that: a. that aren&#39;t useful for ML b. leak data about the final sale . ## Drop columns that aren&#39;t useful for ML df = df.drop([&quot;PID&quot;, &quot;Order&quot;], axis=1) ## Drop columns that leak info about the final sale df = df.drop([&quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Yr Sold&quot;], axis=1) . Let&#39;s update transform_features() . def transform_features(df): num_missing = df.isnull().sum() drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() df = df.drop(drop_missing_cols.index, axis=1) text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] df = df.fillna(replacement_values_dict) years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod df = df.drop([1702, 2180, 2181], axis=0) df = df.drop([&quot;PID&quot;, &quot;Order&quot;, &quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis=1) return df def select_features(df): return df[[&quot;Gr Liv Area&quot;, &quot;SalePrice&quot;]] def train_and_test(df): train = df[:1460] test = df[1460:] ## You can use `pd.DataFrame.select_dtypes()` to specify column types ## and return only those columns as a data frame. numeric_train = train.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) numeric_test = test.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) ## You can use `pd.Series.drop()` to drop a value. features = numeric_train.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df) rmse . 55275.367312413066 . Feature Selection . numerical_df = transform_df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]) numerical_df . MS SubClass Lot Area Overall Qual Overall Cond Mas Vnr Area BsmtFin SF 1 BsmtFin SF 2 Bsmt Unf SF Total Bsmt SF 1st Flr SF 2nd Flr SF Low Qual Fin SF Gr Liv Area Bsmt Full Bath Bsmt Half Bath Full Bath Half Bath Bedroom AbvGr Kitchen AbvGr TotRms AbvGrd Fireplaces Garage Cars Garage Area Wood Deck SF Open Porch SF Enclosed Porch 3Ssn Porch Screen Porch Pool Area Misc Val Yr Sold SalePrice Years Before Sale Years Since Remod . 0 20 | 31770 | 6 | 5 | 112.0 | 639.0 | 0.0 | 441.0 | 1080.0 | 1656 | 0 | 0 | 1656 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 7 | 2 | 2.0 | 528.0 | 210 | 62 | 0 | 0 | 0 | 0 | 0 | 2010 | 215000 | 50 | 50 | . 1 20 | 11622 | 5 | 6 | 0.0 | 468.0 | 144.0 | 270.0 | 882.0 | 896 | 0 | 0 | 896 | 0.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 0 | 1.0 | 730.0 | 140 | 0 | 0 | 0 | 120 | 0 | 0 | 2010 | 105000 | 49 | 49 | . 2 20 | 14267 | 6 | 6 | 108.0 | 923.0 | 0.0 | 406.0 | 1329.0 | 1329 | 0 | 0 | 1329 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 312.0 | 393 | 36 | 0 | 0 | 0 | 0 | 12500 | 2010 | 172000 | 52 | 52 | . 3 20 | 11160 | 7 | 5 | 0.0 | 1065.0 | 0.0 | 1045.0 | 2110.0 | 2110 | 0 | 0 | 2110 | 1.0 | 0.0 | 2 | 1 | 3 | 1 | 8 | 2 | 2.0 | 522.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 244000 | 42 | 42 | . 4 60 | 13830 | 5 | 5 | 0.0 | 791.0 | 0.0 | 137.0 | 928.0 | 928 | 701 | 0 | 1629 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 6 | 1 | 2.0 | 482.0 | 212 | 34 | 0 | 0 | 0 | 0 | 0 | 2010 | 189900 | 13 | 12 | . 5 60 | 9978 | 6 | 6 | 20.0 | 602.0 | 0.0 | 324.0 | 926.0 | 926 | 678 | 0 | 1604 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 470.0 | 360 | 36 | 0 | 0 | 0 | 0 | 0 | 2010 | 195500 | 12 | 12 | . 6 120 | 4920 | 8 | 5 | 0.0 | 616.0 | 0.0 | 722.0 | 1338.0 | 1338 | 0 | 0 | 1338 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 6 | 0 | 2.0 | 582.0 | 0 | 0 | 170 | 0 | 0 | 0 | 0 | 2010 | 213500 | 9 | 9 | . 7 120 | 5005 | 8 | 5 | 0.0 | 263.0 | 0.0 | 1017.0 | 1280.0 | 1280 | 0 | 0 | 1280 | 0.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 506.0 | 0 | 82 | 0 | 0 | 144 | 0 | 0 | 2010 | 191500 | 18 | 18 | . 8 120 | 5389 | 8 | 5 | 0.0 | 1180.0 | 0.0 | 415.0 | 1595.0 | 1616 | 0 | 0 | 1616 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 1 | 2.0 | 608.0 | 237 | 152 | 0 | 0 | 0 | 0 | 0 | 2010 | 236500 | 15 | 14 | . 9 60 | 7500 | 7 | 5 | 0.0 | 0.0 | 0.0 | 994.0 | 994.0 | 1028 | 776 | 0 | 1804 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 442.0 | 140 | 60 | 0 | 0 | 0 | 0 | 0 | 2010 | 189000 | 11 | 11 | . 10 60 | 10000 | 6 | 5 | 0.0 | 0.0 | 0.0 | 763.0 | 763.0 | 763 | 892 | 0 | 1655 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 440.0 | 157 | 84 | 0 | 0 | 0 | 0 | 0 | 2010 | 175900 | 17 | 16 | . 11 20 | 7980 | 6 | 7 | 0.0 | 935.0 | 0.0 | 233.0 | 1168.0 | 1187 | 0 | 0 | 1187 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 6 | 0 | 2.0 | 420.0 | 483 | 21 | 0 | 0 | 0 | 0 | 500 | 2010 | 185000 | 18 | 3 | . 12 60 | 8402 | 6 | 5 | 0.0 | 0.0 | 0.0 | 789.0 | 789.0 | 789 | 676 | 0 | 1465 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 393.0 | 0 | 75 | 0 | 0 | 0 | 0 | 0 | 2010 | 180400 | 12 | 12 | . 13 20 | 10176 | 7 | 5 | 0.0 | 637.0 | 0.0 | 663.0 | 1300.0 | 1341 | 0 | 0 | 1341 | 1.0 | 0.0 | 1 | 1 | 2 | 1 | 5 | 1 | 2.0 | 506.0 | 192 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 171500 | 20 | 20 | . 14 120 | 6820 | 8 | 5 | 0.0 | 368.0 | 1120.0 | 0.0 | 1488.0 | 1502 | 0 | 0 | 1502 | 1.0 | 0.0 | 1 | 1 | 1 | 1 | 4 | 0 | 2.0 | 528.0 | 0 | 54 | 0 | 0 | 140 | 0 | 0 | 2010 | 212000 | 25 | 25 | . 15 60 | 53504 | 8 | 5 | 603.0 | 1416.0 | 0.0 | 234.0 | 1650.0 | 1690 | 1589 | 0 | 3279 | 1.0 | 0.0 | 3 | 1 | 4 | 1 | 12 | 1 | 3.0 | 841.0 | 503 | 36 | 0 | 0 | 210 | 0 | 0 | 2010 | 538000 | 7 | 7 | . 16 50 | 12134 | 8 | 7 | 0.0 | 427.0 | 0.0 | 132.0 | 559.0 | 1080 | 672 | 0 | 1752 | 0.0 | 0.0 | 2 | 0 | 4 | 1 | 8 | 0 | 2.0 | 492.0 | 325 | 12 | 0 | 0 | 0 | 0 | 0 | 2010 | 164000 | 22 | 5 | . 17 20 | 11394 | 9 | 2 | 350.0 | 1445.0 | 0.0 | 411.0 | 1856.0 | 1856 | 0 | 0 | 1856 | 1.0 | 0.0 | 1 | 1 | 1 | 1 | 8 | 1 | 3.0 | 834.0 | 113 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 394432 | 0 | 0 | . 18 20 | 19138 | 4 | 5 | 0.0 | 120.0 | 0.0 | 744.0 | 864.0 | 864 | 0 | 0 | 864 | 0.0 | 0.0 | 1 | 0 | 2 | 1 | 4 | 0 | 2.0 | 400.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 141000 | 59 | 59 | . 19 20 | 13175 | 6 | 6 | 119.0 | 790.0 | 163.0 | 589.0 | 1542.0 | 2073 | 0 | 0 | 2073 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 7 | 2 | 2.0 | 500.0 | 349 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 210000 | 32 | 22 | . 20 20 | 11751 | 6 | 6 | 480.0 | 705.0 | 0.0 | 1139.0 | 1844.0 | 1844 | 0 | 0 | 1844 | 0.0 | 0.0 | 2 | 0 | 3 | 1 | 7 | 1 | 2.0 | 546.0 | 0 | 122 | 0 | 0 | 0 | 0 | 0 | 2010 | 190000 | 33 | 33 | . 21 85 | 10625 | 7 | 6 | 81.0 | 885.0 | 168.0 | 0.0 | 1053.0 | 1173 | 0 | 0 | 1173 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 6 | 2 | 2.0 | 528.0 | 0 | 120 | 0 | 0 | 0 | 0 | 0 | 2010 | 170000 | 36 | 36 | . 22 60 | 7500 | 7 | 5 | 0.0 | 533.0 | 0.0 | 281.0 | 814.0 | 814 | 860 | 0 | 1674 | 1.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 0 | 2.0 | 663.0 | 0 | 96 | 0 | 0 | 0 | 0 | 0 | 2010 | 216000 | 10 | 10 | . 23 20 | 11241 | 6 | 7 | 180.0 | 578.0 | 0.0 | 426.0 | 1004.0 | 1004 | 0 | 0 | 1004 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 1 | 2.0 | 480.0 | 0 | 0 | 0 | 0 | 0 | 0 | 700 | 2010 | 149000 | 40 | 40 | . 24 20 | 12537 | 5 | 6 | 0.0 | 734.0 | 0.0 | 344.0 | 1078.0 | 1078 | 0 | 0 | 1078 | 1.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 1 | 2.0 | 500.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 149900 | 39 | 2 | . 25 20 | 8450 | 5 | 6 | 0.0 | 775.0 | 0.0 | 281.0 | 1056.0 | 1056 | 0 | 0 | 1056 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 1 | 1.0 | 304.0 | 0 | 85 | 184 | 0 | 0 | 0 | 0 | 2010 | 142000 | 42 | 42 | . 26 20 | 8400 | 4 | 5 | 0.0 | 804.0 | 78.0 | 0.0 | 882.0 | 882 | 0 | 0 | 882 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 4 | 0 | 2.0 | 525.0 | 240 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 126000 | 40 | 40 | . 27 20 | 10500 | 4 | 5 | 0.0 | 432.0 | 0.0 | 432.0 | 864.0 | 864 | 0 | 0 | 864 | 0.0 | 0.0 | 1 | 0 | 3 | 1 | 5 | 1 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 115000 | 39 | 39 | . 28 120 | 5858 | 7 | 5 | 0.0 | 1051.0 | 0.0 | 354.0 | 1405.0 | 1337 | 0 | 0 | 1337 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 1 | 2.0 | 511.0 | 203 | 68 | 0 | 0 | 0 | 0 | 0 | 2010 | 184000 | 11 | 11 | . 29 160 | 1680 | 6 | 5 | 504.0 | 156.0 | 0.0 | 327.0 | 483.0 | 483 | 504 | 0 | 987 | 0.0 | 0.0 | 1 | 1 | 2 | 1 | 5 | 0 | 1.0 | 264.0 | 275 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 96000 | 39 | 39 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2900 20 | 13618 | 8 | 5 | 198.0 | 1350.0 | 0.0 | 378.0 | 1728.0 | 1960 | 0 | 0 | 1960 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 8 | 2 | 3.0 | 714.0 | 172 | 38 | 0 | 0 | 0 | 0 | 0 | 2006 | 320000 | 1 | 0 | . 2901 20 | 11443 | 8 | 5 | 208.0 | 1460.0 | 0.0 | 408.0 | 1868.0 | 2028 | 0 | 0 | 2028 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 7 | 2 | 3.0 | 880.0 | 326 | 66 | 0 | 0 | 0 | 0 | 0 | 2006 | 369900 | 1 | 0 | . 2902 20 | 11577 | 9 | 5 | 382.0 | 1455.0 | 0.0 | 383.0 | 1838.0 | 1838 | 0 | 0 | 1838 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 9 | 1 | 3.0 | 682.0 | 161 | 225 | 0 | 0 | 0 | 0 | 0 | 2006 | 359900 | 1 | 0 | . 2903 20 | 31250 | 1 | 3 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1600 | 0 | 0 | 1600 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 270.0 | 0 | 0 | 135 | 0 | 0 | 0 | 0 | 2006 | 81500 | 55 | 55 | . 2904 90 | 7020 | 7 | 5 | 200.0 | 1243.0 | 0.0 | 45.0 | 1288.0 | 1368 | 0 | 0 | 1368 | 2.0 | 0.0 | 2 | 0 | 2 | 2 | 8 | 0 | 4.0 | 784.0 | 0 | 48 | 0 | 0 | 0 | 0 | 0 | 2006 | 215000 | 9 | 9 | . 2905 120 | 4500 | 6 | 5 | 116.0 | 897.0 | 0.0 | 319.0 | 1216.0 | 1216 | 0 | 0 | 1216 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 402.0 | 0 | 125 | 0 | 0 | 0 | 0 | 0 | 2006 | 164000 | 8 | 8 | . 2906 120 | 4500 | 6 | 5 | 443.0 | 1201.0 | 0.0 | 36.0 | 1237.0 | 1337 | 0 | 0 | 1337 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 405.0 | 0 | 199 | 0 | 0 | 0 | 0 | 0 | 2006 | 153500 | 8 | 8 | . 2907 20 | 17217 | 5 | 5 | 0.0 | 0.0 | 0.0 | 1140.0 | 1140.0 | 1140 | 0 | 0 | 1140 | 0.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 0 | 0.0 | 0.0 | 36 | 56 | 0 | 0 | 0 | 0 | 0 | 2006 | 84500 | 0 | 0 | . 2908 160 | 2665 | 5 | 6 | 0.0 | 0.0 | 0.0 | 264.0 | 264.0 | 616 | 688 | 0 | 1304 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 1 | 1.0 | 336.0 | 141 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 104500 | 29 | 29 | . 2909 160 | 2665 | 5 | 6 | 0.0 | 548.0 | 173.0 | 36.0 | 757.0 | 925 | 550 | 0 | 1475 | 0.0 | 0.0 | 2 | 0 | 4 | 1 | 6 | 1 | 1.0 | 336.0 | 104 | 26 | 0 | 0 | 0 | 0 | 0 | 2006 | 127000 | 29 | 29 | . 2910 160 | 3964 | 6 | 4 | 0.0 | 837.0 | 0.0 | 105.0 | 942.0 | 1291 | 1230 | 0 | 2521 | 1.0 | 0.0 | 2 | 1 | 5 | 1 | 10 | 1 | 2.0 | 576.0 | 728 | 20 | 0 | 0 | 0 | 0 | 0 | 2006 | 151400 | 33 | 33 | . 2911 20 | 10172 | 5 | 7 | 0.0 | 441.0 | 0.0 | 423.0 | 864.0 | 874 | 0 | 0 | 874 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 5 | 0 | 1.0 | 288.0 | 0 | 120 | 0 | 0 | 0 | 0 | 0 | 2006 | 126500 | 38 | 3 | . 2912 90 | 11836 | 5 | 5 | 0.0 | 149.0 | 0.0 | 1503.0 | 1652.0 | 1652 | 0 | 0 | 1652 | 0.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 3.0 | 928.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 146500 | 36 | 36 | . 2913 180 | 1470 | 4 | 6 | 0.0 | 522.0 | 0.0 | 108.0 | 630.0 | 630 | 0 | 0 | 630 | 1.0 | 0.0 | 1 | 0 | 1 | 1 | 3 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 73000 | 36 | 36 | . 2914 160 | 1484 | 4 | 4 | 0.0 | 252.0 | 0.0 | 294.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 1.0 | 253.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 79400 | 34 | 34 | . 2915 20 | 13384 | 5 | 5 | 194.0 | 119.0 | 344.0 | 641.0 | 1104.0 | 1360 | 0 | 0 | 1360 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 8 | 1 | 1.0 | 336.0 | 160 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 140000 | 37 | 27 | . 2916 180 | 1533 | 5 | 7 | 0.0 | 553.0 | 0.0 | 77.0 | 630.0 | 630 | 0 | 0 | 630 | 1.0 | 0.0 | 1 | 0 | 1 | 1 | 3 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 92000 | 36 | 36 | . 2917 160 | 1533 | 4 | 5 | 0.0 | 408.0 | 0.0 | 138.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 1.0 | 286.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 87550 | 36 | 36 | . 2918 160 | 1526 | 4 | 5 | 0.0 | 0.0 | 0.0 | 546.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 0.0 | 0.0 | 0 | 34 | 0 | 0 | 0 | 0 | 0 | 2006 | 79500 | 36 | 36 | . 2919 160 | 1936 | 4 | 7 | 0.0 | 0.0 | 0.0 | 546.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 90500 | 36 | 36 | . 2920 160 | 1894 | 4 | 5 | 0.0 | 252.0 | 0.0 | 294.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 286.0 | 0 | 24 | 0 | 0 | 0 | 0 | 0 | 2006 | 71000 | 36 | 36 | . 2921 90 | 12640 | 6 | 5 | 0.0 | 936.0 | 396.0 | 396.0 | 1728.0 | 1728 | 0 | 0 | 1728 | 0.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 2.0 | 574.0 | 40 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 150900 | 30 | 30 | . 2922 90 | 9297 | 5 | 5 | 0.0 | 1606.0 | 0.0 | 122.0 | 1728.0 | 1728 | 0 | 0 | 1728 | 2.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 2.0 | 560.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 188000 | 30 | 30 | . 2923 20 | 17400 | 5 | 5 | 0.0 | 936.0 | 0.0 | 190.0 | 1126.0 | 1126 | 0 | 0 | 1126 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 5 | 1 | 2.0 | 484.0 | 295 | 41 | 0 | 0 | 0 | 0 | 0 | 2006 | 160000 | 29 | 29 | . 2924 20 | 20000 | 5 | 7 | 0.0 | 1224.0 | 0.0 | 0.0 | 1224.0 | 1224 | 0 | 0 | 1224 | 1.0 | 0.0 | 1 | 0 | 4 | 1 | 7 | 1 | 2.0 | 576.0 | 474 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 131000 | 46 | 10 | . 2925 80 | 7937 | 6 | 6 | 0.0 | 819.0 | 0.0 | 184.0 | 1003.0 | 1003 | 0 | 0 | 1003 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 0 | 2.0 | 588.0 | 120 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 142500 | 22 | 22 | . 2926 20 | 8885 | 5 | 5 | 0.0 | 301.0 | 324.0 | 239.0 | 864.0 | 902 | 0 | 0 | 902 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 0 | 2.0 | 484.0 | 164 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 131000 | 23 | 23 | . 2927 85 | 10441 | 5 | 5 | 0.0 | 337.0 | 0.0 | 575.0 | 912.0 | 970 | 0 | 0 | 970 | 0.0 | 1.0 | 1 | 0 | 3 | 1 | 6 | 0 | 0.0 | 0.0 | 80 | 32 | 0 | 0 | 0 | 0 | 700 | 2006 | 132000 | 14 | 14 | . 2928 20 | 10010 | 5 | 5 | 0.0 | 1071.0 | 123.0 | 195.0 | 1389.0 | 1389 | 0 | 0 | 1389 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 6 | 1 | 2.0 | 418.0 | 240 | 38 | 0 | 0 | 0 | 0 | 0 | 2006 | 170000 | 32 | 31 | . 2929 60 | 9627 | 7 | 5 | 94.0 | 758.0 | 0.0 | 238.0 | 996.0 | 996 | 1004 | 0 | 2000 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 9 | 1 | 3.0 | 650.0 | 190 | 48 | 0 | 0 | 0 | 0 | 0 | 2006 | 188000 | 13 | 12 | . 2927 rows × 34 columns . abs_corr_coeffs = numerical_df.corr()[&#39;SalePrice&#39;].abs().sort_values() abs_corr_coeffs . BsmtFin SF 2 0.006127 Misc Val 0.019273 Yr Sold 0.030358 3Ssn Porch 0.032268 Bsmt Half Bath 0.035875 Low Qual Fin SF 0.037629 Pool Area 0.068438 MS SubClass 0.085128 Overall Cond 0.101540 Screen Porch 0.112280 Kitchen AbvGr 0.119760 Enclosed Porch 0.128685 Bedroom AbvGr 0.143916 Bsmt Unf SF 0.182751 Lot Area 0.267520 2nd Flr SF 0.269601 Bsmt Full Bath 0.276258 Half Bath 0.284871 Open Porch SF 0.316262 Wood Deck SF 0.328183 BsmtFin SF 1 0.439284 Fireplaces 0.474831 TotRms AbvGrd 0.498574 Mas Vnr Area 0.506983 Years Since Remod 0.534985 Full Bath 0.546118 Years Before Sale 0.558979 1st Flr SF 0.635185 Garage Area 0.641425 Total Bsmt SF 0.644012 Garage Cars 0.648361 Gr Liv Area 0.717596 Overall Qual 0.801206 SalePrice 1.000000 Name: SalePrice, dtype: float64 . ## Let&#39;s only keep columns with a correlation coefficient of larger than 0.4 (arbitrary, worth experimenting later!) abs_corr_coeffs[abs_corr_coeffs &gt; 0.4] . BsmtFin SF 1 0.439284 Fireplaces 0.474831 TotRms AbvGrd 0.498574 Mas Vnr Area 0.506983 Years Since Remod 0.534985 Full Bath 0.546118 Years Before Sale 0.558979 1st Flr SF 0.635185 Garage Area 0.641425 Total Bsmt SF 0.644012 Garage Cars 0.648361 Gr Liv Area 0.717596 Overall Qual 0.801206 SalePrice 1.000000 Name: SalePrice, dtype: float64 . ## Drop columns with less than 0.4 correlation with SalePrice transform_df = transform_df.drop(abs_corr_coeffs[abs_corr_coeffs &lt; 0.4].index, axis=1) . Which categorical columns should we keep? . ## Create a list of column names from documentation that are *meant* to be categorical nominal_features = [&quot;PID&quot;, &quot;MS SubClass&quot;, &quot;MS Zoning&quot;, &quot;Street&quot;, &quot;Alley&quot;, &quot;Land Contour&quot;, &quot;Lot Config&quot;, &quot;Neighborhood&quot;, &quot;Condition 1&quot;, &quot;Condition 2&quot;, &quot;Bldg Type&quot;, &quot;House Style&quot;, &quot;Roof Style&quot;, &quot;Roof Matl&quot;, &quot;Exterior 1st&quot;, &quot;Exterior 2nd&quot;, &quot;Mas Vnr Type&quot;, &quot;Foundation&quot;, &quot;Heating&quot;, &quot;Central Air&quot;, &quot;Garage Type&quot;, &quot;Misc Feature&quot;, &quot;Sale Type&quot;, &quot;Sale Condition&quot;] . Which columns are currently numerical but need to be encoded as categorical instead (because the numbers don&#39;t have any semantic meaning)? If a categorical column has hundreds of unique values (or categories), should we keep it? When we dummy code this column, hundreds of columns will need to be added back to the data frame. . ## Which categorical columns have we still carried with us? We&#39;ll test tehse transform_cat_cols = [] for col in nominal_features: if col in transform_df.columns: transform_cat_cols.append(col) ## How many unique values in each categorical column? uniqueness_counts = transform_df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values() ## Aribtrary cutoff of 10 unique values (worth experimenting) drop_nonuniq_cols = uniqueness_counts[uniqueness_counts &gt; 10].index transform_df = transform_df.drop(drop_nonuniq_cols, axis=1) . ## Select just the remaining text columns and convert to categorical text_cols = transform_df.select_dtypes(include=[&#39;object&#39;]) for col in text_cols: transform_df[col] = transform_df[col].astype(&#39;category&#39;) ## Create dummy columns and add back to the dataframe! transform_df = pd.concat([ transform_df, pd.get_dummies(transform_df.select_dtypes(include=[&#39;category&#39;])) ], axis=1) . Update select_features() . def transform_features(df): num_missing = df.isnull().sum() drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() df = df.drop(drop_missing_cols.index, axis=1) text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] df = df.fillna(replacement_values_dict) years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod df = df.drop([1702, 2180, 2181], axis=0) df = df.drop([&quot;PID&quot;, &quot;Order&quot;, &quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis=1) return df def select_features(df, coeff_threshold=0.4, uniq_threshold=10): numerical_df = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]) abs_corr_coeffs = numerical_df.corr()[&#39;SalePrice&#39;].abs().sort_values() df = df.drop(abs_corr_coeffs[abs_corr_coeffs &lt; coeff_threshold].index, axis=1) nominal_features = [&quot;PID&quot;, &quot;MS SubClass&quot;, &quot;MS Zoning&quot;, &quot;Street&quot;, &quot;Alley&quot;, &quot;Land Contour&quot;, &quot;Lot Config&quot;, &quot;Neighborhood&quot;, &quot;Condition 1&quot;, &quot;Condition 2&quot;, &quot;Bldg Type&quot;, &quot;House Style&quot;, &quot;Roof Style&quot;, &quot;Roof Matl&quot;, &quot;Exterior 1st&quot;, &quot;Exterior 2nd&quot;, &quot;Mas Vnr Type&quot;, &quot;Foundation&quot;, &quot;Heating&quot;, &quot;Central Air&quot;, &quot;Garage Type&quot;, &quot;Misc Feature&quot;, &quot;Sale Type&quot;, &quot;Sale Condition&quot;] transform_cat_cols = [] for col in nominal_features: if col in df.columns: transform_cat_cols.append(col) uniqueness_counts = df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values() drop_nonuniq_cols = uniqueness_counts[uniqueness_counts &gt; 10].index df = df.drop(drop_nonuniq_cols, axis=1) text_cols = df.select_dtypes(include=[&#39;object&#39;]) for col in text_cols: df[col] = df[col].astype(&#39;category&#39;) df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=[&#39;category&#39;]))], axis=1) return df def train_and_test(df, k=0): numeric_df = df.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) features = numeric_df.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() if k == 0: train = df[:1460] test = df[1460:] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse if k == 1: # Randomize *all* rows (frac=1) from `df` and return shuffled_df = df.sample(frac=1, ) train = df[:1460] test = df[1460:] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions_one = lr.predict(test[features]) mse_one = mean_squared_error(test[&quot;SalePrice&quot;], predictions_one) rmse_one = np.sqrt(mse_one) lr.fit(test[features], test[&quot;SalePrice&quot;]) predictions_two = lr.predict(train[features]) mse_two = mean_squared_error(train[&quot;SalePrice&quot;], predictions_two) rmse_two = np.sqrt(mse_two) avg_rmse = np.mean([rmse_one, rmse_two]) print(rmse_one) print(rmse_two) return avg_rmse else: kf = KFold(n_splits=k, shuffle=True) rmse_values = [] for train_index, test_index, in kf.split(df): train = df.iloc[train_index] test = df.iloc[test_index] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) rmse_values.append(rmse) print(rmse_values) avg_rmse = np.mean(rmse_values) return avg_rmse df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df, k=4) rmse . [25761.875549560471, 36527.812968130842, 24956.485193881424, 28486.738135675929] . 28933.227961812168 .",
            "url": "https://phucnsp.github.io/blog/self-taught/2017/09/15/predict-house-price.html",
            "relUrl": "/self-taught/2017/09/15/predict-house-price.html",
            "date": " • Sep 15, 2017"
        }
        
    
  
    
        ,"post12": {
            "title": "Kaggle competition - titanic machine learning from disaster",
            "content": "import pandas as pd train = pd.read_csv(&quot;train.csv&quot;) holdout = pd.read_csv(&quot;test.csv&quot;) print(holdout.head()) . PassengerId Pclass Name Sex 0 892 3 Kelly, Mr. James male 1 893 3 Wilkes, Mrs. James (Ellen Needs) female 2 894 2 Myles, Mr. Thomas Francis male 3 895 3 Wirz, Mr. Albert male 4 896 3 Hirvonen, Mrs. Alexander (Helga E Lindqvist) female Age SibSp Parch Ticket Fare Cabin Embarked 0 34.5 0 0 330911 7.8292 NaN Q 1 47.0 1 0 363272 7.0000 NaN S 2 62.0 0 0 240276 9.6875 NaN Q 3 27.0 0 0 315154 8.6625 NaN S 4 22.0 1 1 3101298 12.2875 NaN S . # %load functions.py def process_missing(df): &quot;&quot;&quot;Handle various missing values from the data set Usage holdout = process_missing(holdout) &quot;&quot;&quot; df[&quot;Fare&quot;] = df[&quot;Fare&quot;].fillna(train[&quot;Fare&quot;].mean()) df[&quot;Embarked&quot;] = df[&quot;Embarked&quot;].fillna(&quot;S&quot;) return df def process_age(df): &quot;&quot;&quot;Process the Age column into pre-defined &#39;bins&#39; Usage train = process_age(train) &quot;&quot;&quot; df[&quot;Age&quot;] = df[&quot;Age&quot;].fillna(-0.5) cut_points = [-1,0,5,12,18,35,60,100] label_names = [&quot;Missing&quot;,&quot;Infant&quot;,&quot;Child&quot;,&quot;Teenager&quot;,&quot;Young Adult&quot;,&quot;Adult&quot;,&quot;Senior&quot;] df[&quot;Age_categories&quot;] = pd.cut(df[&quot;Age&quot;],cut_points,labels=label_names) return df def process_fare(df): &quot;&quot;&quot;Process the Fare column into pre-defined &#39;bins&#39; Usage train = process_fare(train) &quot;&quot;&quot; cut_points = [-1,12,50,100,1000] label_names = [&quot;0-12&quot;,&quot;12-50&quot;,&quot;50-100&quot;,&quot;100+&quot;] df[&quot;Fare_categories&quot;] = pd.cut(df[&quot;Fare&quot;],cut_points,labels=label_names) return df def process_cabin(df): &quot;&quot;&quot;Process the Cabin column into pre-defined &#39;bins&#39; Usage train process_cabin(train) &quot;&quot;&quot; df[&quot;Cabin_type&quot;] = df[&quot;Cabin&quot;].str[0] df[&quot;Cabin_type&quot;] = df[&quot;Cabin_type&quot;].fillna(&quot;Unknown&quot;) df = df.drop(&#39;Cabin&#39;,axis=1) return df def process_titles(df): &quot;&quot;&quot;Extract and categorize the title from the name column Usage train = process_titles(train) &quot;&quot;&quot; titles = { &quot;Mr&quot; : &quot;Mr&quot;, &quot;Mme&quot;: &quot;Mrs&quot;, &quot;Ms&quot;: &quot;Mrs&quot;, &quot;Mrs&quot; : &quot;Mrs&quot;, &quot;Master&quot; : &quot;Master&quot;, &quot;Mlle&quot;: &quot;Miss&quot;, &quot;Miss&quot; : &quot;Miss&quot;, &quot;Capt&quot;: &quot;Officer&quot;, &quot;Col&quot;: &quot;Officer&quot;, &quot;Major&quot;: &quot;Officer&quot;, &quot;Dr&quot;: &quot;Officer&quot;, &quot;Rev&quot;: &quot;Officer&quot;, &quot;Jonkheer&quot;: &quot;Royalty&quot;, &quot;Don&quot;: &quot;Royalty&quot;, &quot;Sir&quot; : &quot;Royalty&quot;, &quot;Countess&quot;: &quot;Royalty&quot;, &quot;Dona&quot;: &quot;Royalty&quot;, &quot;Lady&quot; : &quot;Royalty&quot; } extracted_titles = df[&quot;Name&quot;].str.extract(&#39; ([A-Za-z]+) .&#39;,expand=False) df[&quot;Title&quot;] = extracted_titles.map(titles) return df def create_dummies(df,column_name): &quot;&quot;&quot;Create Dummy Columns (One Hot Encoding) from a single Column Usage train = create_dummies(train,&quot;Age&quot;) &quot;&quot;&quot; dummies = pd.get_dummies(df[column_name],prefix=column_name) df = pd.concat([df,dummies],axis=1) return df . #preprocess the data def pre_process(df): df = process_missing(df) df = process_age(df) df = process_fare(df) df = process_titles(df) df = process_cabin(df) for col in [&quot;Age_categories&quot;,&quot;Fare_categories&quot;, &quot;Title&quot;,&quot;Cabin_type&quot;,&quot;Sex&quot;]: df = create_dummies(df,col) return df train = pre_process(train) holdout = pre_process(holdout) . Data exploration . #Inspect data type of column explore_cols = [&quot;SibSp&quot;,&quot;Parch&quot;,&quot;Survived&quot;] explore = train[explore_cols].copy() explore.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 3 columns): SibSp 891 non-null int64 Parch 891 non-null int64 Survived 891 non-null int64 dtypes: int64(3) memory usage: 21.0 KB . # Histogram to view the distribution of 2 columns: SibSp and Parch import matplotlib.pyplot as plt %matplotlib inline explore.drop(&quot;Survived&quot;,axis=1).plot.hist(alpha=0.5,bins=8) plt.xticks(range(11)) plt.show() . explore[&quot;familysize&quot;] = explore[[&quot;SibSp&quot;,&quot;Parch&quot;]].sum(axis=1) explore.drop(&quot;Survived&quot;,axis=1).plot.hist(alpha=0.5,bins=10) plt.xticks(range(11)) plt.show() . # Use pivot tables to look at the survival rate for different values of the columns import numpy as np for col in explore.columns.drop(&quot;Survived&quot;): pivot = explore.pivot_table(index=col,values=&quot;Survived&quot;) pivot.plot.bar(ylim=(0,1),yticks=np.arange(0,1,.1)) plt.axhspan(.3, .6, alpha=0.2, color=&#39;red&#39;) plt.show() . The SibSp column shows the number of siblings and/or spouses each passenger had on board, while the Parch columns shows the number of parents or children each passenger had onboard. Neither column has any missing values. . The distribution of values in both columns is skewed right, with the majority of values being zero. . You can sum these two columns to explore the total number of family members each passenger had onboard. The shape of the distribution of values in this case is similar, however there are less values at zero, and the quantity tapers off less rapidly as the values increase. . Looking at the survival rates of the the combined family members, you can see that few of the over 500 passengers with no family members survived, while greater numbers of passengers with family members survived. . Engineering new features . # Based on the observation about few surviver with no family group, let&#39;s create a binary value column where 1 is with # family and 0 is without family def feature_alone(df): df[&quot;familysize&quot;] = df[[&quot;SibSp&quot;,&quot;Parch&quot;]].sum(axis=1) df[&quot;isalone&quot;] = 0 df.loc[(df[&quot;familysize&quot;] == 0),&quot;isalone&quot;] = 1 df.drop(&quot;familysize&quot;, axis = 1) return df train = feature_alone(train) holdout = feature_alone(holdout) . Feature selection/preparation . # Select the best-performing features from sklearn.ensemble import RandomForestClassifier from sklearn.feature_selection import RFECV def select_features(df): # Remove non-numeric columns, columns that have null values df = df.select_dtypes([np.number]).dropna(axis=1) all_X = df.drop([&quot;Survived&quot;,&quot;PassengerId&quot;],axis=1) all_y = df[&quot;Survived&quot;] clf = RandomForestClassifier(random_state=1) selector = RFECV(clf,cv=10) selector.fit(all_X,all_y) best_columns = list(all_X.columns[selector.support_]) print(&quot;Best Columns n&quot;+&quot;-&quot;*12+&quot; n{}&quot;.format(best_columns)) return best_columns cols = select_features(train) . Best Columns [&#39;Pclass&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Age_categories_Adult&#39;, &#39;Age_categories_Infant&#39;, &#39;Age_categories_Missing&#39;, &#39;Age_categories_Senior&#39;, &#39;Age_categories_Teenager&#39;, &#39;Age_categories_Young Adult&#39;, &#39;Fare_categories_0-12&#39;, &#39;Fare_categories_100+&#39;, &#39;Fare_categories_12-50&#39;, &#39;Fare_categories_50-100&#39;, &#39;Title_Master&#39;, &#39;Title_Miss&#39;, &#39;Title_Mr&#39;, &#39;Title_Mrs&#39;, &#39;Title_Officer&#39;, &#39;Cabin_type_C&#39;, &#39;Cabin_type_D&#39;, &#39;Cabin_type_E&#39;, &#39;Cabin_type_Unknown&#39;, &#39;Sex_female&#39;, &#39;Sex_male&#39;, &#39;familysize&#39;, &#39;isalone&#39;] . Model selection/Tuning . # Write a function to train 3 different models. # Using grid search to train using different combinations of hyperparameters to find best performing models. from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import GridSearchCV def select_model(df,features): all_X = df[features] all_y = df[&quot;Survived&quot;] # List of dictionaries, each containing a model name, # it&#39;s estimator and a dict of hyperparameters models = [ { &quot;name&quot;: &quot;LogisticRegression&quot;, &quot;estimator&quot;: LogisticRegression(), &quot;hyperparameters&quot;: { &quot;solver&quot;: [&quot;newton-cg&quot;, &quot;lbfgs&quot;, &quot;liblinear&quot;] } }, { &quot;name&quot;: &quot;KNeighborsClassifier&quot;, &quot;estimator&quot;: KNeighborsClassifier(), &quot;hyperparameters&quot;: { &quot;n_neighbors&quot;: range(1,20,2), &quot;weights&quot;: [&quot;distance&quot;, &quot;uniform&quot;], &quot;algorithm&quot;: [&quot;ball_tree&quot;, &quot;kd_tree&quot;, &quot;brute&quot;], &quot;p&quot;: [1,2] } }, { &quot;name&quot;: &quot;RandomForestClassifier&quot;, &quot;estimator&quot;: RandomForestClassifier(random_state=1), &quot;hyperparameters&quot;: { &quot;n_estimators&quot;: [4, 6, 9], &quot;criterion&quot;: [&quot;entropy&quot;, &quot;gini&quot;], &quot;max_depth&quot;: [2, 5, 10], &quot;max_features&quot;: [&quot;log2&quot;, &quot;sqrt&quot;], &quot;min_samples_leaf&quot;: [1, 5, 8], &quot;min_samples_split&quot;: [2, 3, 5] } } ] for model in models: print(model[&#39;name&#39;]) print(&#39;-&#39;*len(model[&#39;name&#39;])) grid = GridSearchCV(model[&quot;estimator&quot;], param_grid=model[&quot;hyperparameters&quot;], cv=10) grid.fit(all_X,all_y) model[&quot;best_params&quot;] = grid.best_params_ model[&quot;best_score&quot;] = grid.best_score_ model[&quot;best_model&quot;] = grid.best_estimator_ print(&quot;Best Score: {}&quot;.format(model[&quot;best_score&quot;])) print(&quot;Best Parameters: {} n&quot;.format(model[&quot;best_params&quot;])) return models result = select_model(train,cols) . LogisticRegression Best Score: 0.8226711560044894 Best Parameters: {&#39;solver&#39;: &#39;liblinear&#39;} KNeighborsClassifier -- Best Score: 0.7833894500561167 Best Parameters: {&#39;algorithm&#39;: &#39;kd_tree&#39;, &#39;n_neighbors&#39;: 3, &#39;p&#39;: 1, &#39;weights&#39;: &#39;uniform&#39;} RandomForestClassifier - Best Score: 0.8451178451178452 Best Parameters: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 10, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 3, &#39;n_estimators&#39;: 9} . Submit to Kaggle . def save_submission_file(model,cols,filename=&quot;submission.csv&quot;): holdout_data = holdout[cols] predictions = model.predict(holdout_data) holdout_ids = holdout[&quot;PassengerId&quot;] submission_df = {&quot;PassengerId&quot;: holdout_ids, &quot;Survived&quot;: predictions} submission = pd.DataFrame(submission_df) submission.to_csv(filename,index=False) best_rf_model = result[2][&quot;best_model&quot;] save_submission_file(best_rf_model,cols) .",
            "url": "https://phucnsp.github.io/blog/kaggle/2017/08/20/kaggle-titanic-machine-learning-from-disaster.html",
            "relUrl": "/kaggle/2017/08/20/kaggle-titanic-machine-learning-from-disaster.html",
            "date": " • Aug 20, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I have been working as Data Scientist at MTI Technology Vietnam since 2018 and my journey in AI field started since 2017. In here, I mainly work with OCR (optical charcter recognition) projects where we not only have to extract texts from documents but also classify it into some specific fields defined by clients. .",
          "url": "https://phucnsp.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}