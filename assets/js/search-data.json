{
  
    
        "post0": {
            "title": "This notebook tracks my progress in learning nlp",
            "content": "fastai nlp course . video 11 . nn.Embedding(len_vocab, len_vector_embedding) . like a matrix with nr_row = nr of words in vocab and nr_column = len_vector_represent_each_word = nr_features. This matrix can be imported from other network or can be learned from training. It is used to placed at the start of language-related networks to represent words input. | references: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/word_embeddings_tutorial.ipynb#scrollTo=rFc2HSo8SKo9 | . | nn.Linear . Parameter(tensor) | . | nn.init.kaiminguniform this funciton implements the initialization recommendation from the sound paper Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | https://towardsdatascience.com/understand-kaiming-initialization-and-implementation-detail-in-pytorch-f7aa967e9138 | | . | .",
            "url": "https://phucnsp.github.io/blog/self-learning/2020/04/20/self-taught-nlp.html",
            "relUrl": "/self-learning/2020/04/20/self-taught-nlp.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Giới thiệu phương pháp học Leitner",
            "content": "Gi&#7899;i thi&#7879;u . Những thứ không hiệu quả cho việc học như: bài giảng, nhồi nhét và đọc lại. . Phương pháp này dựa trên việc học tập một kiến thức mới lặp đi lặp lại, chúng ta kiểm tra trí nhớ của mình về một kiến thức nào đó lặp đi lặp lại và trong những khoảng thời gian khác nhau. Phương pháp này không phải là một study trick hay life hack mà là một cách để điều khiển bộ não của mình, giúp não bộ nhớ lâu hơn. Phương pháp này được phát minh bở nhà tâm lý học người Đức Hermann Ebbinghaus. Ông đã tiến hành một thí nghiệm, theo dõi khả năng nhớ hàng nghìn từ vựng vô nghĩa của mình và ghi chép việc quên của mình. Ông khám phá ra rằng ông đã quên hầu hết mọi thứ đã học trong vòng 24h đầu tiên và những thứ còn xót lại tiếp tục bị quên dần những ngày sau đó. Tuy nhiên, nhìn chung tốc độ quên những thứ đã học giảm dần khi chúng ta được chủ động xem lại kiến thức đó (không phải bị động) - mặc dù khi cyahúng ta dừng thực hành, bộ nhớ tiếp tục giảm. Do vậy, để học bất cứ thứ gì, bạn cần phải xem lại nó ngay lúc bộ não bắt đầu quên và thời gian giữa các lần review phải tăng dần. . forgeting_curve . Điều thú vị trong cách học này là bạn không phải học 1 thứ lặp đi lặp lại mà bạn chỉ học thứ mới hoặc thứ bạn cứ quên quài. . H&#7897;p Leitner . Đầu tiên, chúng ta chia hộp thành 7 cấp độ, bạn có thể có nhiều hoặc ít hơn nếu thích. Một card mới sẽ được bỏ vào hộp thứ 1, khuyến khich bắt đầu với 5 card mơi mỗi ngày. Khi bạn review một card, nếu bạn nhớ đúng kiến thức trong đó, chuyển card đó up một level. Nếu card của bạn ở level cuối cùng, chúc mừng bạn, bây giờ bạn có thể vứt chiệc card đó đi, kiến thức bên trong chiếc card đó sẽ theo bạn suốt đời. Tuy nhiên nếu bạn quên một card nào đó, bạn phải chuyển nó về hộp ban đầu, hộp đầu tiên. Thời gian review hộp leitner: . level 1: hằng ngày | level 2: 2 ngày một lần | level 3: 4 ngày một lần ... cứ như vậy, mỗi lần lên một bậc thì gấp đôi thời gian review Mỗi lần review, chúng ta sẽ review level từ trên xuống, như vậy level 1 sẽ được review cuối cùng. Như vậy chúng ta sẽ biết được những card mình bị quên và những card mới bỏ vào. Hằng ngày, cố gắng không để sót lại bất kì card nào ở level 1, học đi học lại đến khi nào mình nhớ nó và move nó lên level 2. | . review first =&gt; add new card later . mỗi ngày chúng ta cố dành khoảng 20-30 phút để học thay vì xem tivi - bạn có thể nhớ mọi thứ trên đời. Tuy nhiên để xây dựng được thới quen mới khá khó, nếu bạn khởi đầu lớn, có thể bạn sẽ kết thúc nó ngay ngày hôm sau. Nếu bạn khởi đầu nhỏ và lấy cảm hứng dần dần, bạn có thể học nhiều hơn mỗi ngày, vì vậy mà mình khuyến khích học 5 card mỗi ngày. . Sau khi chúng ta đã quen với cách học này, chúng ta có thể có 10,15,20,25,30 card/ngày. Nếu bạn học 30 card mỗi ngày, một năm bạn sẽ học được 10.000+ điều mới . Nh&#7919;ng vi&#7879;c c&#243; th&#7875; d&#7851;n t&#7899;i h&#7885;c sai . Việc học sẽ fail nếu những card của bạn cồng kềnh, không liên quan tới nhau và không có nghĩa. Mặc khác nếu những card của bạn là những mảng nhỏ, kết nôi vơi nhau thì sẽ tốt hơn. Đó cũng là cách nao bộ hoạt động, lots of small and connected things. Vấn đề không phải ở chỗ collection mà ở chỗ connection. | . =&gt; card của bạn phải nhỏ, kết nối, có ý nghĩa. . Nhỏ: quá nhiều thông tin trên một card =&gt; hãy cắt nhỏ nó ra thành nhiều card, smaller and connected pieces. =&gt; rule of thumb: mỗi card chỉ nếu có một và chỉ một idea. | connected: nếu bạn vẽ hình, ghi hoàn cảnh hoặc thông tin cá nhân lên card, sẽ gợi nhớ tốt hơn có card. | ý nghĩa: hãy chọn một topic nào đó mà bạn đang theo đuổi, học piano, đọc truyện, chơi game và bắt đầu dùng leitner box để học mọi thứ về bộ môn đó. Mình tin rằng cách tốt nhất để giữ motivation cho việc học là mình đọc học thứ gì đó mình quan tâm . | không quan trọng bạn học buổi sáng hay chiều, quan trọng là hằng ngày đều phải học (đôi khi có thể skip 1 ngày) . | . you cheat: correct answer, time, | you dont shuffle | you dont cart your victories | you dont educate yourself | your flashcard are wrong | . https://www.youtube.com/watch?v=HN0OUnLxFeU&amp;list=PLdddsM1tHEe-I7QpcFmoNsmwP3dWH_3s_ https://www.youtube.com/watch?v=hs5qmTKBSU0&amp;list=PLdddsM1tHEe-I7QpcFmoNsmwP3dWH_3s_&amp;index=3 https://www.youtube.com/watch?v=ad1nHS-3stg https://ncase.me/remember/ https://en.wikipedia.org/wiki/Leitner_system https://www.ankiapp.com/ https://tinycards.duolingo.com/ .",
            "url": "https://phucnsp.github.io/blog/tutorial/2020/04/20/phuong-phap-hoc-tap-leitner.html",
            "relUrl": "/tutorial/2020/04/20/phuong-phap-hoc-tap-leitner.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Explain Kaiming Initialization",
            "content": "nn.init.kaiminguniform this funciton implements the initialization recommendation from the sound paper Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | https://towardsdatascience.com/understand-kaiming-initialization-and-implementation-detail-in-pytorch-f7aa967e9138 | . | . Firstly, we take a brief look on notation of a standard neural network. The image below shows notation of 4 layer neural network. . The initialialization method proposed in this paper was to tackle the problem of hard convergence with randomly initialized weight drawn from Gaussian distribution. Earlier, there was another paper ,Xavier initialization, which also tackled this problem but they only considered linear layer and did not consider non-linear layer. . I will go along part 2.2 in the original paper,Delving Deep in Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, with some explanations in detail either in math or code. . The central idea is to investigate the impact of initialization in the variance of responses in each layer. . Forward Propagation Case . For each layer, the response is: begin{equation} mathbf{y}_{l}= mathrm{W}_{l} mathbf{x}_{l}+ mathbf{b}_{l} label{eq1} tag{1} end{equation} . begin{equation} mathbf{x}_{l}=f left( mathbf{y}_{l-1} right) label{eq2} tag{2} end{equation} To be consistent with the paper, `x` is used instead of `a` - response after activation fucntion. . A few assumption about the initialization: . Initialized elements of $ mathrm{W}_{l}$ and $ mathrm{x}_{l}$ are mutually independent and share the same distribution. | $ mathrm{W}_{l}$ and $ mathrm{x}_{l}$ are independent each other. | . Now, let do some transformation from the function ref{eq1}, ref{eq2} . Given $ mathrm{y&#39;}_{l}$, $ mathrm{x&#39;}_{l}$, $ mathrm{w&#39;}_{l}$ is the random variables of each element in $ mathrm{y}_{l}$, $ mathrm{x}_{l}$ and $ mathrm{W}_{l}$ respectively. Then we have: . begin{equation} begin{aligned} operatorname{Var} left[y&#39;_{l} right] &amp;= operatorname{Var} sum_{1}^{n_{l}} left(w&#39;_{l} x&#39;_{l} right) &amp;= sum_{1}^{n_{l}} operatorname{Var} left[w&#39;_{l} x&#39;_{l} right] &amp;= n_{l} operatorname{Var} left[w&#39;_{l} x&#39;_{l} right] end{aligned} label{eq3} tag{3} end{equation}Intuitively explaination: . for each node, such as node 1 in layer 2, its value will be sum of product of x and w. Therefore, the variance of y will be variance of sum of those products. Because all the w and x follow the same distribution (respectively), I am using a common notation $ mathrm{w&#39;}_{l}$$ mathrm{x&#39;}_{l}$ which represent those products. | $ mathrm{n}_{l-1}$ represent the number of product between $ mathrm{w&#39;}_{l}$ and $ mathrm{x&#39;}_{l}$ | Bias term $ mathrm{b}$ is ignore because it usually is initialized with a constant value, so it variance is 0. | Because w and x are independent, so equation 3 can be transformed to equation 4. | . begin{equation} begin{aligned} operatorname{Var} left[y&#39;_{l} right] &amp;= n_{l} operatorname{Var} left[w&#39;_{l} x&#39;_{l} right] &amp;= n_{l}( underbrace{ mathbb{E} left[{w&#39;}_{l}^{2} right]}_{= operatorname{Var} left[w&#39;_{l} right]} mathbb{E} left[{x&#39;}_{l}^{2} right]- underbrace{ mathbb{E} left[w&#39;_{l} right]^{2}}_{=0} mathbb{E} left[x&#39;_{l} right]^{2}) &amp;=n_{l} operatorname{Var} left[w&#39;_{l} right] mathbb{E} left[{x&#39;}_{l}^{2} right] end{aligned} label{eq4} tag{4} end{equation}Intuitively explaination: . assuming 2 random variables are independent, we can derive line 1 into line 2 by applying formular wiki formular variance . | By assuming random variable $ mathrm{w}_{l}$ has zero mean, we have: begin{equation} begin{aligned} operatorname{Var} left[w&#39;_{l} right] &amp;= mathbb{E} left[w&#39;^{2} right]- mathbb{E}[w&#39;]^{2} &amp;= mathbb{E} left[w&#39;^{2} right] end{aligned} label{eq5} tag{5} end{equation} | . But $ mathbb{E}[x&#39;]^{2} neq operatorname{Var} left[x&#39; right]$ because $ mathrm{E}[x&#39;]$ does not have zero mean, it is the result of ReLU function, $x_{l}= max left(0, y_{l-1} right)$, from previous layer. . begin{equation} begin{aligned} mathbb{E} left[{x&#39;}_{l}^{2} right] &amp;= mathbb{E} left[ max left(0, y&#39;_{l-1} right)^{2} right] &amp;= frac{1}{2} mathbb{E} left[{y&#39;}_{l-1}^{2} right] &amp;= frac{1}{2} operatorname{Var} left[y&#39;_{l-1} right] end{aligned} label{eq6} tag{6} end{equation}Intuitively explaination: . Assuming $ mathrm{w}_{l-1}$ has a symmetric distribution around 0 and $ mathrm{b}_{l-1}$ = 0 then $ mathrm{y}_{l-1}$ has zero mean and symmetric distribution around 0 =&gt; that&#39;s why we can derive equation ref{eq6}. | . begin{equation} begin{aligned} mathbb{E} left(y_{l-1} right) &amp;= mathbb{E} left(w_{l-1} x_{l-1} right) &amp;= mathbb{E} left(w_{l-1} right) mathbb{E} left(x_{l-1} right) &amp;=0 end{aligned} label{eq7} tag{7} end{equation} begin{equation} begin{aligned} mathbb{P} left(y_{l-1}&gt;0 right) &amp;= mathbb{P} left(w_{l-1} x_{l-1}&gt;0 right) &amp;= mathbb{P} left( left(w_{l-1}&gt;0 text { and } x_{l-1}&gt;0 right) text { or } left(w_{l-1}&lt;0 text { and } x_{l-1}&lt;0 right) right) &amp;= mathbb{P} left(w_{l-1}&gt;0 right) mathbb{P} left(x_{l-1}&gt;0 right)+ mathbb{P} left(w_{l-1}&lt;0 right) mathbb{P} left(x_{l-1}&lt;0 right) &amp;= frac{1}{2} mathbb{P} left(x_{l-1}&gt;0 right)+ frac{1}{2} mathbb{P} left(x_{l-1}&lt;0 right) &amp;= frac{1}{2} end{aligned} label{eq8} tag{8} end{equation}Plugging back to equation ref{eq4} we have: begin{equation} operatorname{Var} left[y_{l} right]= frac{1}{2} n_{l} operatorname{Var} left[w_{l} right] operatorname{Var} left[y_{l-1} right] label{eq9} tag{9} end{equation} . With L layers put together, we have: begin{equation} operatorname{Var} left[y_{L} right]= operatorname{Var} left[y_{1} right] left( prod_{l=2}^{L} frac{1}{2} n_{l} operatorname{Var} left[w_{l} right] right) label{eq10} tag{10} end{equation} . From here, the layer part on paper is quite clear, equation ref{eq10} is the key to the initialization design. A proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. So we expect the above product to take a proper scalar, eg 1. A sufficient condition is: begin{equation} frac{1}{2} n_{l} operatorname{Var} left[w_{l} right]=1, quad forall l label{eq11} tag{11} end{equation} . begin{equation} begin{aligned} Rightarrow operatorname{Var} left[w_{l} right] = frac{2}{n_{l}} Rightarrow mathbb{E} left[w_{l} right] = sqrt{ frac{2}{n_{l}}} end{aligned} label{eq12} tag{12} end{equation}Equation ref{12} is the He. initialilzation, together with b=0. . Note: from equation ref{eq6} we can see that if previous layer is not ReLU-kind, such as the first layer, we will have $ mathbb{E} left[{x&#39;}_{l}^{2} right] = operatorname{Var} left[y&#39;_{l-1} right]$ then $ mathbb{E} left[w_{l} right] = sqrt{ frac{1}{n_{l}}}$. But the factor 1/2 here does not matter if it just exists on one layer. So we adopt equation ref . Backward Propagation Case . Code implementation . https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/ https://medium.com/a-paper-a-day-will-have-you-screaming-hurray/day-8-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification-f449a886e604 . trace of matrix: deep learning page 44 | expectation, variance and covariance: deep learning page 58 | .",
            "url": "https://phucnsp.github.io/blog/self-learning/2020/04/20/explain-kaiming-initialization.html",
            "relUrl": "/self-learning/2020/04/20/explain-kaiming-initialization.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Roadmap to dominate in AI",
            "content": "dataquest.io | Andrew Ng Machine learning course | Andrew Ng. DL specialization | fastai course: 2018v1, 2018v2, 2019v1, npl course | deeplizard pytorch |",
            "url": "https://phucnsp.github.io/blog/self-taught/2020/04/20/Roadmap_AI.html",
            "relUrl": "/self-taught/2020/04/20/Roadmap_AI.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "This notebook tracks my progress in learning nlp",
            "content": "fastai nlp course . video 11 . nn.Embedding(len_vocab, len_vector_embedding) . like a matrix with nr_row = nr of words in vocab and nr_column = len_vector_represent_each_word = nr_features. This matrix can be imported from other network or can be learned from training. It is used to placed at the start of language-related networks to represent words input. | references: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/word_embeddings_tutorial.ipynb#scrollTo=rFc2HSo8SKo9 | . | nn.Linear . Parameter(tensor) | . | nn.init.kaiminguniform this funciton implements the initialization recommendation from the sound paper Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | https://towardsdatascience.com/understand-kaiming-initialization-and-implementation-detail-in-pytorch-f7aa967e9138 | | . | .",
            "url": "https://phucnsp.github.io/blog/self-learning/2020/04/19/self-taught-nlp.html",
            "relUrl": "/self-learning/2020/04/19/self-taught-nlp.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Giới thiệu phương pháp học Leitner",
            "content": "Gi&#7899;i thi&#7879;u . Những thứ không hiệu quả cho việc học như: bài giảng, nhồi nhét và đọc lại. . Phương pháp này dựa trên việc học tập một kiến thức mới lặp đi lặp lại, chúng ta kiểm tra trí nhớ của mình về một kiến thức nào đó lặp đi lặp lại và trong những khoảng thời gian khác nhau. Phương pháp này không phải là một study trick hay life hack mà là một cách để điều khiển bộ não của mình, giúp não bộ nhớ lâu hơn. Phương pháp này được phát minh bở nhà tâm lý học người Đức Hermann Ebbinghaus. Ông đã tiến hành một thí nghiệm, theo dõi khả năng nhớ hàng nghìn từ vựng vô nghĩa của mình và ghi chép việc quên của mình. Ông khám phá ra rằng ông đã quên hầu hết mọi thứ đã học trong vòng 24h đầu tiên và những thứ còn xót lại tiếp tục bị quên dần những ngày sau đó. Tuy nhiên, nhìn chung tốc độ quên những thứ đã học giảm dần khi chúng ta được chủ động xem lại kiến thức đó (không phải bị động) - mặc dù khi cyahúng ta dừng thực hành, bộ nhớ tiếp tục giảm. Do vậy, để học bất cứ thứ gì, bạn cần phải xem lại nó ngay lúc bộ não bắt đầu quên và thời gian giữa các lần review phải tăng dần. . forgeting_curve . Điều thú vị trong cách học này là bạn không phải học 1 thứ lặp đi lặp lại mà bạn chỉ học thứ mới hoặc thứ bạn cứ quên quài. . H&#7897;p Leitner . Đầu tiên, chúng ta chia hộp thành 7 cấp độ, bạn có thể có nhiều hoặc ít hơn nếu thích. Một card mới sẽ được bỏ vào hộp thứ 1, khuyến khich bắt đầu với 5 card mơi mỗi ngày. Khi bạn review một card, nếu bạn nhớ đúng kiến thức trong đó, chuyển card đó up một level. Nếu card của bạn ở level cuối cùng, chúc mừng bạn, bây giờ bạn có thể vứt chiệc card đó đi, kiến thức bên trong chiếc card đó sẽ theo bạn suốt đời. Tuy nhiên nếu bạn quên một card nào đó, bạn phải chuyển nó về hộp ban đầu, hộp đầu tiên. Thời gian review hộp leitner: . level 1: hằng ngày | level 2: 2 ngày một lần | level 3: 4 ngày một lần ... cứ như vậy, mỗi lần lên một bậc thì gấp đôi thời gian review Mỗi lần review, chúng ta sẽ review level từ trên xuống, như vậy level 1 sẽ được review cuối cùng. Như vậy chúng ta sẽ biết được những card mình bị quên và những card mới bỏ vào. Hằng ngày, cố gắng không để sót lại bất kì card nào ở level 1, học đi học lại đến khi nào mình nhớ nó và move nó lên level 2. | . review first =&gt; add new card later . mỗi ngày chúng ta cố dành khoảng 20-30 phút để học thay vì xem tivi - bạn có thể nhớ mọi thứ trên đời. Tuy nhiên để xây dựng được thới quen mới khá khó, nếu bạn khởi đầu lớn, có thể bạn sẽ kết thúc nó ngay ngày hôm sau. Nếu bạn khởi đầu nhỏ và lấy cảm hứng dần dần, bạn có thể học nhiều hơn mỗi ngày, vì vậy mà mình khuyến khích học 5 card mỗi ngày. . Sau khi chúng ta đã quen với cách học này, chúng ta có thể có 10,15,20,25,30 card/ngày. Nếu bạn học 30 card mỗi ngày, một năm bạn sẽ học được 10.000+ điều mới . Nh&#7919;ng vi&#7879;c c&#243; th&#7875; d&#7851;n t&#7899;i h&#7885;c sai . Việc học sẽ fail nếu những card của bạn cồng kềnh, không liên quan tới nhau và không có nghĩa. Mặc khác nếu những card của bạn là những mảng nhỏ, kết nôi vơi nhau thì sẽ tốt hơn. Đó cũng là cách nao bộ hoạt động, lots of small and connected things. Vấn đề không phải ở chỗ collection mà ở chỗ connection. | . =&gt; card của bạn phải nhỏ, kết nối, có ý nghĩa. . Nhỏ: quá nhiều thông tin trên một card =&gt; hãy cắt nhỏ nó ra thành nhiều card, smaller and connected pieces. =&gt; rule of thumb: mỗi card chỉ nếu có một và chỉ một idea. | connected: nếu bạn vẽ hình, ghi hoàn cảnh hoặc thông tin cá nhân lên card, sẽ gợi nhớ tốt hơn có card. | ý nghĩa: hãy chọn một topic nào đó mà bạn đang theo đuổi, học piano, đọc truyện, chơi game và bắt đầu dùng leitner box để học mọi thứ về bộ môn đó. Mình tin rằng cách tốt nhất để giữ motivation cho việc học là mình đọc học thứ gì đó mình quan tâm . | không quan trọng bạn học buổi sáng hay chiều, quan trọng là hằng ngày đều phải học (đôi khi có thể skip 1 ngày) . | . you cheat: correct answer, time, | you dont shuffle | you dont cart your victories | you dont educate yourself | your flashcard are wrong | . https://www.youtube.com/watch?v=HN0OUnLxFeU&amp;list=PLdddsM1tHEe-I7QpcFmoNsmwP3dWH_3s_ https://www.youtube.com/watch?v=hs5qmTKBSU0&amp;list=PLdddsM1tHEe-I7QpcFmoNsmwP3dWH_3s_&amp;index=3 https://www.youtube.com/watch?v=ad1nHS-3stg https://ncase.me/remember/ https://en.wikipedia.org/wiki/Leitner_system https://www.ankiapp.com/ https://tinycards.duolingo.com/ .",
            "url": "https://phucnsp.github.io/blog/tutorial/2020/04/19/phuong-phap-hoc-tap-leitner.html",
            "relUrl": "/tutorial/2020/04/19/phuong-phap-hoc-tap-leitner.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Explain Kaiming Initialization",
            "content": "nn.init.kaiminguniform this funciton implements the initialization recommendation from the sound paper Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification | https://towardsdatascience.com/understand-kaiming-initialization-and-implementation-detail-in-pytorch-f7aa967e9138 | . | . Firstly, we take a brief look on notation of a standard neural network. The image below shows notation of 4 layer neural network. . The initialialization method proposed in this paper was to tackle the problem of hard convergence with randomly initialized weight drawn from Gaussian distribution. Earlier, there was another paper ,Xavier initialization, which also tackled this problem but they only considered linear layer and did not consider non-linear layer. . I will go along part 2.2 in the original paper,Delving Deep in Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, with some explanations in detail either in math or code. . The central idea is to investigate the impact of initialization in the variance of responses in each layer. . Forward Propagation Case . For each layer, the response is: begin{equation} mathbf{y}_{l}= mathrm{W}_{l} mathbf{x}_{l}+ mathbf{b}_{l} label{eq1} tag{1} end{equation} . begin{equation} mathbf{x}_{l}=f left( mathbf{y}_{l-1} right) label{eq2} tag{2} end{equation} To be consistent with the paper, `x` is used instead of `a` - response after activation fucntion. . A few assumption about the initialization: . Initialized elements of $ mathrm{W}_{l}$ and $ mathrm{x}_{l}$ are mutually independent and share the same distribution. | $ mathrm{W}_{l}$ and $ mathrm{x}_{l}$ are independent each other. | . Now, let do some transformation from the function ref{eq1}, ref{eq2} . Given $ mathrm{y&#39;}_{l}$, $ mathrm{x&#39;}_{l}$, $ mathrm{w&#39;}_{l}$ is the random variables of each element in $ mathrm{y}_{l}$, $ mathrm{x}_{l}$ and $ mathrm{W}_{l}$ respectively. Then we have: . begin{equation} begin{aligned} operatorname{Var} left[y&#39;_{l} right] &amp;= operatorname{Var} sum_{1}^{n_{l}} left(w&#39;_{l} x&#39;_{l} right) &amp;= sum_{1}^{n_{l}} operatorname{Var} left[w&#39;_{l} x&#39;_{l} right] &amp;= n_{l} operatorname{Var} left[w&#39;_{l} x&#39;_{l} right] end{aligned} label{eq3} tag{3} end{equation}Intuitively explaination: . for each node, such as node 1 in layer 2, its value will be sum of product of x and w. Therefore, the variance of y will be variance of sum of those products. Because all the w and x follow the same distribution (respectively), I am using a common notation $ mathrm{w&#39;}_{l}$$ mathrm{x&#39;}_{l}$ which represent those products. | $ mathrm{n}_{l-1}$ represent the number of product between $ mathrm{w&#39;}_{l}$ and $ mathrm{x&#39;}_{l}$ | Bias term $ mathrm{b}$ is ignore because it usually is initialized with a constant value, so it variance is 0. | Because w and x are independent, so equation 3 can be transformed to equation 4. | . begin{equation} begin{aligned} operatorname{Var} left[y&#39;_{l} right] &amp;= n_{l} operatorname{Var} left[w&#39;_{l} x&#39;_{l} right] &amp;= n_{l}( underbrace{ mathbb{E} left[{w&#39;}_{l}^{2} right]}_{= operatorname{Var} left[w&#39;_{l} right]} mathbb{E} left[{x&#39;}_{l}^{2} right]- underbrace{ mathbb{E} left[w&#39;_{l} right]^{2}}_{=0} mathbb{E} left[x&#39;_{l} right]^{2}) &amp;=n_{l} operatorname{Var} left[w&#39;_{l} right] mathbb{E} left[{x&#39;}_{l}^{2} right] end{aligned} label{eq4} tag{4} end{equation}Intuitively explaination: . assuming 2 random variables are independent, we can derive line 1 into line 2 by applying formular wiki formular variance . | By assuming random variable $ mathrm{w}_{l}$ has zero mean, we have: begin{equation} begin{aligned} operatorname{Var} left[w&#39;_{l} right] &amp;= mathbb{E} left[w&#39;^{2} right]- mathbb{E}[w&#39;]^{2} &amp;= mathbb{E} left[w&#39;^{2} right] end{aligned} label{eq5} tag{5} end{equation} | . But $ mathbb{E}[x&#39;]^{2} neq operatorname{Var} left[x&#39; right]$ because $ mathrm{E}[x&#39;]$ does not have zero mean, it is the result of ReLU function, $x_{l}= max left(0, y_{l-1} right)$, from previous layer. . begin{equation} begin{aligned} mathbb{E} left[{x&#39;}_{l}^{2} right] &amp;= mathbb{E} left[ max left(0, y&#39;_{l-1} right)^{2} right] &amp;= frac{1}{2} mathbb{E} left[{y&#39;}_{l-1}^{2} right] &amp;= frac{1}{2} operatorname{Var} left[y&#39;_{l-1} right] end{aligned} label{eq6} tag{6} end{equation}Intuitively explaination: . Assuming $ mathrm{w}_{l-1}$ has a symmetric distribution around 0 and $ mathrm{b}_{l-1}$ = 0 then $ mathrm{y}_{l-1}$ has zero mean and symmetric distribution around 0 =&gt; that&#39;s why we can derive equation ref{eq6}. | . begin{equation} begin{aligned} mathbb{E} left(y_{l-1} right) &amp;= mathbb{E} left(w_{l-1} x_{l-1} right) &amp;= mathbb{E} left(w_{l-1} right) mathbb{E} left(x_{l-1} right) &amp;=0 end{aligned} label{eq7} tag{7} end{equation} begin{equation} begin{aligned} mathbb{P} left(y_{l-1}&gt;0 right) &amp;= mathbb{P} left(w_{l-1} x_{l-1}&gt;0 right) &amp;= mathbb{P} left( left(w_{l-1}&gt;0 text { and } x_{l-1}&gt;0 right) text { or } left(w_{l-1}&lt;0 text { and } x_{l-1}&lt;0 right) right) &amp;= mathbb{P} left(w_{l-1}&gt;0 right) mathbb{P} left(x_{l-1}&gt;0 right)+ mathbb{P} left(w_{l-1}&lt;0 right) mathbb{P} left(x_{l-1}&lt;0 right) &amp;= frac{1}{2} mathbb{P} left(x_{l-1}&gt;0 right)+ frac{1}{2} mathbb{P} left(x_{l-1}&lt;0 right) &amp;= frac{1}{2} end{aligned} label{eq8} tag{8} end{equation}Plugging back to equation ref{eq4} we have: begin{equation} operatorname{Var} left[y_{l} right]= frac{1}{2} n_{l} operatorname{Var} left[w_{l} right] operatorname{Var} left[y_{l-1} right] label{eq9} tag{9} end{equation} . With L layers put together, we have: begin{equation} operatorname{Var} left[y_{L} right]= operatorname{Var} left[y_{1} right] left( prod_{l=2}^{L} frac{1}{2} n_{l} operatorname{Var} left[w_{l} right] right) label{eq10} tag{10} end{equation} . From here, the layer part on paper is quite clear, equation ref{eq10} is the key to the initialization design. A proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. So we expect the above product to take a proper scalar, eg 1. A sufficient condition is: begin{equation} frac{1}{2} n_{l} operatorname{Var} left[w_{l} right]=1, quad forall l label{eq11} tag{11} end{equation} . begin{equation} begin{aligned} Rightarrow operatorname{Var} left[w_{l} right] = frac{2}{n_{l}} Rightarrow mathbb{E} left[w_{l} right] = sqrt{ frac{2}{n_{l}}} end{aligned} label{eq12} tag{12} end{equation}Equation ref{12} is the He. initialilzation, together with b=0. . Note: from equation ref{eq6} we can see that if previous layer is not ReLU-kind, such as the first layer, we will have $ mathbb{E} left[{x&#39;}_{l}^{2} right] = operatorname{Var} left[y&#39;_{l-1} right]$ then $ mathbb{E} left[w_{l} right] = sqrt{ frac{1}{n_{l}}}$. But the factor 1/2 here does not matter if it just exists on one layer. So we adopt equation ref . Backward Propagation Case . Code implementation . https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/ https://medium.com/a-paper-a-day-will-have-you-screaming-hurray/day-8-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification-f449a886e604 . trace of matrix: deep learning page 44 | expectation, variance and covariance: deep learning page 58 | .",
            "url": "https://phucnsp.github.io/blog/self-learning/2020/04/19/explain-kaiming-initialization.html",
            "relUrl": "/self-learning/2020/04/19/explain-kaiming-initialization.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Roadmap to dominate in AI",
            "content": "dataquest.io | Andrew Ng Machine learning course | Andrew Ng. DL specialization | fastai course: 2018v1, 2018v2, 2019v1, npl course | deeplizard pytorch |",
            "url": "https://phucnsp.github.io/blog/self-taught/2020/04/19/Roadmap_AI.html",
            "relUrl": "/self-taught/2020/04/19/Roadmap_AI.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Pytorch part 3 - training an CNN",
            "content": "In this part, we will go through a basic pipeline of training and debugging neural network which contains 4 main steps: . Prepare the data for training | Build the model | Train the model | Analyze the model&#39;s results | In reality there are 2 very improtant steps that we are ignoring here: . Data collection and labeling: this step comes before step 1 and is the most tricky and time consuming. | Deployment and testing: after finishing training, we need to deploy it to production. We will try to cover this step later. To be simple and pytorch oritented, we will use the same dataset like part 2, FASHION MNIST, but now we will require some helps from torchvision package. | . So now, let&#39;s get started! . Data and Data processing . Prepare the data for training - we are here | Build the model | Train the model | Analyze the model&#39;s results | Briefly remind, the Fashion-MNIST dataset which was designed to mirror the original MNIST dataset as closely as possible while introducing higher difficulty in training due to simply having more complex data than hand written images. . The dataset has the following ten classes of fashion items: . Index 0 1 2 3 4 5 6 7 8 9 . Label | T-shirt/top | Trouser | Pullover | Dress | Coat | Sandal | Shirt | Sneaker | Bag | Ankle boot | . A sample of the items look like this: . . Firstly we need to import all the necessary packages. . # import common Pytorch packages import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F import torchvision import torchvision.transforms as transforms # import other common packages used for data science import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix import pdb # python debugger torch.set_printoptions(linewidth=120) # set print options for pytorch printe statements . We&#39;ll follow ETL process using torchvision: . Extract: Get the Fashion-MNIST image data from the source. | Transform: Put our data into tensor form. | Load: Put our data into an object to make it easily accessible. | . For these purposes, PyTorch provides us with two classes: . torch.utils.data.Dataset: an abstract class for representing a dataset. An abstract class is a Python class that has methods we must implement, in our case are __getitem__ and __len__. In order to create a custom dataset, we need to subclass the Dataset class and override __len__, that provides the size of the dataset, and __getitem__, supporting integer indexing in range from 0 to len(self) exclusive. Upon doing this, our new subclass can then be passed to the a PyTorch DataLoader object. | torch.utils.data.DataLoader: wraps a dataset and provides access to the underlying data. | . The fashion-MNIST dataset that comes built-in with the torchvision package, so we won’t have to do the steps mentioned above for creatign a new dataset. Just know that the Fashion-MNIST built-in dataset class is doing this behind the scenes. When we run the code below for the first time, the Fashion-MNIST dataset will be downloaded locally. Subsequent calls check for the data before downloading it. Thus, we don&#39;t have to worry about double downloads or repeated network calls. . train_set = torchvision.datasets.FashionMNIST( root=&#39;./data&#39;, # The location on disk where the data is located. train=True, # If the dataset is the training set download=True, # If the data should be downloaded. transform=transforms.Compose([ # A composition of transformations performed on dataset elements. transforms.ToTensor() ]) ) . Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw Processing... Done! . Wrap the dataset into dataloader and load our first batch of sammple. . train_dl = torch.utils.data.DataLoader(train_set, batch_size=4, shuffle=True) # the batch will be different each time a call next occurs. images, labels = next(iter(train_dl)) images.shape, labels.shape . (torch.Size([4, 1, 28, 28]), torch.Size([4])) . The size of each dimention in the image tensor is defined as (batch size, number of color channels, image height, image width). The labels tensor has a single axis with a shape of 4, which corresponds to the 4 images inside our batch. One label for each image. . So now, we have our desired dataset and dataloader in order to feed into the neural net. We can play around with the dataset to better understand it. . # the number of samples in our dataset len(train_set) . 60000 . # label of all 6000 images in train_set train_set.targets . tensor([9, 0, 0, ..., 3, 0, 5]) . # classes to predict train_set.classes . [&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;] . # number of samples per class train_set.targets.bincount() . tensor([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000]) . # take a look on a single item ys[0].item() plt.imshow(xs[0].squeeze(), cmap=&quot;gray&quot;) . &lt;matplotlib.image.AxesImage at 0x1a2a0ee790&gt; . # take a look on a batch grid = torchvision.utils.make_grid(xs, nrow=10) plt.figure(figsize=(15,15)) plt.imshow(grid.permute(1,2,0)) # plt.imshow(np.transpose(grid, (1,2,0))) . &lt;matplotlib.image.AxesImage at 0x1a2a383e90&gt; . Neural Net design with Pytorch . Prepare the data for training | Build the model - we are here | Train the model | Analyze the model&#39;s results | We’re building a CNN, so the two types of layers we&#39;ll use are linear layers and convolutional layers. In order to build model using Pytorch, we have 3 essential steps: . Create a neural network class that extends the nn.Module base class. torch.nn is PyTorch’s neural network (nn) library which contains the primary components to construct network&#39;s layers. | Within the torch.nn package, there is a class called Module, and it is the base class for all of neural network modules which includes layers. All of the layers in PyTorch need to extend this base class in order to inherit all of PyTorch’s built-in functionality within the nn.Module class. | For each layer, there are two primary items encapsulated inside, a forward function definition and a weight tensor which is updated as the network learns during the training process. PyTorch&#39;s neural network module class keeps track of the weight tensors inside each layer as learnable parameters of our network. Take a look at Random topics section for more detail info about network parameters. | . | Define the network&#39;s layers as class attributes in the class constructor. We have two convolutional layers, self.conv1 and self.conv2, and three linear layers, self.fc1, self.fc2, self.out. | We used the abbreviation fc in fc1 and fc2 because linear layers are also called fully connected layers or dense layer. So linear = dense = fully connected. | . | Define the network&#39;s forward method using the network&#39;s layer attributes as well nn.functional API operations. Every Pytorch nn.Module has a forward() method and so when we are building layers and networks, we must provide an implementation of the forward() method. The forward method is the actual transformation. | The tensor input is passed forward though each layer transformation until the tensor reaches the output layer. The composition of all the individual layer forward passes defines the overall forward pass transformation for the network. The goal of the overall transformation is to transform or map the input to the correct prediction output class, and during the training process, the layer weights (data) are updated in such a way that cause the mapping to adjust to make the output closer to the correct prediction. | When we implement the forward() method of our nn.Module subclass, we will typically use functions from the nn.functional package. This package provides us with many neural network operations that we can use for building layers. In fact, many of the nn.Module layer classes use nn.functional functions to perform their operations. | . | #collapse-show import torch.nn as nn import torch.nn.functional as F class Network(nn.Module): # Extend the nn.Module base class. def __init__(self): # Define layers as class attributes using pre-built layers from torch.nn super().__init__() # Extend the nn.Module base class. self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5) self.fc1 = nn.Linear(in_features=12*4*4, out_features=120) # linear = dense = fc self.fc2 = nn.Linear(in_features=120, out_features=60) self.out = nn.Linear(in_features=60, out_features=10) def forward(self, x): # Implement the forward() using network’s layer attributes and method from the nn.functional # (1) input layer print(f&quot;tensor shape before input layer 1: {x.shape}&quot;) x = x # (2) hidden conv layer print(f&quot;tensor shape before conv layer 2: {x.shape}&quot;) x = self.conv1(x) print(f&quot;tensor shape after conv operation: {x.shape}&quot;) x = F.relu(x) print(f&quot;tensor shape after relu operation: {x.shape}&quot;) x = F.max_pool2d(x, kernel_size=2, stride=2) print(f&quot;tensor shape after max_pool2d operation: {x.shape}&quot;) # (3) hidden conv layer print(f&quot;tensor shape before conv layer 3: {x.shape}&quot;) x = self.conv2(x) x = F.relu(x) x = F.max_pool2d(x, kernel_size=2, stride=2) # (4) hidden linear layer print(f&quot;tensor shape before linear layer 4: {x.shape}&quot;) x = x.flatten(start_dim=1) # x.reshape(-1, 12*4*4) x = self.fc1(x) x = F.relu(x) # (5) hidden linear layer print(f&quot;tensor shape before linear layer 5: {x.shape}&quot;) x = self.fc2(x) x = F.relu(x) # (6) output layer print(f&quot;tensor shape before output layer 6: {x.shape}&quot;) x = self.out(x) # x = F.softmax(x, dim=1) print(f&quot;tensor shape after output layer 6: {x.shape}&quot;) return x . . Now let&#39;s feed a batch of images to our model and get its output. . model = Network() model . Network( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=192, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=60, bias=True) (out): Linear(in_features=60, out_features=10, bias=True) ) . preds = model(images) print(f&quot; nprediction tensor: n{preds}&quot;) . tensor shape before input layer 1: torch.Size([4, 1, 28, 28]) tensor shape before conv layer 2: torch.Size([4, 1, 28, 28]) tensor shape after conv operation: torch.Size([4, 6, 24, 24]) tensor shape after relu operation: torch.Size([4, 6, 24, 24]) tensor shape after max_pool2d operation: torch.Size([4, 6, 12, 12]) tensor shape before conv layer 3: torch.Size([4, 6, 12, 12]) tensor shape before linear layer 4: torch.Size([4, 12, 4, 4]) tensor shape before linear layer 5: torch.Size([4, 120]) tensor shape before output layer 6: torch.Size([4, 60]) tensor shape after output layer 6: torch.Size([4, 10]) prediction tensor: tensor([[ 0.1019, 0.0483, 0.0173, 0.0009, -0.0897, 0.0271, 0.0504, 0.0375, -0.0684, 0.0975], [ 0.1034, 0.0475, 0.0144, 0.0004, -0.0898, 0.0264, 0.0559, 0.0403, -0.0646, 0.0972], [ 0.1083, 0.0468, 0.0131, 0.0014, -0.0858, 0.0244, 0.0559, 0.0406, -0.0670, 0.0975], [ 0.1042, 0.0481, 0.0165, -0.0039, -0.0911, 0.0290, 0.0476, 0.0394, -0.0651, 0.0957]], grad_fn=&lt;AddmmBackward&gt;) . One thing to point out that Pytorch neural network modules are callable Python objects - we called the Network instance as if it were a function. What makes this possible is that PyTorch module classes implement a special Python function called __call__(). which will be invoked anytime the object instance is called. After the object instance is called, the __call__() method is invoked under the hood, and the __call__() in turn invokes the forward() method. Instead of calling the forward() method directly, we call the object instance. This applies to all PyTorch neural network modules, namely, networks and layers. . In order to access the model parameters, we can use parameters() or named_parameters() method. To keep track of all the weight tensors inside the network. PyTorch has a special class called Parameter. The weight tensor inside every layer is an instance of this Parameter class. PyTorch’s nn.Module class is basically looking for any attributes whose values are instances of the Parameter class, and when it finds an instance of the parameter class, it keeps track of it. . for param in model.parameters(): print(param.shape) . torch.Size([6, 1, 5, 5]) torch.Size([6]) torch.Size([12, 6, 5, 5]) torch.Size([12]) torch.Size([120, 192]) torch.Size([120]) torch.Size([60, 120]) torch.Size([60]) torch.Size([10, 60]) torch.Size([10]) . for name, param in model.named_parameters(): print(f&quot;{name}: {param.shape}&quot;) . conv1.weight: torch.Size([6, 1, 5, 5]) conv1.bias: torch.Size([6]) conv2.weight: torch.Size([12, 6, 5, 5]) conv2.bias: torch.Size([12]) fc1.weight: torch.Size([120, 192]) fc1.bias: torch.Size([120]) fc2.weight: torch.Size([60, 120]) fc2.bias: torch.Size([60]) out.weight: torch.Size([10, 60]) out.bias: torch.Size([10]) . Ok so now, we will go through each layer in order to inspect, debug it. One basic technique is to inspect tensor shape at each layer. Remember, the shape of a tensor really encodes all the information we need to know about the tensor. . Input layer 1 . tensor shape before input layer 1: torch.Size([1, 1, 28, 28]) tensor shape before conv layer 2: torch.Size([1, 1, 28, 28]) . This value in each of these dimensions represent the following values: (batch size, color channels, height, width). Since the input layer is just the identity function, the output shape doesn’t change. . Hidden conv layer 2 . tensor shape after relu operation: torch.Size([1, 6, 24, 24]) tensor shape after max_pool2d operation: torch.Size([1, 6, 12, 12]) . Our convolution layer includes 3 operations: convolution, relu and max_pooling 2d. Now we will go through each operation. Be attention that the batch_size is fixed as we move though the entire forward pass. . Convolution Operation: . tensor shape before conv layer 2: torch.Size([1, 1, 28, 28]) tensor shape after conv operation: torch.Size([1, 6, 24, 24]) . model.conv1 . Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) . model.conv1.weight.shape, model.conv1.bias.shape . (torch.Size([6, 1, 5, 5]), torch.Size([6])) . Some keynotes about this operation: . This operation convolves the input tensor using 6 randomly initialized 5x5 layers, also known as filters or kernels, to produce 6 output channels. The stride tells the conv layer how far the filter should slide after each operation in the overall convolution. | explain parameters of this operation: kernel_size: Sets the filter size. The words kernel and filter are interchangeable. | out_channels: Sets the number of filters. One filter produces one output channel. | out_features: Sets the size of the output tensor. | . | The weight tensor is a rank-4 tensor and reflects (nr of filters, depth, height, width). | The filters have depth that accounts for the input channels. | The parameter out_channels defined in self.conv1 determines how many filters will be created. However, we dont actually have 6 distinct tensors but all tensors are represented using a single weight tensor. | After convolution, the output channels are generated which is also called feature maps. This is due to the fact that the pattern detection that emerges as the weights are updated represent features like edges and other more sophisticated patterns. | One pattern that shows up quite often is that we increase our out_channels as we add additional conv layers, and after we switch to linear layers we shrink our out_features as we filter down to our number of output classes. | . Relu operation: . This operation maps all negative values to 0. This means that all the values in the tensor are now positive. This function is pure operation and does not have weights. That&#39;s why we call them directly from the nn.functional. . Max pooling operation: . This operation extracts the max value from each 2x2 section of the fix feature maps that were created by the convolutions. The same as relu, this operation does not have weight and is called directly from nn.functional. Sometimes we may see pooling operations referred to as pooling layers. However, what makes a layer distinct from an operation is that layers have weights. Since pooling operations and activation functions do not have weights, we will refer to them as operations and view them as being added to the collection of layer operations. . Let&#39;s have a look at the formula for computing the output size of the tensor after performing convolutional and pooling operations. Given $n_{h} times n_{w}$ input, $f_{h} times f_{w}$ filter, padding $p$ and stride $s$. The height, width of the output size $O_{h}$, $O_{w}$ is defined by this formular: . begin{equation} O_{h}= frac{n_{h}-f_{h}+2 p}{s}+1; O_{w}= frac{n_{w}-f_{w}+2 p}{s}+1 end{equation} Hidden conv layer 3 . tensor shape before conv layer 3: torch.Size([1, 6, 12, 12]) tensor shape before linear layer 4: torch.Size([1, 12, 4, 4]) . model.conv2 . Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1)) . model.conv2.weight.shape, model.conv2.bias.shape . (torch.Size([12, 6, 5, 5]), torch.Size([12])) . This layer transforms the tensor in the same was as self.conv1 and reduces the height and width dimensions further. However, each filter now has a depth of 6 which come from the 6 output channels from the first convolution layer. The shape of the resulting output of self.conv2 allows us to see why we reshape, also called flattening, the tensor using 12*4*4 before passing the tensor to the first linear layer, self.fc1. The resulting shape is 1x192. The 1 in this case represents the batch size, and the 192 is the number of elements in the tensor that are now in the same dimension. . linear layer 4, 5 . tensor shape before linear layer 4: torch.Size([1, 12, 4, 4]) tensor shape before linear layer 5: torch.Size([1, 120]) tensor shape before output layer 6: torch.Size([1, 60]) tensor shape after output layer 6: torch.Size([1, 10]) . print(model.fc1) model.fc1.weight.shape, model.fc1.bias.shape . Linear(in_features=192, out_features=120, bias=True) . (torch.Size([120, 192]), torch.Size([120])) . print(model.fc2) model.fc2.weight.shape, model.fc2.bias.shape . Linear(in_features=120, out_features=60, bias=True) . (torch.Size([60, 120]), torch.Size([60])) . print(model.out) model.out.weight.shape, model.out.bias.shape . Linear(in_features=60, out_features=10, bias=True) . (torch.Size([10, 60]), torch.Size([10])) . Now, we just have a series of linear layers followed by non-linear activation function until we reach the output layer. Linear layers use matrix multiplication to transform their in_features to out_features. For example self.fc1 has in_feature&#39;s shape (1, 192) and out_feature&#39;s shape (1, 120). Its weight matrix will have the shape of (120,192). In general, the weight matrix defines a linear function that maps a 1-dimensional tensor with 192 elements to a 1-dimensional tensor that has 120 elements. The weight matrix is auto generated with randomly initizalized weights when we create an instance of linear layer class. . Given $A$, $x$, $b$, $y$ are Weight matrix tensor, Input tensor, Bias tensor and Output tensor, respectively. Mathematical notation of the linear transformation is: begin{equation} y=A x+b end{equation} . output layer 6 . tensor shape before output layer 6: torch.Size([1, 60]) tensor shape after output layer 6: torch.Size([1, 10]) . prediction tensor: tensor([[-0.0197, 0.1284, -0.1158, -0.0096, -0.0135, 0.0599, -0.0663, 0.0724, -0.0030, -0.0063], [-0.0270, 0.1266, -0.1168, -0.0085, -0.0073, 0.0600, -0.0675, 0.0805, -0.0065, -0.0103], [-0.0255, 0.1297, -0.1183, -0.0134, -0.0149, 0.0569, -0.0709, 0.0623, -0.0059, -0.0065], [-0.0316, 0.1310, -0.1178, -0.0057, -0.0064, 0.0585, -0.0710, 0.0796, -0.0054, -0.0063]], grad_fn=&lt;AddmmBackward&gt;) . The prediction tensor has a shape of (4, 10) which reflects (batch_size, number of prediction classes). The number of prediction classes is 10 because we have 10 classes, each number is the assigned value of the specific output class. . Inside the network we usually use relu() as our non-linear activation function, but for the output layer, we use softmax(). The softmax function returns a positive probability for each of the prediction classes, and the probabilities sum to 1. However, in our case, we won&#39;t use softmax() because the loss function that we&#39;ll use, F.cross_entropy(), implicitly performs the softmax() operation on its input, so we&#39;ll just return the result of the last linear transformation. (save one operation at inference^^) . To check the predictions against the labels, we use the argmax() function to figure out which index contains the highest prediction value. Once we know which index has the highest prediction value, we can compare the index with the label to see if there is a match. . F.softmax(preds) . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. &#34;&#34;&#34;Entry point for launching an IPython kernel. . tensor([[0.0976, 0.1131, 0.0886, 0.0986, 0.0982, 0.1057, 0.0931, 0.1070, 0.0992, 0.0989], [0.0969, 0.1130, 0.0886, 0.0987, 0.0988, 0.1057, 0.0930, 0.1079, 0.0989, 0.0985], [0.0973, 0.1137, 0.0887, 0.0985, 0.0984, 0.1057, 0.0930, 0.1063, 0.0993, 0.0992], [0.0964, 0.1134, 0.0885, 0.0989, 0.0989, 0.1055, 0.0927, 0.1078, 0.0990, 0.0989]], grad_fn=&lt;SoftmaxBackward&gt;) . pred_after_argmax = torch.argmax(F.softmax(preds), dim=1) pred_after_argmax . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. &#34;&#34;&#34;Entry point for launching an IPython kernel. . tensor([1, 1, 1, 1]) . Now we can compare the prediciton result against the label tensor by using eq operation which will do element-wise comparision between prediction tensor and label tensor. And then we will sum the correct prediction to see how many predictions are correct. . pred_after_argmax.eq(labels).sum() . tensor(2) . Training NN . We are now ready to begin the training process. The training process is an iterative process which including following steps: . Get batch from the training set. . Since we have 60,000 samples in our training set, we will have 60,000 / 100 = 600 iterations. Something to notice, batch_size will directly impact to the number of times the weights updated. In our case, the weights will be updated 600 times by the end of each loop. So far, there is no rule-of-thump for selecting the value of batch size so we still need to do trial and error to figure out the best value. | . | Pass batch to network. . | Calculate the loss (difference between the predicted values and the true values). . To be notice, the function cross_entropy has a parameter called reduction which has 3 option - none, mean, sum. By default, this variable is set to mean which will average the loss value by batch_size. None and sum means there is no reduction applied and the output will be summed, respectively. | . | Calculate the gradient of the loss function w.r.t the network&#39;s weights. . Before calculate the gradient, we need to zero out these gradients, by calling optimizer.zero_grad(), because after we call the loss.backward() method, the gradients will be calculated and added to the grad attributes of our network&#39;s parameters. | Calculating the gradients is very easy using PyTorch. Since PyTorch has created a computation graph under the hood. As our tensor flowed forward through our network, all of the computations where added to the graph. The computation graph is then used by PyTorch to calculate the gradients of the loss function with respect to the network&#39;s weights. | . | Update the weights using the gradients to reduce the loss. . The gradients calculated from step 4 are used by the optimizer to update the respective weights. | . | Repeat steps 1-5 until one epoch is completed. . | Repeat steps 1-6 for as many epochs required to reach the minimum loss. | To create our optimizer, we use the torch.optim package that has many optimization algorithm implementations that we can use, for example: Adam. The model&#39;s parameters and learning rate are passed to the optimizer. lr tells optimizer how far to step in the direction of minimizing loss fct while Parameters provide Adam the ability to access gradients. . model = Network() train_dl = torch.utils.data.DataLoader(train_set, batch_size=100) optimizer = torch.optim.Adam(model.parameters(), lr=0.01) accuracy = lambda preds, lbs: (F.softmax(preds).argmax(dim=1) == lbs).sum().item() for epoch in range(3): total_loss = 0 total_acc = 0 for batch in train_dl: images, labels = batch # 1. Get batch from the training set. preds = model(images) # 2. Pass batch to network. loss = F.cross_entropy(preds, labels) # 3. Calculate the loss optimizer.zero_grad() loss.backward() # 4. Calculate the gradient optimizer.step() # 5. Update the weights total_loss += loss.item() total_acc += accuracy(preds, labels) nr_iters_per_epoch = len(train_dl) print(f&quot;epoch: {epoch} navg_loss: {total_loss/nr_iters_per_epoch} navg_acc : {total_acc/nr_iters_per_epoch}&quot;) . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. after removing the cwd from sys.path. . epoch: 0 avg_loss: 0.6262958686550458 avg_acc : 76.13 epoch: 1 avg_loss: 0.4225387443850438 avg_acc : 84.28333333333333 epoch: 2 avg_loss: 0.37975253333648046 avg_acc : 85.75166666666667 . Analyze the model&#39;s results . Analyse classification result using a Confusion Matrix . The confusion matrix will show us which categories the model is predicting correctly and which categories the model is predicting incorrectly. For the incorrect predictions, this will show us which categories are confusing the model. . Now, if we compare the two tensors element-wise and count the number of predicted labels vs the target labels, the values inside the two tensors act as coordinates for our matrix. Let&#39;s stack these two tensors along the second dimension so we can have 60,000 ordered pairs. . len(train_set), len(train_set.targets) . (60000, 60000) . # Get predictions for the entire training set @torch.no_grad() def get_all_preds(model, loader): all_preds = torch.tensor([]) for batch in loader: imgs, lbs = batch preds = model(imgs) all_preds = torch.cat((all_preds, preds), dim=0) return all_preds with torch.no_grad(): prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000) train_preds = get_all_preds(model, prediction_loader) stacked = torch.stack( ( train_set.targets, train_preds.argmax(dim=1) ), dim=1) stacked.shape . torch.Size([60000, 2]) . # method 1: manually calculate confusion matrix cmt = torch.zeros(10,10, dtype=torch.int64) for p in stacked: tl, pl = p.tolist() cmt[tl,pl] += 1 cmt . tensor([[5523, 1, 46, 59, 15, 2, 306, 0, 48, 0], [ 36, 5829, 3, 77, 15, 0, 32, 0, 8, 0], [ 144, 6, 3704, 51, 1359, 3, 603, 0, 129, 1], [ 554, 26, 2, 5077, 189, 0, 135, 0, 15, 2], [ 49, 3, 99, 161, 5298, 1, 316, 0, 72, 1], [ 13, 0, 0, 0, 0, 5671, 0, 178, 81, 57], [1443, 10, 244, 75, 693, 0, 3414, 0, 121, 0], [ 1, 0, 0, 0, 0, 38, 0, 5823, 19, 119], [ 45, 0, 4, 8, 17, 1, 34, 4, 5884, 3], [ 0, 0, 0, 0, 0, 23, 0, 261, 6, 5710]]) . # method 2: use sklearn import matplotlib.pyplot as plt from sklearn.metrics import confusion_matrix cm = confusion_matrix(train_set.targets, train_preds.argmax(dim=1)) cm . array([[5523, 1, 46, 59, 15, 2, 306, 0, 48, 0], [ 36, 5829, 3, 77, 15, 0, 32, 0, 8, 0], [ 144, 6, 3704, 51, 1359, 3, 603, 0, 129, 1], [ 554, 26, 2, 5077, 189, 0, 135, 0, 15, 2], [ 49, 3, 99, 161, 5298, 1, 316, 0, 72, 1], [ 13, 0, 0, 0, 0, 5671, 0, 178, 81, 57], [1443, 10, 244, 75, 693, 0, 3414, 0, 121, 0], [ 1, 0, 0, 0, 0, 38, 0, 5823, 19, 119], [ 45, 0, 4, 8, 17, 1, 34, 4, 5884, 3], [ 0, 0, 0, 0, 0, 23, 0, 261, 6, 5710]]) . To nicely plot the confusion matrix, we can use below util function. . import itertools import numpy as np import matplotlib.pyplot as plt def plot_confusion_matrix(cm, classes, normalize=False, title=&#39;Confusion matrix&#39;, cmap=plt.cm.Blues): if normalize: cm = cm.astype(&#39;float&#39;) / cm.sum(axis=1)[:, np.newaxis] print(&quot;Normalized confusion matrix&quot;) else: print(&#39;Confusion matrix, without normalization&#39;) print(cm) plt.imshow(cm, interpolation=&#39;nearest&#39;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) fmt = &#39;.2f&#39; if normalize else &#39;d&#39; thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; thresh else &quot;black&quot;) plt.tight_layout() plt.ylabel(&#39;True label&#39;) plt.xlabel(&#39;Predicted label&#39;) . names = ( &#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39; ) plt.figure(figsize=(10,10)) plot_confusion_matrix(cm, names) . Confusion matrix, without normalization [[5523 1 46 59 15 2 306 0 48 0] [ 36 5829 3 77 15 0 32 0 8 0] [ 144 6 3704 51 1359 3 603 0 129 1] [ 554 26 2 5077 189 0 135 0 15 2] [ 49 3 99 161 5298 1 316 0 72 1] [ 13 0 0 0 0 5671 0 178 81 57] [1443 10 244 75 693 0 3414 0 121 0] [ 1 0 0 0 0 38 0 5823 19 119] [ 45 0 4 8 17 1 34 4 5884 3] [ 0 0 0 0 0 23 0 261 6 5710]] . The confusion matrix has three axes: . Prediction label (class) | True label | Heat map value (color) | . The prediction label and true labels show us which prediction class we are dealing with. The matrix diagonal represents locations in the matrix where the prediction and the truth are the same, so this is where we want the heat map to be darker. . Any values that are not on the diagonal are incorrect predictions because the prediction and the true label don&#39;t match. To read the plot, we can use these steps: . Choose a prediction label on the horizontal axis. | Check the diagonal location for this label to see the total number correct. | Check the other non-diagonal locations to see where the network is confused. | For example, the network is confusing a T-shirt/top with a shirt, but is not confusing the T-shirt/top with things like: Ankle boot, Sneaker, Sandal. If we think about it, this makes pretty good sense. As our model learns, we will see the numbers that lie outside the diagonal become smaller and smaller. . Analyse training loop by using TensorBoard with PyTorch . Tensorboard provides visualization and tooling needed for machine learning experimentation. With tensorboard, we can: . Tracking and visualizing metrics such as loss and accuracy | Visualizing the model graph (ops and layers) | Viewing histograms of weights, biases, or other tensors as they change over time | Projecting embeddings to a lower dimensional space | Displaying images, text, and audio data | Profiling TensorFlow programs | And much more | . It is a font-end web interface that essentially reads data from a file and displays it. PyTorch has created a utility class called SummaryWriter which will help us get the data out of our program, save on disk so as to tensorboard can read. To use tensorboard, PyTorch version 1.1.0 is required. . import torch print(f&quot; torch version: {torch.__version__}&quot;) . torch version: 1.4.0 . !pip install tensorboard==1.15.0; from torch.utils.tensorboard import SummaryWriter . The SummaryWriter class comes with a bunch of method that we can call to selectively pick and choose which data we want to be available to TensorBoard. We will add 4 types of data to tensorboard for visualization: . images | graph | scalar value | histogram | These added values are even updated and showed in real-time on tensorboard as the network trains. . It is helpful to see the loss and accuracy values over time. However, the real power of TensorBoard is its out-of-the-box capability of comparing multiple runs. This allows us to rapidly experiment by varying the hyperparameter values and comparing runs to see which parameters are working best. . One more thing to be notices, with PyTorch&#39;s SummaryWriter a run starts when the writer object instance is created and ends when the writer instance is closed or goes out of scope. . model = Network() train_dl = torch.utils.data.DataLoader(train_set, batch_size=10) optimizer = torch.optim.Adam(model.parameters(), lr=0.01) accuracy = lambda preds, lbs: (F.softmax(preds).argmax(dim=1) == lbs).sum().item() imgs, lbls = next(iter(train_dl)) grid = torchvision.utils.make_grid(imgs) tb = SummaryWriter() tb.add_image(&#39;image&#39;, grid) #1. Add a batch of images to the writer tb.add_graph(model, imgs) #2. Add graph of our model to the writer for epoch in range(3): total_loss = 0 total_acc = 0 for batch in train_dl: imgs, lbls = batch preds = model(imgs) loss = F.cross_entropy(preds, lbls) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss total_acc += accuracy(preds, lbls) nr_batch = len(train_set) #3. Add scalar values, average loss and average accuracy, to display on tensorboard over time or over epoch. tb.add_scalar(&#39;Average_Loss&#39;, total_loss/nr_batch, epoch) tb.add_scalar(&#39;Average_Accuracy&#39;, total_acc/nr_batch, epoch) #4. Add values to histograms to see its frequency distributions. tb.add_histogram(&#39;conv1.bias&#39;, model.conv1.bias, epoch) tb.add_histogram(&#39;conv1.weight&#39;, model.conv1.weight, epoch) tb.add_histogram(&#39;conv1.weight.grad&#39;, model.conv1.weight.grad, epoch) print(f&quot;epoch: {epoch} navg_loss: {total_loss/nr_batch} navg_acc : {total_acc/nr_batch}&quot;) tb.close() . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. after removing the cwd from sys.path. . epoch: 0 avg_loss: 0.06133313849568367 avg_acc : 0.7699666666666667 epoch: 1 avg_loss: 0.05296260863542557 avg_acc : 0.80595 epoch: 2 avg_loss: 0.051954615861177444 avg_acc : 0.8122833333333334 . By default, the PyTorch SummaryWriter object writes the data to disk in a directory called ./runs that is created in the current working directory. To launch TensorBoard, we have 2 ways: . run and show directly on jupyter notebook | run the tensorboard command at our terminal and open http://localhost:6006 on browser. | To be noticed, the defaut save path and open port are able to change if you want. . # 1. run and show directly on jupyter notebook %load_ext tensorboard %tensorboard --logdir=runs --port=6006 . # 2. run the tensorboard command at our terminal and open `http://localhost:6006` on browser. !tensorboard --logdir=runs . TensorFlow installation not found - running with reduced feature set. TensorBoard 1.15.0 at http://VN0130.local:6006/ (Press CTRL+C to quit) ^C . Hyperparameter Experimenting - Training Neural Networks . The best part about TensorBoard is its out-of-the-box capability of tracking our hyperparameters over time and across runs. We can use TensorBoard to rapidly experiment with different training hyperparameters comparing the results. . To uniquely identify each run, we can either set the file name of the run directly, or pass a comment string to the constructor that will be appended to the auto-generated file name. . Note the cross_entropy function with reduction=mean has averaged the loss over batch size and our total_loss is then summed up. It is correct if we have a fix batch size and its value is divisible by number of sample (if not, last batch will has less sample than others). However, in the case of batch size vary, we have to change reduction=sum. . If we have a list of parameters, we can package them up into a set for each of our runs using the Cartesian product. The Cartesian product takes multiple sets as arguments and return a set of all ordered pairs. Note that this is equivalent to nested for-loops. . from itertools import product # itertools&#39; Cartesian product implementation # define the list of parameters and their value list that we want to tolerate parameters = dict( lr = [0.1, 0.01], batch_size = [10, 100], ) param_values = [v for v in parameters.values()] # use the Cartesian product to iterate over the pairs of parameters&#39;values for lr, batch_size in product(*param_values): print(lr, batch_size) . 0.1 10 0.1 100 0.01 10 0.01 100 . accuracy = lambda preds, lbs: (F.softmax(preds).argmax(dim=1) == lbs).sum().item() model = Network() for lr, batch_size in product(*param_values): train_dl = torch.utils.data.DataLoader(train_set, batch_size=batch_size) optimizer = torch.optim.Adam(model.parameters(), lr=lr) imgs, lbls = next(iter(train_dl)) grid = torchvision.utils.make_grid(imgs) comment = f&quot; batch_size = {batch_size}, lr = {lr}&quot; tb = SummaryWriter(comment=comment) tb.add_image(&#39;images&#39;, grid) tb.add_graph(model, imgs) for epoch in range(3): total_loss = 0 total_acc = 0 for batch in train_dl: imgs, lbls = batch preds = model(imgs) loss = F.cross_entropy(preds, lbls, reduction=&#39;sum&#39;) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() total_acc += accuracy(preds, lbls) tb.add_scalar(&#39;average_loss&#39;, total_loss/len(train_set), epoch) tb.add_scalar(&#39;average_acc&#39;, total_acc/len(train_set), epoch) for name,param in model.named_parameters(): tb.add_histogram(name, param, epoch) tb.add_histogram(f&quot;{name}.grad&quot;, param.grad, epoch) print(f&quot;epoch: {epoch} navg_loss: {total_loss/len(train_set)} navg_acc : {total_acc/len(train_set)}&quot;) tb.close() . /Users/phucnsp/anaconda3/envs/fastai2/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument. &#34;&#34;&#34;Entry point for launching an IPython kernel. . epoch: 0 avg_loss: 2.3510418080329893 avg_acc : 0.10213333333333334 epoch: 1 avg_loss: 2.3241496319452923 avg_acc : 0.10206666666666667 epoch: 2 avg_loss: 2.3241496464411417 avg_acc : 0.10208333333333333 epoch: 0 avg_loss: 2.31076997756958 avg_acc : 0.09978333333333333 epoch: 1 avg_loss: 2.3104461242675782 avg_acc : 0.0994 epoch: 2 avg_loss: 2.3104453076680502 avg_acc : 0.09963333333333334 epoch: 0 avg_loss: 2.3050063188870746 avg_acc : 0.10093333333333333 epoch: 1 avg_loss: 2.3049483956654866 avg_acc : 0.1014 epoch: 2 avg_loss: 2.3049483927408856 avg_acc : 0.1014 epoch: 0 avg_loss: 2.3035935297648114 avg_acc : 0.10016666666666667 epoch: 1 avg_loss: 2.303564562733968 avg_acc : 0.09968333333333333 epoch: 2 avg_loss: 2.303563890838623 avg_acc : 0.09968333333333333 . !tensorboard --logdir=runs . TensorFlow installation not found - running with reduced feature set. TensorBoard 1.15.0 at http://VN0130.local:6007/ (Press CTRL+C to quit) ^C . We can refactor the way we manage parameters by using the following RunBuilder Class. The method get_runs() will get the runs for us that it builds based on the parameters we pass in. . from collections import OrderedDict from collections import namedtuple from itertools import product class RunBuilder(): @staticmethod def get_runs(params): # create new tuple subclass called Run with named fields got from params.keys Run = namedtuple(&#39;Run&#39;, params.keys()) runs = [] # use Cartesian product to pair parameters&#39;values and pass to Run. for v in product(*params.values()): runs.append(Run(*v)) return runs params = OrderedDict( lr = [.01, .001], batch_size = [10, 100] ) runs = RunBuilder.get_runs(params) runs . [Run(lr=0.01, batch_size=10), Run(lr=0.01, batch_size=100), Run(lr=0.001, batch_size=10), Run(lr=0.001, batch_size=100)] . We can see that the RunBuilder class has built and returned a list of four runs. Each of these runs has a learning rate and a batch size that defines the run. Notice the string representation of the run output. This string representation was automatically generated for us by the Run tuple class, and this string can be used to uniquely identify the run if we want to write out run statistics to disk for TensorBoard or any other visualization program. Additionally, because the run is object is a tuple with named attributes, we can access the values using dot notation like so: . run = runs[0] run.lr, run.batch_size . (0.01, 10) . All we have to do to add additional values is to add them to the original parameter list, and if we want to add an additional type of parameter, all we have to do is add it. The new parameter and its values will automatically become available to be consumed inside the run. The string output for the run also updates as well. . This functionality will allow us to have greater control as we experiment with different values during training. . accuracy = lambda preds, lbs: (F.softmax(preds).argmax(dim=1) == lbs).sum().item() model = Network() # for lr, batch_size in product(*param_values): # old code for run in RunBuilder.get_runs(params): # Changes for RunBuilder train_dl = torch.utils.data.DataLoader(train_set, batch_size=run.batch_size) # Changes for RunBuilder optimizer = torch.optim.Adam(model.parameters(), lr=run.lr) # Changes for RunBuilder imgs, lbls = next(iter(train_dl)) grid = torchvision.utils.make_grid(imgs) # comment = f&quot; batch_size = {batch_size}, lr = {lr}&quot; # old code comment = f&#39;-{run}&#39; # Changes for RunBuilder tb = SummaryWriter(comment=comment) tb.add_image(&#39;images&#39;, grid) tb.add_graph(model, imgs) for epoch in range(3): total_loss = 0 total_acc = 0 for batch in train_dl: imgs, lbls = batch preds = model(imgs) loss = F.cross_entropy(preds, lbls, reduction=&#39;sum&#39;) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() total_acc += accuracy(preds, lbls) tb.add_scalar(&#39;average_loss&#39;, total_loss/len(train_set), epoch) tb.add_scalar(&#39;average_acc&#39;, total_acc/len(train_set), epoch) for name,param in model.named_parameters(): tb.add_histogram(name, param, epoch) tb.add_histogram(f&quot;{name}.grad&quot;, param.grad, epoch) print(f&quot;epoch: {epoch} navg_loss: {total_loss/len(train_set)} navg_acc : {total_acc/len(train_set)}&quot;) tb.close() . Speeding up the training process by increasing num_workers . To speed up the training process, we will make use of the num_workers optional attribute of the DataLoader class. By setting this number to 1, default set to 0, the speep can increase up to 20% and does not improve more or even worse with setting more than 1. . The num_workers attribute tells the data loader instance how many sub-processes to use for data loading. By default, the num_workers value is set to zero, the training process will work sequentially inside the main process. After a batch is used during the training process and another one is needed, we read the batch data from disk. Now, if we have a worker process, we can make use of the fact that our machine has multiple cores. This means that the next batch can already be loaded and ready to go by the time the main process is ready for another batch. This is where the speed up comes from. If we add more batches to the queue doesn&#39;t mean the batches are being processes faster because we are bounded by the time it takes to forward and backward propagate a given batch. . Random topics . Neural net layer parameters . Parameter Vs Argument . Parameters are used in function definitions as place-holders while arguments are the actual values that are passed to the function. The parameters can be thought of as local variables that live inside a function. In our network&#39;s case, the names are the parameters and the values that we have specified are the arguments. . Types Of Parameters: . A lot of terms in deep learning are used loosely, and the word parameter is one of them. The goal of these particular categories is to help us remember how each parameter&#39;s value is decided. . Hyperparameters: hyperparameters are parameters whose values are picked arbitrarily. The hyperparameters are used to construct our network’s architecture and are mainly chosen based on trial and error. | Convolutional layers have three parameters: in_channels, out_channels, kernel_size. | Linear layers have 2 parameters: in_features, out_features. | All of these parameters impact the network&#39;s architecture. Specifically, these parameters directly impact the weight tensors inside the layers. | . | Data dependent hyperparameters. . Data dependent hyperparameters are parameters whose values are dependent on data. | Some candidates from network above are: in_channels parameter of the conv layer: depend on the number of channels of previous layer. | in_features parameter of the linear layer: depend on the data coming from previous layer. | out_features parameter of the output layer: depend on the number of classes of our training set. | . | . | Learnable Parameters . Learnable parameters are parameters whose values are learned during the training process. | With learnable parameters, we typically start out with a set of arbitrary values, and these values then get updated in an iterative fashion as the network learns. Appropriate values are values that minimize the loss function. | The learnable parameters are the weights inside our network, and they live inside each layer. | . | . Stack vs Concat . Concatenating joins a sequence of tensors along an existing axis, and stacking joins a sequence of tensors along a new axis. . import torch t1 = torch.tensor([1,1,1]) t2 = torch.tensor([2,2,2]) t3 = torch.tensor([3,3,3]) . # Now, let’s concatenate these with one another. Notice that each of these tensors have a single axis. This # means that the result of the cat function will also have a single axis. This is because when we concatenate, # we do it along an existing axis. Notice that in this example, the only existing axis is the first axis. # Alright, so we took three single axis tensors each having an axis length of three, and now we have a single # tensor with an axis length of nine. torch.cat((t1,t2,t3),dim=0).shape . torch.Size([9]) . # Now, let’s stack these tensors along a new axis that we’ll insert. We’ll insert an axis at the first index. # Note that this insertion will be happening implicitly under the hood by the stack function. # This gives us a new tensor that has a shape of 3 x 3. Notice how the three tensors are concatenated along # the first axis of this tensor. Note that we can also insert the new axis explicitly, and preform the # concatenation directly. torch.stack((t1,t2,t3), dim=0).shape . torch.Size([3, 3]) . # example of combining stack and concat # Joining Images With An Existing Batch # Suppose we have the same three separate image tensors. Only, this time, we already have a batch tensor. # Assume our task is to join these three separate images with the batch. import torch batch = torch.zeros(3,3,28,28) t1 = torch.zeros(3,28,28) t2 = torch.zeros(3,28,28) t3 = torch.zeros(3,28,28) torch.cat( ( batch ,torch.stack( (t1,t2,t3) ,dim=0 ) ) ,dim=0 ).shape . # Joining Batches Into A Single Batch import torch t1 = torch.zeros(1,3,28,28) t2 = torch.zeros(1,3,28,28) t3 = torch.zeros(1,3,28,28) torch.cat( (t1,t2,t3) ,dim=0 ).shape . torch.Size([3, 3, 28, 28]) . # Joining Images Into A Single Batch import torch t1 = torch.zeros(3,28,28) t2 = torch.zeros(3,28,28) t3 = torch.zeros(3,28,28) torch.stack( (t1,t2,t3) ,dim=0 ).shape . torch.Size([3, 3, 28, 28]) . How many types of number of DL? . In deep learning, there are 2 types of numbers: parameters and activation. Parameters are numbers that are learned. Activations are numbers that are calculated (by affine functions &amp; element-wise non-linearities). The hidden state is actuall a set of activation. . Reference . Some good sources: . pytorch zero to all | deeplizard | effective pytorch | what is torch.nn really? | recommend walk with pytorch | official tutorial | DL(with Pytorch) | Pytorch project template | nlp turorial with pytorch | UDACITY course | awesome pytorch list | deep learning with pytorch | others: https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc | Grokking Algorithms: An illustrated guide for programmers and other curious people 1st Edition | . | .",
            "url": "https://phucnsp.github.io/blog/self-taught/tutorial/2020/03/22/self-taught-pytorch-part3-training-nn.html",
            "relUrl": "/self-taught/tutorial/2020/03/22/self-taught-pytorch-part3-training-nn.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Pytorch part 2 - neural net from scratch",
            "content": "In part 1 of this serie, we have gone through the basic elements of neural network. In this part we will start writing a program. . Computer programs in general consist of two primary components, code and data. With traditional programming, the programmer’s job is to explicitly write the software or code to perform computations. But with deep learning and neural networks, this job explicitly belongs to the optimization algorithm. It will compile our data into code which is actually neural net&#39;s weights. The programmer’s job is to oversee and guide the learning process though training. We can think of this as an indirect way of writing software or code. . In order to have an overview of where we are, we will briefly talk about fundamental stages of an machine learning/deep learning project. . Project planning and project setup Gather team, define requirements, goals and allocate resources | Data collection and labelling Define which data to collect and label them | Training and debugging: we are here Start implementing, debugging and improving model. This serie will focus on this stage using Pytorch framework. | Deploying and testing Write tests to prevent regresison, roll out in production. | So now, let&#39;s get started! . Pytorch provides the elegantly designed modules and classes to help you create and train neural networks. Following are the most fundamental modules which we will repeatly work with along the way. . Package Description . torch | The top-level PyTorch package and tensor library. | . torch.nn | A subpackage that contains modules and extensible classes for building neural networks. | . torch.nn.functional | A functional interface that contains typical operations used for building neural networks like loss functions and convolutions. | . torch.autograd | handle the automatic differentiation of arbitrary scalar valued functions. | . torch.optim | A subpackage that contains standard optimization operations like SGD and Adam. | . torchvision | A package that provides access to popular datasets, model architectures, and image transformations for computer vision. | . torchvision.transforms | An interface that contains common transforms for image processing. | . In order to fully understand exactly what ther are doing, we will create and train a very basic neural network from scratch which initially only use the most basic Pytorch tensor functionality and gradually refactor it with those Pytorch built-in modules. . Data setup . Data is the primary ingredient of deep learning. Before feeding data into our network, we need to consider many aspects such as: . Who created the dataset? | How was the dataset created? | What transformations were used? | What intent does the dataset have? | Possible unintentional consequences? | Is the dataset biased? | Are there ethical issues with the dataset? | . In this tutorial, we will use the well-prepared Fashion-MNIST dataset which was created by research lab of Zalando - a German based multi-national fashion commerce company. The dataset was designed to mirror the original MNIST dataset as closely as possible while introducing higher difficulty in training due to simply having more complex data than hand written images. The abstract from its paper: . We present Fashion-MNIST, a new dataset comprising of 28 × 28 grayscale images of 70, 000 fashion products from 10 categories, with 7, 000 images per category. The training set has 60, 000 images and the test set has 10, 000 images. Fashion-MNIST is intended to serve as a direct dropin replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist. . The Fashion-MNIST was built, unlike the hand-drawn MNIST dataset, from actual images on Zalando’s website. However, they have been transformed to more closely correspond to the MNIST specifications. This is the general conversion process that each image from the site went through: . Converted to PNG | Trimmed | Resized | Sharpened | Extended | Negated | Gray-scaled | . The dataset has the following ten classes of fashion items: . idx2clas = {0 : &quot;T-shirt/top&quot;, 1 : &quot;Trouser&quot;, 2 : &quot;Pullover&quot;, 3 : &quot;Dress&quot;, 4 : &quot;Coat&quot;, 5 : &quot;Sandal&quot;, 6 : &quot;Shirt&quot;, 7 : &quot;Sneaker&quot;, 8 : &quot;Bag&quot;, 9 : &quot;Ankle boot&quot;} . A sample of the items look like this: . . That&#39;s enough background information about the dataset. Now we will prepare data for our network. . The general idea of this step is to transform our dataset into tensor format so we can take advantages of GPU&#39;s parallel computing for later steps such as data augmentation, training model, etc. . We&#39;ll follow the ETL process to prepare data: . Extract: Get the Fashion-MNIST image data from the source. | Transform: Put our data into tensor form. | Load: Put our data into an object to make it easily accessible. | . The Fashion-MNIST source code can be accessed here. We will use pathlib for dealing with paths and will download 4 parts - training set images, training set labels, test set images and test set labels - using requests library. Since this dataset has been stored using pickle, a python-specific format for serializing data, we need to unzip and deserialize it in order to read the content. . # a. extract data from source from pathlib import Path import requests PATH_ROOT = Path(&quot;data/fashion_mnist&quot;) PATH_ROOT.mkdir(parents=True, exist_ok=True) URLs = [ &quot;http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz&quot;, &quot;http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz&quot;, &quot;http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz&quot;, &quot;http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz&quot; ] def download_data(path_root, url): filename = Path(url).name if not (path_root / filename).exists(): content = requests.get(url).content (path_root / filename).open(&quot;wb&quot;).write(content) for url in URLs: download_data(PATH_ROOT, url) . def load_data(path, kind=&#39;train&#39;): import os import gzip import numpy as np &quot;&quot;&quot;Load MNIST data from `path`&quot;&quot;&quot; labels_path = os.path.join(path, &#39;%s-labels-idx1-ubyte.gz&#39; % kind) images_path = os.path.join(path, &#39;%s-images-idx3-ubyte.gz&#39; % kind) with gzip.open(labels_path, &#39;rb&#39;) as lbpath: labels = np.frombuffer(lbpath.read(), dtype=np.uint8, offset=8) with gzip.open(images_path, &#39;rb&#39;) as imgpath: images = np.frombuffer(imgpath.read(), dtype=np.uint8, offset=16).reshape(len(labels), 784) return images, labels x_train, y_train = load_data(PATH_ROOT, kind=&#39;train&#39;) x_valid, y_valid = load_data(PATH_ROOT, kind=&#39;t10k&#39;) . . Note: I am using the work valid and test interchangeble but in reality they are different and will be covered in later part. . This dataset is in numpy array format. Each image is 28*28 and is being stored as a flattened row of length 784. Let&#39;s reshape and take a look at one. . x_train.shape, y_train.shape . ((60000, 784), (60000,)) . y_train.max(), y_train.min() . (9, 0) . x_valid.shape, y_valid.shape . ((10000, 784), (10000,)) . y_valid.max(), y_valid.min() . (9, 0) . from matplotlib import pyplot as plt import numpy as np plt.imshow(x_train[0].reshape(28,28), cmap=&quot;gray&quot;) idx2clas[y_train[0].item()] . &#39;Ankle boot&#39; . Now we will transform data into tensor format. . import torch # transform 1. convert to tensor x_train, x_valid = map(lambda p: torch.tensor(p, dtype=torch.float32), (x_train, x_valid)) y_train, y_valid = map(lambda p: torch.tensor(p, dtype=torch.int64), (y_train, y_valid)) . And normalize the image tensor. This is standard step in image processing which helps faster convergence. . # transform 2. normalize images mean = x_train.mean() std = x_train.std() print(f&quot;mean: {mean}, std: {std}&quot;) x_train, x_valid = map(lambda p: (p - mean)/std, (x_train, x_valid)) . mean: 72.9342041015625, std: 90.02118682861328 . x_train.mean(), x_train.std() . (tensor(-4.0474e-07), tensor(1.0000)) . x_valid.mean(), x_valid.std() . (tensor(0.0023), tensor(0.9984)) . . Important: we need to set dtype=torch.float32 in order to be able to make matrix multiplication later on with linear layer&#8217;s weight matrix. Two tensor have to have the same datatype and device so as to do operations. And finally, load data into an object to make it easily accessible. This task normally is handled by Pytorch DataLoader object but since we are building everything from scratch, let&#39;s use the traditional for loop to access batches of data. . # c. load batches of data batch_size = 64 nr_iters = len(x_train) // batch_size for i in range((nr_iters + 1)): start_index = i * batch_size end_index = start_index + batch_size xs = x_train[start_index:end_index] ys = y_train[start_index:end_index] print(xs.shape) print(ys.shape) . Create and training simple neural net from scratch . For our 10-class classification problem, we will create a simple network which contains only a linear layer and a non-linear layer - softmax. . In general, a layer contains 2 parts: . data: represents the state of that layer. In particular, they are weight and bias - learnable parameters which are updated/learned during training process. | transformation: the operation which transform layer&#39;s input to output using learnable parameters. | . Linear layers&#39;s data is weight matrix tensor and bias tensor while its transformation is the matrix multiplication. The weight matrix defines a linear function that maps a 1-dimentional tensor with 784 elements to a 1-dimensional tensor with 10 elements. . Briefly remind the mathematical function of linear layer. Given $A$, $x$, $b$, $y$ are Weight matrix tensor, Input tensor, Bias tensor and Output tensor, respectively. Mathematical notation of the linear transformation is: begin{equation} y=A x+b end{equation} . We will initialize the weight matrix tensor following the recommendation from Xavier initialisation paper. This paper tackled the problem with randomly initialized weight drawn from Gaussian distribution which caused hard convergence of deep network. . Tip: - we set requires_grad after initialization, since we don&#8217;t want that step included in the gradident. - The trailing _ in Pytorch signifies that the operation is performed in-place. . import math weights = torch.randn(784,10) / math.sqrt(784) weights.requires_grad_() bias = torch.zeros(10, requires_grad=True) . Thanks to Pytorch&#39;s ability to calculate gradients automatically, we can use any standard Python function (or callable object) as a model. The log_softmax function is implemented using log-sum-exp trick for numerically stable. We will not go to detail this trick but you can go here or here for details explanation. The formular for this trick is: $$ log _softmax(x) = x - logsumexp(x) $$ . def log_softmax(x): return x - x.exp().sum(-1).log().unsqueeze(-1) def simplenet(x): return log_softmax(x @ weights + bias) . . Tip: @ stand for dot product operation. Now we have had a simple network and data setup. Let&#39;s try to predict a batch. . bs = 16 xs, ys = x_train[:bs], y_train[:bs] preds = simplenet(xs) preds.shape . torch.Size([16, 10]) . preds . tensor([[-1.4405, -3.5105, -2.7565, -1.4291, -2.4875, -2.0924, -4.0225, -4.0119, -2.5381, -2.2188], [-4.8259, -3.7734, -2.0315, -2.3298, -0.9287, -2.6093, -3.6926, -2.2518, -1.9918, -5.2320], [-2.6215, -2.3225, -1.9596, -1.8593, -2.6342, -2.3268, -1.6219, -2.6988, -2.7874, -3.3023], [-3.2222, -3.1183, -1.6531, -1.5975, -2.2923, -2.7484, -1.7113, -2.3899, -2.6972, -4.0560], [-4.0472, -4.3383, -2.3563, -1.5838, -0.7511, -2.6288, -3.0189, -3.5576, -3.2544, -4.6537], [-2.2369, -3.2984, -0.9843, -2.8722, -2.3741, -1.8116, -4.1416, -5.1253, -2.9522, -2.3493], [-1.8748, -4.8596, -2.7152, -1.2111, -3.2741, -2.0744, -2.4669, -2.8398, -2.7450, -2.2655], [-2.7887, -4.9238, -1.3429, -3.5693, -2.6362, -1.4553, -5.4066, -6.6005, -3.2211, -1.2336], [-3.9236, -2.5255, -2.9410, -0.6889, -4.0574, -3.9685, -2.6249, -1.6157, -4.2080, -3.7726], [-1.7502, -1.8132, -1.7902, -1.9453, -3.3830, -3.7103, -2.6871, -3.1987, -2.2021, -2.5852], [-3.7683, -3.3224, -2.6373, -2.0115, -0.7844, -2.8311, -2.6774, -2.6067, -2.6543, -4.9239], [-3.2636, -3.2367, -1.4613, -2.2395, -2.4437, -2.0885, -2.9661, -3.5086, -1.3777, -3.2262], [-1.4874, -4.1664, -2.1750, -1.5891, -3.1738, -2.5524, -2.2160, -3.7046, -2.8008, -2.0662], [-2.6747, -3.6151, -2.5766, -1.4708, -3.7484, -3.1546, -1.2979, -3.1983, -2.5499, -1.9650], [-2.2535, -4.0095, -3.1679, -0.7156, -3.5641, -2.4768, -2.4170, -3.0718, -3.2315, -2.8394], [-1.1524, -4.3791, -3.3212, -1.0543, -3.4922, -2.1228, -4.1253, -3.9749, -2.9178, -3.0380]], grad_fn=&lt;SubBackward0&gt;) . What we did above is one forward pass, we load a batch of image add feed it through the network. The result will not be better than a random prediction at this stage because we start with random weights. . As we see in the preds tensor, it contains not only the tensor values but also a gradient function. As mentioned in part 1, pytorch use dynamic computational graph to track function operations that occur on tensors. These graph are then used to compute the derivatives. . preds.grad_fn . &lt;SubBackward0 at 0x12bde0310&gt; . Now we need to define the loss function which is the model&#39;s objective. Our weights and bias will be updated in the direction which make this loss decreased. One of the most common loss function is negative log-likelihood. . def nll(input, target): return -input[range(input.shape[0]), target.tolist()].mean() loss_function = nll loss_function(preds, ys) . tensor(2.9355, grad_fn=&lt;NegBackward&gt;) . Next, We will define a metric. During the training, reducing the loss is what our model tries to do but it is hard for us, as human, can intuitively understand how good the weights set are along the way. So we need a human-interpretable value which help us understand the training progress and it is the metric. . def accuracy(input, target): return (torch.argmax(input, dim=1) == target).float().mean() accuracy(preds, ys) . tensor(0.1250) . We are now ready to begin the training process. The training process is an iterative process which including following steps: . Get batch from the training set. Since we have 60,000 samples in our training set, we will have 60,000 / 100 = 600 iterations. Something to notice, batch_size will directly impact to the number of times the weights updated. In our case, the weights will be updated 600 times by the end of each loop. So far, there is no rule-of-thump for selecting the value of batch size so we still need to do trial and error to figure out the best value. . Important: to be simple, I am not shuffling the training set at this stage. In reality, the training set should be shuffled to prevent correlation between batches and overfitting. If we keep feeding the network batch-by-batch in order, the network might remember this order and causes overfitting with this order. On the other hand, the validation loss will be identical whether we shuffle the validation set or not. Since shuffling takes extra time, it makes no sense to shuffle the validation data. | . | Pass batch to network. . | Calculate the loss (difference between the predicted values and the true values). . | Calculate the gradient of the loss function w.r.t the network&#39;s weights. . Calculating the gradients is very easy using PyTorch. Since PyTorch has created a computation graph under the hood. As our tensor flowed forward through our network, all of the computations where added to the graph. The computation graph is then used by PyTorch to calculate the gradients of the loss function with respect to the network&#39;s weights. | . | Update the weights using the gradients to reduce the loss. . The gradients calculated from step 4 are used by the optimizer to update the respective weights. | We have disabled PyTorch gradient tracking at this step because we don&#39;t want these actions to be recorded for our next calculation of the gradient. There are many ways to disable this functionality, please check Random topics at the end of notebook for more information. | After updating the weight, we need to zero out the gradients because the gradients will be calculated and added to the grad attributes of our network&#39;s parameters after calling loss.backward() at the next iteration. | . | Repeat steps 1-5 until one epoch is completed. . | Calculate mean loss of validation set . Note: We can use a batch size for the validation set that is twice as large as that for the training set. This is because the validation set does not need backpropagation and thus takes less memory (it doesn’t need to store the gradients). We take advantage of this to use a larger batch size and compute the loss more quickly. . | Repeat steps 1-6 for as many epochs required to reach the minimum loss. | We will use Stochastic Gradient Descent (SGD) optimizer to update our learnable parameters during training. lr tells the optimizer how far to step in the direction of minimizing loss function. . lr = 0.01 epochs = 5 batch_size = 64 nr_iters = len(x_train) // bs for epoch in range(epochs): for i in range((nr_iters + 1)): # step 1. get batch of training set start_index = i * batch_size end_index = start_index + batch_size xs = x_train[start_index:end_index] ys = y_train[start_index:end_index] # step 2. pass batch to network preds = simplenet(xs) # step 3. calculate the loss loss = loss_function(preds, ys) # step 4. calculate the gradient of the loss w.r.t the network&#39;s parameters loss.backward() with torch.no_grad(): # step 5. update the weights using SGD algorithm weights -= lr * weights.grad bias -= lr * bias.grad weights.grad.zero_() bias.grad.zero_() # step 6. calculate mean of valid loss after each epoch to see the improvement batch_size_valid = batch_size * 2 nr_iters_valid = len(x_valid) // batch_size_valid total_loss = 0 total_acc = 0 with torch.no_grad(): for i in range((nr_iters_valid + 1)): start_index = i * batch_size_valid end_index = start_index + batch_size_valid xs = x_valid[start_index:end_index] ys = y_valid[start_index:end_index] preds = simplenet(xs) loss = loss_function(preds, ys) total_loss += loss.item() * xs.shape[0] acc = accuracy(preds, ys) total_acc += acc.item() * xs.shape[0] print(f&quot;epoch {epoch}, valid_loss {total_loss / len(x_valid)}, accuracy {total_acc / len(x_valid)}&quot;) . epoch 0, valid_loss 0.5325431520462036, accuracy 0.8139 epoch 1, valid_loss 0.49808417506217956, accuracy 0.8252 epoch 2, valid_loss 0.48265715327262876, accuracy 0.8305 epoch 3, valid_loss 0.4735100971221924, accuracy 0.8335 epoch 4, valid_loss 0.46733067717552185, accuracy 0.8356 . During the first training epoches, the valid loss should decrease and the accuracy should increase. Otherwise, you did something wrong. . Refactor neural network with Pytorch modules . We will gradually refactor our simple net with Pytorch built-in modules, so that it does the same thing as before but start taking advantage of Pytorch&#39;s modules to make it more concise, more understandable and/or flexible. To make things more gradual and more understandable, the refactoring will be divided into 3 stages. . Refactor stage 1 . Refactor loss fuction with torch.nn.functional.cross_entropy function. | Refactor model with nn.Module, nn.Parameter class. | Refactor optimization algorithm with model.parameters and model.zero_grad method. | . We first will refactor the log_softmax and nll method with Pytorch built-in function torch.nn.functional.cross_entropy that combines the two. So we can even remove the activation function from our model. . import torch.nn.functional as F # old functions # def log_softmax(x): return x - x.exp().sum(-1).log().unsqueeze(-1) # def simplenet(x): return log_softmax(x @ weights + bias) # new functions loss_function = F.cross_entropy def simplenet(x): return x @ weights + bias . Next we will refactor our simplenet using torch.nn module. . torch.nn is PyTorch’s neural network (nn) library which contains the primary components to construct network&#39;s layers. Within the torch.nn package, there is a class called Module, and it is the base class for all of neural network modules, including layers. All of the layers in PyTorch need to extend this base class in order to inherit all of PyTorch’s built-in functionality within the nn.Module class. . Note: nn.Module (uppercase M) is a Pytorch specific concept, and is a class we&#8217;ll be using a lot. Do not confuse with the Python concept of a (lowercase m) module, which is a file of Python code that can be imported. In order to create model using nn.Module, we have 3 essential steps: . Create a neural network class that extends the nn.Module base class. | Define the network&#39;s layers as class attributes in __init__ method. The layers&#39;s learnable parameters are initialized in this step. But they need to be wrapped in nn.Parameters class in order to help nn.Module know those are learnable parameter. The weight tensor inside every layer is an instance of this Parameter class. PyTorch’s nn.Module class is basically looking for any attributes whose values are instances of the Parameter class, and when it finds an instance of the parameter class, it keeps track of it. Take a look at Random topics section for more detail information about network parameters. | . | Define the network&#39;s transformation (operation) in forward method. Every Pytorch nn.Module has a forward() method and so when we are building layers and networks, we must provide an implementation of the forward() method. The forward method is the actual transformation. | The tensor input is passed forward though each layer transformation until the tensor reaches the output layer. The composition of all the individual layer forward passes defines the overall forward pass transformation for the network. The goal of the overall transformation is to transform or map the input to the correct prediction output class, and during the training process, the layer weights (data) are updated in such a way that cause the mapping to adjust to make the output closer to the correct prediction. | When we implement the forward() method of our nn.Module subclass, we will typically use layers&#39;attributes and functions from the nn.functional package. This package provides us with many neural network operations that we can use for building layers. In fact, many of the nn.Module layer classes use nn.functional functions to perform their operations. | . | import math import torch.nn as nn # old codes # weights = torch.randn(784,10) / math.sqrt(784) # weights.requires_grad_() # bias = torch.zeros(10, requires_grad=True) # def simplenet(x): return log_softmax(x @ weights + bias) # new code class SimpleNet(nn.Module): def __init__(self): super().__init__() self.weights = nn.Parameter(torch.randn(784,10) / math.sqrt(784)) self.bias = nn.Parameter(torch.zeros(10)) def forward(self, x): return x @ self.weights + self.bias simplenet = SimpleNet() . One thing to point out that Pytorch neural network modules are callable Python objects. It means we can call the simplenet as it was a function. What makes this possible is that PyTorch module classes implement a special Python function called __call__(). which will be invoked anytime the object instance is called. After the object instance is called, the __call__() method is invoked under the hood, and the __call__() in turn invokes the forward() method. Instead of calling the forward() method directly, we call the object instance. This applies to all PyTorch neural network modules, namely, networks and layers. . In order to access the model parameters, we can use parameters() or named_parameters() method. Next we will refactor optimization algorithm using nn.Module.parameters and nn.Module.zero_grad method. In addition, we also bring training loop inside a function for easier reuse. . lr = 0.01 epochs = 5 batch_size = 64 nr_iters = len(x_train) // bs for epoch in range(epochs): for i in range((nr_iters + 1)): # step 1. get batch of training set start_index = i * batch_size end_index = start_index + batch_size xs = x_train[start_index:end_index] ys = y_train[start_index:end_index] # step 2. pass batch to network preds = simplenet(xs) # step 3. calculate the loss loss = loss_function(preds, ys) # step 4. calculate the gradient of the loss w.r.t the network&#39;s parameters loss.backward() with torch.no_grad(): # step 5. update the weights using SGD algorithm # old code # weights -= lr * weights.grad # bias -= lr * bias.grad # weights.grad.zero_() # bias.grad.zero_() # new code for p in simplenet.parameters(): p -= lr * p.grad simplenet.zero_grad() # step 6. calculate mean of valid loss after each epoch to see the improvement batch_size_valid = batch_size * 2 nr_iters_valid = len(x_valid) // batch_size_valid total_loss = 0 total_acc = 0 with torch.no_grad(): for i in range((nr_iters_valid + 1)): start_index = i * batch_size_valid end_index = start_index + batch_size_valid xs = x_valid[start_index:end_index] ys = y_valid[start_index:end_index] preds = simplenet(xs) loss = loss_function(preds, ys) total_loss += loss.item() * xs.shape[0] acc = accuracy(preds, ys) total_acc += acc.item() * xs.shape[0] print(f&quot;epoch {epoch}, valid_loss {total_loss / len(x_valid)}, accuracy {total_acc / len(x_valid)}&quot;) . epoch 0, valid_loss 0.5341379848957062, accuracy 0.8131 epoch 1, valid_loss 0.49990872111320495, accuracy 0.8275 epoch 2, valid_loss 0.48446780610084533, accuracy 0.8317 epoch 3, valid_loss 0.47528300595283507, accuracy 0.8343 epoch 4, valid_loss 0.4690569020748138, accuracy 0.8355 . Ok so we have finished refactor stage 1. Let&#39;s move to refactor stage 2. . Refactor stage 2 . Refactor model with nn.Linear class. | Refactor data setup with torch.utils.data.TensorDataset, torch.utils.data.DataLoader. | Refactor optimization algorithm with torch.optim module. | . Pytorch nn.Linear class does all the things that we have done for linear layer, including intialize learnable parameters and define linear operation. . Note: We used the abbreviation fc below because linear layers are also called fully connected layers or dense layer. So linear = dense = fully connected. . class SimpleNet(nn.Module): def __init__(self): super().__init__() self.fc = nn.Linear(784, 10) def forward(self, x): return self.fc(x) simplenet = SimpleNet() simplenet . SimpleNet( (fc): Linear(in_features=784, out_features=10, bias=True) ) . . Note: we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases. . Next is Dataset and DataLoader. . torch.utils.data.Dataset is an abstract class for representing a dataset. An abstract class is a Python class that has methods we must implement, in our case are __getitem__ and __len__. In order to create a custom dataset, we need to subclass the Dataset class and override __len__, that provides the size of the dataset, and __getitem__, supporting integer indexing in range from 0 to len(self) exclusive. Upon doing this, our new subclass can then be passed to the a PyTorch DataLoader object. . PyTorch’s TensorDataset is a Dataset wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. This will make it easier to access both the independent and dependent variables in the same line as we train. . Note: we can douple batch size of valid set because we do not need to calculate gradient for it and it can handle larger batch size comparing to training set. torch.utils.data.DataLoader is responsible for managing batches. It makes life easier to iterate over batches. We can create a DataLoader from any Dataset. . from torch.utils.data import TensorDataset from torch.utils.data import DataLoader batch_size = 64 train_ds = TensorDataset(x_train, y_train) valid_ds = TensorDataset(x_valid, y_valid) train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True) valid_dl = DataLoader(valid_ds, batch_size=batch_size * 2, shuffle=False) . The code in training loop is now changed from . for i in range((nr_iters + 1)): # step 1. get batch of training set start_index = i * batch_size end_index = start_index + batch_size xs = x_train[start_index:end_index] ys = y_train[start_index:end_index] # step 2. pass batch to network preds = simplenet(xs) . to . for xs,ys in train_dl: preds = simplenet(xs) . And torch.optim package. This module provides various optimization algorithms. Its API provides step and zero_grad method for weight updating and zero out gradient which will help us refactor our code further. . from torch import optim lr = 0.01 epochs = 5 simplenet = SimpleNet() opt = optim.SGD(simplenet.parameters(), lr=lr) for epoch in range(epochs): simplenet.train() for xs,ys in train_dl: preds = simplenet(xs) loss = loss_function(preds, ys) loss.backward() opt.step() opt.zero_grad() simplenet.eval() with torch.no_grad(): total_loss = sum(loss_function(simplenet(xs),ys)*len(xs) for xs, ys in valid_dl) total_acc = sum(accuracy(simplenet(xs),ys)*len(xs) for xs, ys in valid_dl) print(f&quot;epoch {epoch}, valid_loss {total_loss / len(x_valid)}, accuracy {total_acc / len(x_valid)}&quot;) . epoch 0, valid_loss 0.5226423740386963, accuracy 0.8163999915122986 epoch 1, valid_loss 0.49401089549064636, accuracy 0.82669997215271 epoch 2, valid_loss 0.48364755511283875, accuracy 0.8306000232696533 epoch 3, valid_loss 0.4771580994129181, accuracy 0.8323000073432922 epoch 4, valid_loss 0.4616956412792206, accuracy 0.8371000289916992 . Done! We have finished the whole processes. Thanks to Pytorch built-in modules, our training loop is now dramatically smaller and easier to understand. The refactor stage 3 does not introduce any new Pytorch modules. It is only an bonus step which help the code a bit cleaner and less code. . Refactor stage 3 . get_data returns dataloaders for the training and validation sets. . def get_data(train_ds, valid_ds, bs): return ( DataLoader(train_ds, batch_size=bs, shuffle=True), DataLoader(valid_ds, batch_size=bs, shuffle=False) ) . get_model returns instance of our model and the optimizer. . def get_model(model, lr): m = model() opt = optim.SGD(m.parameters(), lr=lr) return m, opt . Since we go through a similar process twice of calculating the loss for both the training set and the validation set, let’s make that into its own function, loss_batch, which computes the loss for one batch. We pass an optimizer in for the training set, and use it to perform backprop. For the validation set, we don’t pass an optimizer, so the method doesn’t perform backprop. . def loss_batch(model, loss_func, xs, ys, opt=None, metric=None): loss = loss_func(model(xs), ys) if opt is not None: loss.backward() opt.step() opt.zero_grad() if metric is not None: acc = metric(model(xs), ys) return loss.item(), acc.item(), len(xs) else: return loss.item(), len(xs) . fit runs the necessary operations to train our model and compute the training and validation losses for each epoch. . import numpy as np def fit(epochs, model, loss_func, metric, opt, train_dl, valid_dl): for epoch in range(epochs): model.train() for xs, ys in train_dl: loss_batch(model, loss_func, xs, ys, opt) model.eval() with torch.no_grad(): losses, accs, nums = zip(*[loss_batch(model, loss_func, xs, ys, metric=metric) for xs,ys in valid_dl]) total_loss = np.sum(np.multiply(losses, nums)) total_acc = np.sum(np.multiply(accs, nums)) print(f&quot;epoch {epoch}, valid_loss {total_loss / np.sum(nums)}, accuracy {total_acc / np.sum(nums)}&quot;) . bs = 64 lr = 0.01 epochs = 5 train_dl, valid_dl = get_data(train_ds, valid_ds, bs) model, opt = get_model(model=SimpleNet, lr=lr) fit(epochs, model, loss_function, accuracy, opt, train_dl, valid_dl) . epoch 0, valid_loss 0.5245783556461334, accuracy 0.8162 epoch 1, valid_loss 0.49596426906585694, accuracy 0.8261 epoch 2, valid_loss 0.4808829068660736, accuracy 0.8334 epoch 3, valid_loss 0.47098530049324033, accuracy 0.8347 epoch 4, valid_loss 0.4786785946369171, accuracy 0.831 . Done! Now we can use refactored functions to train a wide variety of models. In part 3, we will use it to train a CNN (Convolutional Neural Network) . Random topics . Disabling PyTorch Gradient Tracking . Get predictions for the entire training set Note at the top, we have annotated the function using the @torch.no_grad() PyTorch decoration. This is because we want this functions execution to omit gradient tracking. This is because gradient tracking uses memory, and during inference (getting predictions while not training) there is no need to keep track of the computational graph. The decoration is one way of locally turning off the gradient tracking feature while executing specific functions. We specifically need the gradient calculation feature anytime we are going to calculate gradients using the backward() function. Otherwise, it is a good idea to turn it off because having it off will reduce memory consumption for computations, e.g. when we are using networks for predicting (inference). . As another example, we can use Python&#39;s with context manger keyword to specify that a specify block of code should exclude gradient computations. Both of these options are valid . @torch.no_grad() def get_all_preds(model, loader): all_preds = torch.tensor([]) for batch in loader: imgs, lbs = batch preds = model(imgs) all_preds = torch.cat((all_preds, preds), dim=0) return all_preds # Locally Disabling PyTorch Gradient Tracking with torch.no_grad(): prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000) train_preds = get_all_preds(model, prediction_loader) . For more information, please check here . log-sum-exp trick . https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/ https://www.tensorflow.org/api_docs/python/tf/nn/log_softmax https://stackoverflow.com/questions/44081007/logsoftmax-stability . come back later! . Reference . Some good sources: . pytorch zero to all | deeplizard | effective pytorch | what is torch.nn really? | recommend walk with pytorch | official tutorial | DL(with Pytorch) | Pytorch project template | nlp turorial with pytorch | UDACITY course | awesome pytorch list | deep learning with pytorch | others: https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc | Grokking Algorithms: An illustrated guide for programmers and other curious people 1st Edition | . | .",
            "url": "https://phucnsp.github.io/blog/self-taught/tutorial/2020/03/22/self-taught-pytorch-part2-nn-from-scratch.html",
            "relUrl": "/self-taught/tutorial/2020/03/22/self-taught-pytorch-part2-nn-from-scratch.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Pytorch part 1 - tensor and Pytorch tensor",
            "content": "Section 1: Introducing Pytorch, CUDA and GPU . PyTorch is a deep learning framework and a scientific computing package. The scientific computing aspect of PyTorch is primarily a result PyTorch’s tensor library and associated tensor operations. That means you can take advantage of Pytorch for many computing tasks, thanks to its supporting tensor operation, without touching deep learning modules. . PyTorch tensors and their associated operations are very similar to numpy n-dimensional arrays. . Important: A tensor is actually an n-dimensional array. Pytorch build its library around Object Oriented Programming(OOP) concept. With object oriented programming, we orient our program design and structure around objects (take a look at Random topics for more information). The tensor in Pytorch is presented by the object torch.Tensor which is created from numpy ndarray objects. Two objects share memory. This makes the transition between PyTorch and NumPy very cheap from a performance perspective. . With PyTorch tensors, GPU support is built-in. It’s very easy with PyTorch to move tensors to and from a GPU if we have one installed on our system. Tensors are super important for deep learning and neural networks because they are the data structure that we ultimately use for building and training our neural networks. . Talking a bit about history. The initial release of PyTorch was in October of 2016, and before PyTorch was created, there was and still is, another framework called Torch which is also a machine learning framework but is based on the Lua programming language. The connection between PyTorch and this Lua version, called Torch, exists because many of the developers who maintain the Lua version are the individuals who created PyTorch. And they have been working for Facebook since then till now. . Note: Facebook Created PyTorch These are the primary PyTorch components we’ll be learning about and using as we build neural networks along the way. . Package Description &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; . torch | The top-level PyTorch package and tensor library. | . torch.nn | A subpackage that contains modules and extensible classes for building neural networks. | . torch.autograd | A subpackage that supports all the differentiable Tensor operations in PyTorch. | . torch.nn.functional | A functional interface that contains operations used for building neural net like loss, activation, layer operations... | . torch.optim | A subpackage that contains standard optimization operations like SGD and Adam. | . torch.utils | A subpackage that contains utility classes like data sets and data loaders that make data preprocessing easier. | . torchvision | A package that provides access to popular datasets, models, and image transformations for computer vision. | . Why use PyTorch for deep learning? . PyTorch’s design is modern, Pythonic. When we build neural networks with PyTorch, we are super close to programming neural networks from scratch. When we write PyTorch code, we are just writing and extending standard Python classes, and when we debug PyTorch code, we are using the standard Python debugger. It’s written mostly in Python, and only drops into C++ and CUDA code for operations that are performance bottlenecks. | It is a thin framework, which makes it more likely that PyTorch will be capable of adapting to the rapidly evolving deep learning environment as things change quickly over time. | Stays out of the way and this makes it so that we can focus on neural networks and less on the actual framework. | . Why PyTorch is great for deep learning research . The reason for this research suitability is that Pytorch use dynamic computational graph, in contrast with tensorfow which uses static computational graph, in order to calculate derivatives. . Computational graphs are used to graph the function operations that occur on tensors inside neural networks. These graphs are then used to compute the derivatives needed to optimize the neural network. Dynamic computational graph means that the graph is generated on the fly as the operations are created. Static graphs that are fully determined before the actual operations occur. . It just so happens that many of the cutting edge research topics in deep learning are requiring or benefiting greatly from dynamic graphs. . Installing PyTorch . The recommended best option is to use the Anaconda Python package manager. With Anaconda, it&#39;s easy to get and manage Python, Jupyter Notebook, and other commonly used packages for scientific computing and data science, like PyTorch! . Let’s go over the steps: . Download and install Anaconda (choose the latest Python version). | Go to PyTorch&#39;s site and find the get started locally section. | Specify the appropriate configuration options for your particular environment. | Run the presented command in the terminal to install PyTorch | . For the example: conda install pytorch torchvision cudatoolkit=10.0 -c pytorch . Notice that we are installing both PyTorch and torchvision. Also, there is no need to install CUDA separately. The needed CUDA software comes installed with PyTorch if a CUDA version is selected in step (3). All we need to do is select a version of CUDA if we have a supported Nvidia GPU on our system. . !conda list torch . # packages in environment at /Users/phucnsp/anaconda3/envs/fastai2: # # Name Version Build Channel pytorch 1.4.0 py3.7_0 pytorch torchsummary 1.5.1 pypi_0 pypi torchvision 0.5.0 py37_cpu pytorch . import torch torch.__version__ # to verify pytorch version . &#39;1.4.0&#39; . torch.cuda.is_available() # to verify our GPU capabilities . False . Why deep learning uses GPUs . To understand CUDA, we need to have a working knowledge of graphics processing units (GPUs). A GPU is a processor that is good at handling specialized computations. This is in contrast to a central processing unit (CPU), which is a processor that is good at handling general computations. CPUs are the processors that power most of the typical computations on our electronic devices. . A GPU can be much faster at computing than a CPU. However, this is not always the case. The speed of a GPU relative to a CPU depends on the type of computation being performed. The type of computation most suitable for a GPU is a computation that can be done in parallel. . Parallel computing is a type of computation where by a particular computation is broken into independent smaller computations that can be carried out simultaneously. The resulting computations are then recombined, or synchronized, to form the result of the original larger computation. The number of tasks that a larger task can be broken into depends on the number of cores contained on a particular piece of hardware. Cores are the units that actually do the computation within a given processor, and CPUs typically have four, eight, or sixteen cores while GPUs have potentially thousands. . So why deep learning uses them - Neural networks are embarrassingly parallel. Tasks that embarrassingly parallel are ones where it’s easy to see that the set of smaller tasks are independent with respect to each other. Many of the computations that we do with neural networks can be easily broken into smaller computations in such a way that the set of smaller computations do not depend on one another. One such example is a convolution. . GPU, CUDA and Nvidia . Nvidia is a technology company that designs GPUs, and they have created CUDA as a software platform that pairs with their GPU hardware making it easier for developers to build software that accelerates computations using the parallel processing power of Nvidia GPUs. . GPU computing In the beginning, the main tasks that were accelerated using GPUs were computer graphics. That&#39;s why we have the name graphics processing unit. But in recent years, many more varieties parallel tasks have emerged. One such task as we have seen is deep learning. Deep learning along with many other scientific computing tasks that use parallel programming techniques are leading to a new type of programming model called [GPGPU or general purpose GPU computing](https://arxiv.org/abs/1408.6923). GPGPU computing, more commonly just called GPU computing, is becoming more common to perform a wide variety of tasks on a GPU. . GPU computing stack GPU as the hardware on the bottom, CUDA as the software architecture on top of the GPU, and finally libraries like cuDNN on top of CUDA. Sitting on top of CUDA and cuDNN is PyTorch, which is the framework were we’ll be working that ultimately supports applications on top. Developers use CUDA by downloading the CUDA toolkit which comes with specialized libraries like cuDNN - the CUDA Deep Neural Network library. With PyTorch, CUDA comes baked in from the start. There are no additional downloads required. All we need is to have a supported Nvidia GPU, and we can leverage CUDA using PyTorch. We don’t need to know how to use the CUDA API directly. . After all, PyTorch is written in all of these: Python, C++, CUDA . . Suppose we have the following code: . t = torch.tensor([1,2,3]) . The tensor object created in this way is on the CPU by default. As a result, any operations that we do using this tensor object will be carried out on the CPU. Now, to move the tensor onto the GPU, we just write: . t = t.cuda() . This ability makes PyTorch very flexible because computations can be selectively carried out either on the CPU or on the GPU. . . Note: GPU Can Be Slower Than CPU. The answer is that a GPU is only faster for particular (specialized) tasks. For example, moving data from the CPU to the GPU is costly, so in this case, the overall performance might be slower if the computation task is a simple one. Moving relatively small computational tasks to the GPU won’t speed us up very much and may indeed slow us down. Remember, the GPU works well for tasks that can be broken into many smaller tasks, and if a compute task is already small, we won’t have much to gain by moving the task to the GPU. For this reason, it’s often acceptable to simply use a CPU when just starting out, and as we tackle larger more complicated problems, begin using the GPU more heavily. . Section 2: Introducing Tensors . Tensors - Data Structures of Deep Learning . A tensor is the primary data structure used by neural networks. The inputs, outputs, and transformations within neural networks are all represented using tensors, and as a result, neural network programming utilizes tensors heavily. . The below concepts, that we met in math or computer science, are all refered to tensor in deep learning. . indexes required math computer science . 0 | scalar | number | . 1 | vector | array | . 2 | matrix | 2d-array | . The relationship within each of these pairs, for example vector and array, is that both elements require the same number of indexes to refer to a specific element within the data structure. . # an array or a vector requires 1 index to access its element a = [1,2,3,4] a[3] . 4 . # an matrix or 2d-array requires 2 index to access its element a = [ [1, 2, 3, 4], [5, 6, 7, 8] ] a[0][2] . 3 . When more than two indexes are required to access a specific element, we stop giving specific names to the structures, and we begin using more general language. . In mathematics, we stop using words like scalar, vector, and matrix, and we start using the word tensor or nd-tensor. The n tells us the number of indexes required to access a specific element within the structure. | In computer science, we stop using words like, number, array, 2d-array, and start using the word multidimensional array or nd-array. The n tells us the number of indexes required to access a specific element within the structure. | . The reason we say a tensor is a generalization form is because we use the word tensor for all values of n like so: . A scalar is a 0 dimensional tensor | A vector is a 1 dimensional tensor | A matrix is a 2 dimensional tensor | A nd-array is an n dimensional tensor | . . Note: Tensors and nd-arrays are the same thing! . Fundamental tensor attributes for deep learning - Rank, Axes, and Shape. . These concepts build on one another starting with rank, then axes, and building up to shape. . The rank of a tensor refers to the number of dimensions present within the tensor. It also tells us how many indexes are needed to refer to a specific element within the tensor. A rank-2 tensor means all of the following: a matrix, a 2d-array, a 2d-tensor. . An axis of a tensor is a specific dimension of a tensor. If we say that a tensor is a rank 2 tensor, we mean that the tensor has 2 dimensions, or equivalently, the tensor has two axes. . Let&#39;s get an example how to access elements of an axis. . dd = [ [1,2,3], [4,5,6], [7,8,9] ] . Each element along the first axis, is an array: . dd[0], dd[1], dd[2] . ([1, 2, 3], [4, 5, 6], [7, 8, 9]) . Each element along the second axis, is a number: . dd[0][0], dd[1][0], dd[2][0] . (1, 4, 7) . . Note: with tensors, the elements of the last axis are always numbers. Every other axis will contain n-dimensional arrays. . The shape of a tensor gives us the length of each axis of the tensor. . Note: The shape of a tensor is important because it encodes all of the relevant information about axes, rank, and therefore indexes. Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called reshaping. . t = torch.tensor([ [1,2,3], [5,6,7] ], dtype=torch.float) t.shape . torch.Size([2, 3]) . . Note: size and shape of a tensor are the same thing. . Section 3: Pytorch Tensors . PyTorch tensors are the data structures we&#39;ll be using when programming neural networks in PyTorch. . The tensor in Pytorch is presented by the object torch.Tensor which is created from numpy ndarray objects. Two objects share memory. This makes the transition between PyTorch and NumPy very cheap from a performance perspective. . When programming neural networks, data preprocessing is often one of the first steps in the overall process, and one goal of data preprocessing is to transform the raw input data into tensor form. . torch.Tensor class and its attributes . PyTorch tensors are instances of the torch.Tensor Python class. First, let’s look at a few torch.Tensor&#39;s tensor attributes. . tensor.dtype: | tensor.device | tensor.layout | . t = torch.Tensor() . The dtype specifies the type of the data that is contained within the tensor. . t.dtype . torch.float32 . Table below shows all the tensor types that Pytorch supports. . Note: - Each type has a CPU and GPU version - Tensors contain uniform (of the same type) numerical data . - Tensor operations between tensors must happen between tensors with the same type of data. . . The device specifies the device (CPU or GPU) where the tensor&#39;s data is allocated. This determines where tensor computations for the given tensor will be performed. . t.device . device(type=&#39;cpu&#39;) . PyTorch supports the use of multiple devices, and they are specified using an index like so: . device = torch.device(&#39;cuda:0&#39;) device . device(type=&#39;cuda&#39;, index=0) . If we have a device like above, we can create a tensor on the device by passing the device to the tensor’s constructor. . Note: tensor operations between tensors must happen between tensors that exists on the same device. . import torch t = torch.tensor([1,2,3], dtype=torch.int, device=device) . The layout specifies how the tensor is stored in memory. . t.layout . torch.strided . To learn more about stride check here. . Create a new tensor using data . These are the primary ways of creating tensor objects (instances of the torch.Tensor class), with data (array-like) in PyTorch: . torch.Tensor(data) is the constructor of the torch.Tensor class | torch.tensor(data): is the factory function that constructs torch.Tensor objects. | torch.as_tensor(data) | torch.from_numpy(data) | Let’s look at each of these. They all accept some form of data and give us an instance of the torch.Tensor class. Sometimes when there are multiple ways to achieve the same result, things can get confusing, so let’s break this down. . import numpy as np data = np.array([1,2,3]) o1 = torch.Tensor(data) o2 = torch.tensor(data) o3 = torch.as_tensor(data) o4 = torch.from_numpy(data) print(o1) print(o2) print(o3) print(o4) . tensor([1., 2., 3.]) tensor([1, 2, 3]) tensor([1, 2, 3]) tensor([1, 2, 3]) . print(torch.get_default_dtype()) o1.dtype == torch.get_default_dtype() . torch.float32 . True . The table below compare 4 options and propose which one to use: . method which one to use dtype data in memory . torch.Tensor(data) | | infer from torch default dtype, unable to pass a dtype to the constructor. | copy | . torch.tensor(data) | best option to go, better doc and more config options than torch.Tensor | inferred from data or be explicitly set. | copy | . torch.as_tensor(data) | to-go when we want to tune for performance, better than torch.from_numpy because it accepts a wide variety of array-like objects including other Pytorch tensor. | inferred from data or be explicitly set. | share | . torch.from_nummpy(data) | only accepts numpy.ndarray | inferred from data or be explicitly set. | share | . Data memory is shared means that the actual data in memory exists in a single place. As a result, any changes that occur in the underlying data will be reflected in both objects, the torch.Tensor and the numpy.ndarray. Sharing data is more efficient and uses less memory than copying data because the data is not written to two locations in memory. However, there are something to keep in mind about memory sharing: . Since numpy.ndarray objects are allocated on the CPU, the as_tensor() function must copy the data from the CPU to the GPU when a GPU is being used. | The memory sharing of as_tensor() doesn’t work with built-in Python data structures like list. | The as_tensor() call requires developer knowledge of the sharing feature. This is necessary so we don’t inadvertently make an unwanted change in the underlying data without realizing the change impacts multiple objects. | The as_tensor() performance improvement will be greater if there are a lot of back and forth operations between numpy.ndarray objects and tensor objects. However, if there is just a single load operation, there shouldn’t be much impact from a performance perspective. | . Tips, In order to convert multiple arrays to tensor we can use map . import numpy as np import torch a = np.array([1,2,3]) b = np.array([3,4,5]) c = np.array([1]) a, b, c = map(torch.tensor, (a, b, c)) a, b, c . (tensor([1, 2, 3]), tensor([3, 4, 5]), tensor([1])) . Create a new tensor without data . Here are some other creation options that are available. . # create identity matrix torch.eye(2) . tensor([[1., 0.], [0., 1.]]) . # create a tensor of zeros with the shape of specified shape argument torch.zeros([2,2]) . tensor([[0., 0.], [0., 0.]]) . # create a tensor of ones with the shape of specified shape argument torch.ones([2,2]) . tensor([[1., 1.], [1., 1.]]) . # Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1) # The shape of the tensor is defined by the variable argument size. torch.rand([2,2]) . tensor([[0.3088, 0.4226], [0.8102, 0.9129]]) . # Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 # (also called the standard normal distribution). torch.randn(2, 3) . tensor([[-1.1778, -1.0388, -0.1459], [-0.1746, 0.5764, -1.1491]]) . This is a small subset of the available creation functions that don’t require data. Check with the PyTorch documentation for the full list. . Section 4: Pytorch Tensor Operations . We have the following high-level categories of tensor operations: . Reshaping operations: gave us the ability to position our elements along particular axes. | Element-wise operations: allow us to perform operations on elements between two tensors. | Reduction operations: allow us to perform operations on elements within a single tensor. | Access operations allow us to access to each numerical elements within a single tensor. | . There are a lot of individual operations out there, so much so that it can sometimes be intimidating when you&#39;re just beginning, but grouping similar operations into categories based on their likeness can help make learning about tensor operations more manageable. . Before going to each category, firstly we will take a look on the concept of broadcasting . Broadcasting Tensors . To understand this concept, let&#39;s take a look at an example. Suppose we have the following tensors. . t1 = torch.tensor([ [1,1], [1,1] ], dtype=torch.float32) t2 = torch.tensor([2,4], dtype=torch.float32) t1.shape, t2.shape . (torch.Size([2, 2]), torch.Size([2])) . What will be the result of this element-wise addition operation, t1 + t2 ? . t1 + t2 . tensor([[3., 5.], [3., 5.]]) . Even though these two tenors have differing shapes, the element-wise operation is possible, and broadcasting is what makes the operation possible. The lower rank tensor t2 will be transformed via broadcasting to match the shape of the higher rank tensor t1, and the element-wise operation will be performed as usual. . we can check the broadcast transformation using the broadcast_to() numpy function. . import numpy as np np.broadcast_to(t2.numpy(), t1.shape) . array([[2., 4.], [2., 4.]], dtype=float32) . t1 + t2 . tensor([[3., 5.], [3., 5.]]) . After broadcasting, the addition operation between these two tensors is a regular element-wise operation between tensors of the same shape. . Tensor reshape operation . Reshaping operations are perhaps the most important type of tensor operations because the shape of a tensor gives us something concrete we can use to shape an intuition for our tensors. . Note: Reshaping changes the shape but not the underlying data elements. . Using the reshape() function, we can specify the row x column shape that we are seeking. Notice that the product of the shape&#39;s components has to be equal to the number of elements in the original tensor. . Pytorch has another function called view() that does the same thing as reshape function. . import torch t = torch.tensor([ [1,1,1,1], [2,2,2,2], [3,3,3,3] ], dtype=torch.float32) . t.reshape([3,4]) . tensor([[1., 1., 1., 1.], [2., 2., 2., 2.], [3., 3., 3., 3.]]) . One common reshape type operation is squeeze, unsqueeze. Those operations change the shape of our tensors is by squeezing and unsqueezing them. . Squeezing a tensor removes the dimensions or axes that have a length of one. | Unsqueezing a tensor adds a dimension with a length of one. | . t.reshape([1,12]).shape . torch.Size([1, 12]) . t.reshape([1,12]).squeeze().shape . torch.Size([12]) . t.reshape([1,12]).unsqueeze(dim=0).shape . torch.Size([1, 1, 12]) . Another common reshape type operation is flattening. A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flatted out before the fully connected layer will accept the input. A flatten operation on a tensor reshapes the tensor to have a shape that is equal to the number of elements contained in the tensor. This is the same thing as a 1d-array of elements. . These are some ways to flatten a tensor. . t.reshape(1,-1)[0] . tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]) . t.reshape(-1) . tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]) . t.view(t.numel()) . tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]) . t.flatten() . tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]) . In addition, it is possible to flatten only specific parts of a tensor. . t = torch.tensor([[ [1,1,1,1], [2,2,2,2], [3,3,3,3] ]], dtype=torch.float32) t.shape . torch.Size([1, 3, 4]) . t.flatten(start_dim=1).shape . torch.Size([1, 12]) . Take a deeper look inside the flatten operation, it is actually a composition of reshape and squeeze operation. . Note: flatten operation = reshape operation + squeeze operation . def flatten_ex(t): t = t.reshape(1, -1) t = t.squeeze() return t flatten_ex(t) . tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]) . Tensor element-wise operation . An element-wise operation is an operation between two tensors that operates on corresponding elements within the respective tensors. Two tensors must have the same shape in order to perform element-wise operations on them. . Some common element-wise operations . t1 = torch.tensor([[1,2], [3,4]], dtype=torch.float32) t2 = torch.tensor([[9,8], [7,6]], dtype=torch.float32) . t1 + t2 # equivalent with t1.add(t2) . tensor([[10., 10.], [10., 10.]]) . t1 + 2 # equivalent with t1.add(2) . tensor([[3., 4.], [5., 6.]]) . t1 - 2 # equivalent with t1.sub(2) . tensor([[-1., 0.], [ 1., 2.]]) . t1 * 2 # equivalent with t1.mul(2) . tensor([[2., 4.], [6., 8.]]) . t1 / 2 # equivalent with t1.div(2) . tensor([[0.5000, 1.0000], [1.5000, 2.0000]]) . Comparison Operation is element-wise type operation . t.eq(0) . tensor([[ True, False, True], [False, True, False], [ True, False, True]]) . t.gt(0) . tensor([[False, True, False], [ True, False, True], [False, True, False]]) . t.lt(0) . tensor([[False, False, False], [False, False, False], [False, False, False]]) . With element-wise operations that are functions, it’s fine to assume that the function is applied to each element of the tensor. . t.abs() . tensor([[0., 1., 0.], [2., 0., 2.], [0., 3., 0.]]) . t.sqrt() . tensor([[0.0000, 1.0000, 0.0000], [1.4142, 0.0000, 1.4142], [0.0000, 1.7321, 0.0000]]) . t.neg() . tensor([[-0., -1., -0.], [-2., -0., -2.], [-0., -3., -0.]]) . t.neg().abs() . tensor([[0., 1., 0.], [2., 0., 2.], [0., 3., 0.]]) . . Note: Element-wise = Component-wise = Point-wise . Tensor reduction operations . A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor. Tensors give us the ability to manage our data. The tensor can be reduced to a single scalar value or reduced along an axis. . Let’s look at common tensor reduction operations: . import torch t = torch.tensor([ [0,1,0], [2,0,2], [0,3,0] ], dtype=torch.float32) . Reducing to a tensor with a single element . t.sum(), t.prod(), t.mean(), t.std() . (tensor(8.), tensor(0.), tensor(0.8889), tensor(1.1667)) . Reducing tensors By Axes . t.sum(dim=0) . tensor([2., 4., 2.]) . Argmax tensor reduction operation is very common in neural network. This operation returns the index location of the maximum value inside a tensor. In practice, we often use the argmax() function on a network’s output prediction tensor, to determine which category has the highest prediction value. . t.max(dim=1) . torch.return_types.max( values=tensor([1., 2., 3.]), indices=tensor([1, 2, 1])) . t.argmax(dim=1) . tensor([1, 2, 1]) . Tensor access operation . This operation provides the ability to access data within the tensor. Common tensor access operations are item(), tolist(), numpy() . t.mean().item() . 0.8888888955116272 . t.mean(dim=0).tolist() . [0.6666666865348816, 1.3333333730697632, 0.6666666865348816] . t.mean(dim=0).numpy() . array([0.6666667, 1.3333334, 0.6666667], dtype=float32) . Random topics . Object Oriented Programming and Why Pytorch select it. . When we’re writing programs or building software, there are two key components, code and data. With object oriented programming, we orient our program design and structure around objects. Objects are defined in code using classes. A class defines the object&#39;s specification or spec, which specifies what data and code each object of the class should have. When we create an object of a class, we call the object an instance of the class, and all instances of a given class have two core components: . Methods(code) | Attributes(data) | . In a given program, many objects, a.k.a instances of a given class have the same available attributes and the same available methods. The difference between objects of the same class is the values contained within the object for each attribute. Each object has its own attribute values. These values determine the internal state of the object. The code and data of each object is said to be encapsulated within the object. . Let’s build a simple class to demonstrate how classes encapsulate data and code: . class Sample: #class declaration def __init__(self, name): #class constructor (code) self.name = name #attribute (data) def set_name(self, name): #method declaration (code) self.name = name #method implementation (code) . Let&#39;s switch gears now and look at how object oriented programming fits in with PyTorch. . The primary component we&#39;ll need to build a neural network is a layer, and so, as we might expect, PyTorch&#39;s neural network library contains classes that aid us in constructing layers. As we know, deep neural networks are built using multiple layers. This is what makes the network deep. Each layer in a neural network has two primary components: . A transformation (code) | A collection of weights (data) | . Like many things in life, this fact makes layers great candidates to be represented as objects using Object Oriented Programming - OOP. . References . Some good sources: . pytorch zero to all | deeplizard | effective pytorch | what is torch.nn really? | recommend walk with pytorch | official tutorial | DL(with Pytorch) | Pytorch project template | nlp turorial with pytorch | UDACITY course | awesome pytorch list | deep learning with pytorch | others: https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc | Grokking Algorithms: An illustrated guide for programmers and other curious people 1st Edition | . | .",
            "url": "https://phucnsp.github.io/blog/self-taught/2020/03/22/self-taught-pytorch-part1-tensor.html",
            "relUrl": "/self-taught/2020/03/22/self-taught-pytorch-part1-tensor.html",
            "date": " • Mar 22, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Tutorial jupyter notebook and  Fastpages",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](data/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ . Useful links for jupyter notebook . http://blog.juliusschulz.de/blog/ultimate-ipython-notebook#document-metadata | .",
            "url": "https://phucnsp.github.io/blog/tutorial/2020/02/20/tutorial-notebook-fastpage.html",
            "relUrl": "/tutorial/2020/02/20/tutorial-notebook-fastpage.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Tutorial markdown and Fastpages",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://phucnsp.github.io/blog/tutorial/2020/01/14/tutorial-markdown-and-fastpage.html",
            "relUrl": "/tutorial/2020/01/14/tutorial-markdown-and-fastpage.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Singapore, quan sát và suy ngẫm",
            "content": "Đợt tháng mười vừa rồi tôi có dịp đi dạo quanh anh hàng xóm Singapore. Cũng lâu rồi tôi mới có dịp đi ra khỏi Vietnam, tôi thích cái cảm giác được bay, được transit ở một sân bay nào đó, ngồi ngắm những người xa lạ rảo bước qua lại, cô đơn nhưng thú vị. Singapore nổi tiếng bởi sự hiện đại, bởi những tòa nhà chọc trời, bởi Universal Studio…nhưng đối với tôi thì những cái đó chả có ý nghĩa gì cả. Khi bạn đã từng ngồi cafe ở đại lộ danh vọng Hollywood, đã đi loanh quanh ở quảng trường thời đại NewYork, đã xem biểu tình của người dân Syria trước nhà trắng Mỹ thì liệu những tòa nhà chọc trời ở Singapore có gì để hấp dẫn. Tôi đi Singapore chơi đơn giản vì tôi muốn dẫn vợ đi nước ngoài cho biết và cũng vì tôi cần đi du lịch sau một khoảng thời gian rất rất lâu cắm đầu vô công việc. . Thật may thay, Singapore khác những nơi tôi từng qua và còn làm tôi mất mấy tiếng để ngồi viết cái note này. . Chiều hôm qua tôi trở về Tân Sân Nhất sau hơn 1 tiếng 30 phút bay, đơn giản là cảm giác xót xa. Lạ nhỉ, lần trước bay ở Berlin về chả thấy gì, lần này tự nhiên lại xót xa. Tại người ta ở cái tầm cao hơn mình quá, tôi khen Singapore mà tôi xót cho Saigon, tôi nhìn anh xe ôm, nhìn những chị tay xách nách mang, nhìn cái cách người dân quê tôi băng qua đường chễnh chệ bất chấp đèn tín hiệu, tiếng la hét chèo kéo khách, tiếng bóp còi inh ỏi, xa xa là anh công an ngồi lướt điện thoại ở một xó cạnh lối ra sân bay, tất cả đều làm tôi xót xa. Hôm thứ năm trước khi bay đi chơi còn thấy bình thường mà nhỉ! . Changi đón chào vợ chồng tôi bằng cái thác nước trong nhà cao nhất thế giới, đó là cái woww đầu tiên khi tàu điện kết nối giữa những terminal đưa chúng tôi đi qua cái thác nước nhân tạo này, ngay bên trong sân bay. Chúng tôi về đến hostel trời cũng đã tối, hơn 8pm thì phải, nên cả 2 đều đi ngủ sớm để hôm sau có sức mà đi chơi. . . Ba ngày vi vu ở thành phố này là ba ngày làm tôi suy ngẫm rất nhiều, cái giả định về một thành phố hiện đại với những tòa nhà chọc trời chán phèo dần dần mất đi khi tôi lần lượt đi qua những tụ điểm nổi tiếng ở đây, chứng kiến cái cách họ nâng tầm công nghệ lên thành nghê thuật đã làm tôi thay đổi góc nhìn của mình. Xây cái nhà cao chót vót lên thì ở thành phố lớn nào cũng có nhưng để khoa học và nghệ thuật thật sự gặp nhau thì không phải nơi nào cũng làm được và cũng không phải người dân ở đâu cũng cảm thụ được. Sáng sớm hôm thứ bảy, trong lúc dạo bộ và ngồi nghỉ ngơi ở trạm xe bus gần khu Chinatown, tôi đọc được một bài chia sẻ của anh country manager bên Knorex, từng học tập và làm việc ở Singapore sau đó về quản lý branch ở Ho Chi Minh. Góc nhìn của anh ấy trong công việc thật ra không mới đối với tôi nhưng đơn giản là nó được đọc đúng lúc, đúng thời điểm. Đó là góc nhìn về việc học suốt đời, học ở bất kì hoàn cảnh nào mà không phải chờ có thầy dạy mới học được, về growth mindset, về thái độ làm việc cho đi trước để nhận lại sau, đặc biệt là về sự toàn cầu hóa nhân lực của các công ty, tập đoàn, đồng nghiệp của bạn ngày nay không còn là những anh em bạn dì nữa mà có thể đến từ bất kì đâu trên thế giới này, Ấn độ, Brazile, Chile, Sillicon Valley, etc. Bài chia sẻ ấy như là chất xúc tác cho chuyến đi của tôi, tôi bắt đầu để ý hơn tới con người nơi đây, từng chi tiết nhỏ trên đường, cách họ đào đường lên và lấp lại cẩn thận mà không để nhấp nhô, cách họ giữ gìn sạch sẽ không chỉ phía trước mà cả phía sau của nhà hàng cho đến cách họ quy hoạch bố trí nhà cửa, bố trí mảng xanh khắp nơi… tôi đã cảm thấy thật khó khăn trong việc tìm ra chỗ để bĩu môi chê cười. Mọi thứ được hoàn thiện một cách tuyệt vời ở tầm quốc gia, Singapore có lẽ là ví dụ điển hình nhất cho khái niệm quản lý “tự do trong khuôn khổ”, người dân có không gian để thể hiện cái riêng nhưng phải trong khuôn khổ nhất định để giữ gìn cái chung. . Trưa thứ bảy vợ chồng tôi đi dạo bộ ở khu Marina Bay thì tình cờ được dự giờ một buổi tập hát của mấy em tiểu học. Tụi nhỏ biểu diễn về nhạc kịch hay gì đó đại loại thế, nghe khá xa xỉ đối với đại đa số dân Việt Nam mình. Thật buồn cười khi tôi khen con nít ở Singapore nói tiếng anh hay quá, hát tự tin quá. Nhưng thật sự chúng đã được thừa hưởng di sản phi vật thể quá lớn từ ông Lý, tiếng anh vs tiếng trung, để giờ đây có thể tiếp cận dễ dàng hơn với tinh hoa thế giới, lại thêm cái kiểu giáo dục khai sáng, tự do thể hiện cái tôi cá nhân thế này nữa thì hỡi ơi, mấy đứa cháu ở quê đang đung đưa võng nghe thần tượng Hàn Quốc hát cả ngày hay đang tụ tập quán trà sữa để chém gió, thì rồi cơ may nào để chúng cạnh tranh trong cái thế giới toàn cầu hóa đây. Có thể nhiều người sẽ nghĩ làm gì tới nỗi, vẫn có rất nhiều nhân tài người Việt học trường làng những vẫn nổi danh thế giới đó thôi, nhưng khi đánh giá cái tầm quốc gia thì người ta không nói câu chuyện của một vài người xuất chúng, người ta nói đến sức mạnh của passport index (sức mạnh tấm hộ chiếu Việt Nam hình như đứng gần áp chót bảng xếp hạng), người ta đánh giá thành tích toàn đoàn, có đấy những ngôi sao vàng lẻ loi đoạt huy chương tầm thế giới nhưng có mấy bài báo đăng thành tích của toàn đoàn không! Và theo trải nghiệm cá nhân của tôi, bây giờ khi bạn hỏi một người nước ngoài biết gì về Việt Nam thì câu trả lời khó mà ngoài chiến tranh và gia công quần áo. . À nói một tí về khái niệm “Du lịch, quan sát và suy ngẫm” mà tôi đã từng đọc đâu đó, nó mang tới cho bạn một góc nhìn “thấm” hơn về nơi mình đã đi qua. Du lịch không dừng lại ở những tấm hình selffie khoe trên facebook mà còn cả ở cảm thụ cá nhân. Nhưng cũng có lẽ sự cảm thụ này không dễ mà có được, sau một thời gian dài liên tục học tập, đọc sách, tích lũy kiến thức thì mới may ra chấp chớm cảm nhận được điều này. Đó là lý do vì sao mà chúng ta rất hay thấy du khách nước ngoài tới Việt Nam du lịch thường tới bảo tàng, thường mua sách về Việt Nam để đọc, đơn giản vì họ đang cảm thụ một cách sâu hơn văn hóa, con người, đất nước chúng ta, chứ không chỉ dừng lại bề nổi ở những bức ảnh. Ngày nay tất nhiên không cần phải tới bảo tàng thì chúng ta mới biết về một nơi nào đó, mọi thứ có thể được đọc dễ dàng qua internet nhưng thật sự cảm giác của việc đi du lịch, cảm nhận bằng chính tất cả giác quan để rồi có những suy ngẫm sâu sắc hơn về sự vật sự việc xung quanh mang lại những lợi ích to lớn. Nó là cơn mưa mùa hè cho những bộ não đã bị lu mờ bởi những thứ lặp đi lặp lại hằng ngày, thậm chí nó có thể thay đổi hoàn toàn góc nhìn của mình từ trước tới nay về một việc nào đó. . Quay trở lại câu chuyện xót xa Singapore, có lẽ một dịp nào đó tôi sẽ trở lại đất nước này để xem cảm giác còn như xưa không. Như một lời nhắn nhủ cho bản thân, toàn cầu hóa là có thật và ở ngay đít rồi. Quên đi những xích mích, những câu chuyện vặt ở xung quanh, trong lúc mình ngồi nhiều chuyện thì thằng khác ở đâu đó vẫn đang miệt mài tu luyện. Kỷ luật hơn, học tập ở mọi nơi, học ở bất kì ai, đừng chờ có thầy dạy thì mới học được, đừng sợ học nhiều quá sẽ làm não mình bớt thông minh. Học kiến thức để tăng trí thông mình IQ, học văn hóa nghệ thuật, học cách ửng xử để tăng trí thông minh EQ. Làm việc với một thái độ sẵn sàng hy sinh, thân tôi đây nè, vứt việc cho tôi đi. Dù biết rằng một cánh én nhỏ khó làm nên mùa xuân, tôi có trở thành công dân toàn cầu cũng chưa chắc Việt Nam sẽ tốt hơn nhưng ít nhất tôi không muốn là người góp phần cho đất nước mà tôi yêu quý trở nên tệ hơn! . Tối qua về đến nhà con bạn người Malay đang sống ở Singapore, thấy hình đăng facebook, mới nhắn rủ cafe, cũng hơi tiếc không gặp được. Nó khoe mới có thằng bồ người Ấn, tụi nó vừa đi đám cưới bạn ở đảo Galapagos bên Ecuador về và rủ năm sau ra Hà Nội xem Vietnam F1 Race. . Chắc năm sau ra Hà Nội, quan sát và ngẫm tiếp nhỉ… .",
            "url": "https://phucnsp.github.io/blog/travel/2019/10/07/Singpore-trip-and-remaining.html",
            "relUrl": "/travel/2019/10/07/Singpore-trip-and-remaining.html",
            "date": " • Oct 7, 2019"
        }
        
    
  
    
        ,"post14": {
            "title": "Data Science workflow recommendation",
            "content": "Repository of this workflow is stored here . Production data science template . The template of this repository follows production-data-science workflow, which focuses on productionizing data scientist’s work, make the analysis or research to be reusable, applicable to production. The workflow is separated into 2 phases: . exploration phase is where data scientist explores the project, mainly work with jupyter notebook. All the work in this phase will be stored in exploration folder. | production phase is where data scientists’ works are refactored into packages so it can be reuse, imported. All the work in this phase will be stored in your_package folder. | . How to setup a new repository - for maintainer . git clone https://gitlab.com/Phuc_Su/production_data_science_template.git git clone &lt;your_project_repository&gt; cd &lt;your_project_name&gt; git checkout -b product-initial-setup # open Finder, copy all content of production_data_science_template into your project repository, except .git and .idea folder conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; pip install git-lfs # in case you want to add some large file extension other than .jpg, .pdf, .csv, .xlsx git lfs track &lt;add large file path&gt; # rename &lt;your package&gt; folder and modify setup.py, most importance is require_packages. See example below # write something about your project in README.md pip install -e . pip freeze | grep -v &lt;package_name&gt; &gt; requirements.txt git add . git commit -m &quot;First commit&quot; git push -u origin HEAD . Example of setup.py . setup( name=&#39;your_project&#39;, version=&#39;v0.1&#39;, description=&#39;&#39;, long_description=readme(), classifiers=[ &#39;Programming Language :: Python :: 3&#39;, ], url=&#39;https://github.com/phucnsp/production_data_science_template&#39;, author=&#39;Phuc_Su&#39;, author_email=&#39;&#39;, license=&#39;&#39;, packages=[&#39;your_package&#39;], install_requires=[ &#39;pypandoc&gt;=1.4&#39;, &#39;watermark&gt;=1.5.0&#39;, &#39;pandas&gt;=0.20.3&#39;, &#39;scikit-learn&gt;=0.19.0&#39;, &#39;scipy&gt;=0.19.1&#39;, &#39;matplotlib&gt;=2.1.0&#39;, &#39;pytest&gt;=3.2.3&#39;, &#39;pytest-runner&gt;=2.12.1&#39;, &#39;click&gt;=6.7&#39; ], setup_requires=[&#39;pytest-runner&#39;], tests_require=[&#39;pytest&#39;], ) . and you are ready~! 🎉 . Note: if you want to setup notification on slack for merge request from gitlab, reference here . How to contribute - for developers . Setup first time . bash conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; git clone &lt;repository url&gt; cd to/the/project/directory pip install -r requirements.txt pip install -e . . For a private repository accessible only through an SSH authentication, substitute https://github.com/ with git@github.com:. . Returning to work . Some rules: 1 branch/1 exploration/1 folder | branch-name convention: explore-* for exploration, refactor-* for refactor | . | . git checkout master git pull --all # if you continue to work on old branch git checkout &lt;branch&gt; # if you want to start a new exploration git checkout -b &lt;new_branch&gt; # if your branch is far behind master and you want to merge git merge master ##################### Start working ##################### git add &lt;path_to_work_files/folder&gt; git commit -m &quot;some message&quot; git push -u origin HEAD . Notes . requirements.txt helps to setup your virtual environment, to make sure all contributors working on the same environments. So whenever you have a new libraries need to install, after installing you need to add it into requirements.txt by pip freeze | grep -v &lt;package_name&gt; &gt; requirements.txt | setup.py allows you to create packages that you can redistribute. This script is meant to install your package on the end user’s system, not to prepare the development environment. packages - in-house development packages. | install_requires - packages that our development packages dependence on. | py_modules=[&#39;new_module&#39;] - in-house development modules need to install (placed in root directory) | . | pip install -e . - to install packages/modules from setup.py, in the editable mode. | If you want to add large file into working repository: pip install git-lfs git lfs install # Tell LFS to track files with given path git lfs track &quot;path_to_large_file&quot; # Tell LFS to track files with format &quot;*.jpg&quot; git lfs track &quot;*.jpg&quot; # Tell LFS to track content of the whole directory git lfs track &quot;data/*&quot; . | . How to use the package - for users . Install the library . conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; pip install -e &#39;git+https://github.com/phucnsp/production_data_science_template.git&#39; . For a private repository accessible only through an SSH authentication, substitute git+https://github.com with git+ssh://git@github.com. Note that -e argument above to make the installation editable. . Leisure read . Production Data Science tutorial | Writing a setup script | Minimum structure | gitlab slack notification service | git strategy | .",
            "url": "https://phucnsp.github.io/blog/work/2019/05/10/data-science-template.html",
            "relUrl": "/work/2019/05/10/data-science-template.html",
            "date": " • May 10, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "Predict house price in America",
            "content": "Introduction . import pandas as pd pd.options.display.max_columns = 999 import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import KFold from sklearn.metrics import mean_squared_error from sklearn import linear_model from sklearn.model_selection import KFold . df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) . def transform_features(df): return df def select_features(df): return df[[&quot;Gr Liv Area&quot;, &quot;SalePrice&quot;]] def train_and_test(df): train = df[:1460] test = df[1460:] ## You can use `pd.DataFrame.select_dtypes()` to specify column types ## and return only those columns as a data frame. numeric_train = train.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) numeric_test = test.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) ## You can use `pd.Series.drop()` to drop a value. features = numeric_train.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df) rmse . 57088.251612639091 . Feature Engineering . Handle missing values: All columns: Drop any with 5% or more missing values for now. Text columns: Drop any with 1 or more missing values for now. Numerical columns: For columns with missing values, fill in with the most common value in that column . 1: All columns: Drop any with 5% or more missing values for now. . ## Series object: column name -&gt; number of missing values num_missing = df.isnull().sum() . # Filter Series to columns containing &gt;5% missing values drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() # Drop those columns from the data frame. Note the use of the .index accessor df = df.drop(drop_missing_cols.index, axis=1) . ## Series object: column name -&gt; number of missing values text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) ## Filter Series to columns containing *any* missing values drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) . ## Compute column-wise missing value counts num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() fixable_numeric_cols . BsmtFin SF 1 1 BsmtFin SF 2 1 Bsmt Unf SF 1 Total Bsmt SF 1 Garage Cars 1 Garage Area 1 Bsmt Full Bath 2 Bsmt Half Bath 2 Mas Vnr Area 23 dtype: int64 . ## Compute the most common value for each column in `fixable_nmeric_missing_cols`. replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] replacement_values_dict . {&#39;Bsmt Full Bath&#39;: 0.0, &#39;Bsmt Half Bath&#39;: 0.0, &#39;Bsmt Unf SF&#39;: 0.0, &#39;BsmtFin SF 1&#39;: 0.0, &#39;BsmtFin SF 2&#39;: 0.0, &#39;Garage Area&#39;: 0.0, &#39;Garage Cars&#39;: 2.0, &#39;Mas Vnr Area&#39;: 0.0, &#39;Total Bsmt SF&#39;: 0.0} . ## Use `pd.DataFrame.fillna()` to replace missing values. df = df.fillna(replacement_values_dict) . ## Verify that every column has 0 missing values df.isnull().sum().value_counts() . 0 64 dtype: int64 . years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_sold[years_sold &lt; 0] . 2180 -1 dtype: int64 . years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] years_since_remod[years_since_remod &lt; 0] . 1702 -1 2180 -2 2181 -1 dtype: int64 . ## Create new columns df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod ## Drop rows with negative values for both of these new features df = df.drop([1702, 2180, 2181], axis=0) ## No longer need original year columns df = df.drop([&quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis = 1) . Drop columns that: a. that aren&#39;t useful for ML b. leak data about the final sale . ## Drop columns that aren&#39;t useful for ML df = df.drop([&quot;PID&quot;, &quot;Order&quot;], axis=1) ## Drop columns that leak info about the final sale df = df.drop([&quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Yr Sold&quot;], axis=1) . Let&#39;s update transform_features() . def transform_features(df): num_missing = df.isnull().sum() drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() df = df.drop(drop_missing_cols.index, axis=1) text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] df = df.fillna(replacement_values_dict) years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod df = df.drop([1702, 2180, 2181], axis=0) df = df.drop([&quot;PID&quot;, &quot;Order&quot;, &quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis=1) return df def select_features(df): return df[[&quot;Gr Liv Area&quot;, &quot;SalePrice&quot;]] def train_and_test(df): train = df[:1460] test = df[1460:] ## You can use `pd.DataFrame.select_dtypes()` to specify column types ## and return only those columns as a data frame. numeric_train = train.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) numeric_test = test.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) ## You can use `pd.Series.drop()` to drop a value. features = numeric_train.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df) rmse . 55275.367312413066 . Feature Selection . numerical_df = transform_df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]) numerical_df . MS SubClass Lot Area Overall Qual Overall Cond Mas Vnr Area BsmtFin SF 1 BsmtFin SF 2 Bsmt Unf SF Total Bsmt SF 1st Flr SF 2nd Flr SF Low Qual Fin SF Gr Liv Area Bsmt Full Bath Bsmt Half Bath Full Bath Half Bath Bedroom AbvGr Kitchen AbvGr TotRms AbvGrd Fireplaces Garage Cars Garage Area Wood Deck SF Open Porch SF Enclosed Porch 3Ssn Porch Screen Porch Pool Area Misc Val Yr Sold SalePrice Years Before Sale Years Since Remod . 0 20 | 31770 | 6 | 5 | 112.0 | 639.0 | 0.0 | 441.0 | 1080.0 | 1656 | 0 | 0 | 1656 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 7 | 2 | 2.0 | 528.0 | 210 | 62 | 0 | 0 | 0 | 0 | 0 | 2010 | 215000 | 50 | 50 | . 1 20 | 11622 | 5 | 6 | 0.0 | 468.0 | 144.0 | 270.0 | 882.0 | 896 | 0 | 0 | 896 | 0.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 0 | 1.0 | 730.0 | 140 | 0 | 0 | 0 | 120 | 0 | 0 | 2010 | 105000 | 49 | 49 | . 2 20 | 14267 | 6 | 6 | 108.0 | 923.0 | 0.0 | 406.0 | 1329.0 | 1329 | 0 | 0 | 1329 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 312.0 | 393 | 36 | 0 | 0 | 0 | 0 | 12500 | 2010 | 172000 | 52 | 52 | . 3 20 | 11160 | 7 | 5 | 0.0 | 1065.0 | 0.0 | 1045.0 | 2110.0 | 2110 | 0 | 0 | 2110 | 1.0 | 0.0 | 2 | 1 | 3 | 1 | 8 | 2 | 2.0 | 522.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 244000 | 42 | 42 | . 4 60 | 13830 | 5 | 5 | 0.0 | 791.0 | 0.0 | 137.0 | 928.0 | 928 | 701 | 0 | 1629 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 6 | 1 | 2.0 | 482.0 | 212 | 34 | 0 | 0 | 0 | 0 | 0 | 2010 | 189900 | 13 | 12 | . 5 60 | 9978 | 6 | 6 | 20.0 | 602.0 | 0.0 | 324.0 | 926.0 | 926 | 678 | 0 | 1604 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 470.0 | 360 | 36 | 0 | 0 | 0 | 0 | 0 | 2010 | 195500 | 12 | 12 | . 6 120 | 4920 | 8 | 5 | 0.0 | 616.0 | 0.0 | 722.0 | 1338.0 | 1338 | 0 | 0 | 1338 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 6 | 0 | 2.0 | 582.0 | 0 | 0 | 170 | 0 | 0 | 0 | 0 | 2010 | 213500 | 9 | 9 | . 7 120 | 5005 | 8 | 5 | 0.0 | 263.0 | 0.0 | 1017.0 | 1280.0 | 1280 | 0 | 0 | 1280 | 0.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 506.0 | 0 | 82 | 0 | 0 | 144 | 0 | 0 | 2010 | 191500 | 18 | 18 | . 8 120 | 5389 | 8 | 5 | 0.0 | 1180.0 | 0.0 | 415.0 | 1595.0 | 1616 | 0 | 0 | 1616 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 1 | 2.0 | 608.0 | 237 | 152 | 0 | 0 | 0 | 0 | 0 | 2010 | 236500 | 15 | 14 | . 9 60 | 7500 | 7 | 5 | 0.0 | 0.0 | 0.0 | 994.0 | 994.0 | 1028 | 776 | 0 | 1804 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 442.0 | 140 | 60 | 0 | 0 | 0 | 0 | 0 | 2010 | 189000 | 11 | 11 | . 10 60 | 10000 | 6 | 5 | 0.0 | 0.0 | 0.0 | 763.0 | 763.0 | 763 | 892 | 0 | 1655 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 440.0 | 157 | 84 | 0 | 0 | 0 | 0 | 0 | 2010 | 175900 | 17 | 16 | . 11 20 | 7980 | 6 | 7 | 0.0 | 935.0 | 0.0 | 233.0 | 1168.0 | 1187 | 0 | 0 | 1187 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 6 | 0 | 2.0 | 420.0 | 483 | 21 | 0 | 0 | 0 | 0 | 500 | 2010 | 185000 | 18 | 3 | . 12 60 | 8402 | 6 | 5 | 0.0 | 0.0 | 0.0 | 789.0 | 789.0 | 789 | 676 | 0 | 1465 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 393.0 | 0 | 75 | 0 | 0 | 0 | 0 | 0 | 2010 | 180400 | 12 | 12 | . 13 20 | 10176 | 7 | 5 | 0.0 | 637.0 | 0.0 | 663.0 | 1300.0 | 1341 | 0 | 0 | 1341 | 1.0 | 0.0 | 1 | 1 | 2 | 1 | 5 | 1 | 2.0 | 506.0 | 192 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 171500 | 20 | 20 | . 14 120 | 6820 | 8 | 5 | 0.0 | 368.0 | 1120.0 | 0.0 | 1488.0 | 1502 | 0 | 0 | 1502 | 1.0 | 0.0 | 1 | 1 | 1 | 1 | 4 | 0 | 2.0 | 528.0 | 0 | 54 | 0 | 0 | 140 | 0 | 0 | 2010 | 212000 | 25 | 25 | . 15 60 | 53504 | 8 | 5 | 603.0 | 1416.0 | 0.0 | 234.0 | 1650.0 | 1690 | 1589 | 0 | 3279 | 1.0 | 0.0 | 3 | 1 | 4 | 1 | 12 | 1 | 3.0 | 841.0 | 503 | 36 | 0 | 0 | 210 | 0 | 0 | 2010 | 538000 | 7 | 7 | . 16 50 | 12134 | 8 | 7 | 0.0 | 427.0 | 0.0 | 132.0 | 559.0 | 1080 | 672 | 0 | 1752 | 0.0 | 0.0 | 2 | 0 | 4 | 1 | 8 | 0 | 2.0 | 492.0 | 325 | 12 | 0 | 0 | 0 | 0 | 0 | 2010 | 164000 | 22 | 5 | . 17 20 | 11394 | 9 | 2 | 350.0 | 1445.0 | 0.0 | 411.0 | 1856.0 | 1856 | 0 | 0 | 1856 | 1.0 | 0.0 | 1 | 1 | 1 | 1 | 8 | 1 | 3.0 | 834.0 | 113 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 394432 | 0 | 0 | . 18 20 | 19138 | 4 | 5 | 0.0 | 120.0 | 0.0 | 744.0 | 864.0 | 864 | 0 | 0 | 864 | 0.0 | 0.0 | 1 | 0 | 2 | 1 | 4 | 0 | 2.0 | 400.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 141000 | 59 | 59 | . 19 20 | 13175 | 6 | 6 | 119.0 | 790.0 | 163.0 | 589.0 | 1542.0 | 2073 | 0 | 0 | 2073 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 7 | 2 | 2.0 | 500.0 | 349 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 210000 | 32 | 22 | . 20 20 | 11751 | 6 | 6 | 480.0 | 705.0 | 0.0 | 1139.0 | 1844.0 | 1844 | 0 | 0 | 1844 | 0.0 | 0.0 | 2 | 0 | 3 | 1 | 7 | 1 | 2.0 | 546.0 | 0 | 122 | 0 | 0 | 0 | 0 | 0 | 2010 | 190000 | 33 | 33 | . 21 85 | 10625 | 7 | 6 | 81.0 | 885.0 | 168.0 | 0.0 | 1053.0 | 1173 | 0 | 0 | 1173 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 6 | 2 | 2.0 | 528.0 | 0 | 120 | 0 | 0 | 0 | 0 | 0 | 2010 | 170000 | 36 | 36 | . 22 60 | 7500 | 7 | 5 | 0.0 | 533.0 | 0.0 | 281.0 | 814.0 | 814 | 860 | 0 | 1674 | 1.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 0 | 2.0 | 663.0 | 0 | 96 | 0 | 0 | 0 | 0 | 0 | 2010 | 216000 | 10 | 10 | . 23 20 | 11241 | 6 | 7 | 180.0 | 578.0 | 0.0 | 426.0 | 1004.0 | 1004 | 0 | 0 | 1004 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 1 | 2.0 | 480.0 | 0 | 0 | 0 | 0 | 0 | 0 | 700 | 2010 | 149000 | 40 | 40 | . 24 20 | 12537 | 5 | 6 | 0.0 | 734.0 | 0.0 | 344.0 | 1078.0 | 1078 | 0 | 0 | 1078 | 1.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 1 | 2.0 | 500.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 149900 | 39 | 2 | . 25 20 | 8450 | 5 | 6 | 0.0 | 775.0 | 0.0 | 281.0 | 1056.0 | 1056 | 0 | 0 | 1056 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 1 | 1.0 | 304.0 | 0 | 85 | 184 | 0 | 0 | 0 | 0 | 2010 | 142000 | 42 | 42 | . 26 20 | 8400 | 4 | 5 | 0.0 | 804.0 | 78.0 | 0.0 | 882.0 | 882 | 0 | 0 | 882 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 4 | 0 | 2.0 | 525.0 | 240 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 126000 | 40 | 40 | . 27 20 | 10500 | 4 | 5 | 0.0 | 432.0 | 0.0 | 432.0 | 864.0 | 864 | 0 | 0 | 864 | 0.0 | 0.0 | 1 | 0 | 3 | 1 | 5 | 1 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 115000 | 39 | 39 | . 28 120 | 5858 | 7 | 5 | 0.0 | 1051.0 | 0.0 | 354.0 | 1405.0 | 1337 | 0 | 0 | 1337 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 1 | 2.0 | 511.0 | 203 | 68 | 0 | 0 | 0 | 0 | 0 | 2010 | 184000 | 11 | 11 | . 29 160 | 1680 | 6 | 5 | 504.0 | 156.0 | 0.0 | 327.0 | 483.0 | 483 | 504 | 0 | 987 | 0.0 | 0.0 | 1 | 1 | 2 | 1 | 5 | 0 | 1.0 | 264.0 | 275 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 96000 | 39 | 39 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2900 20 | 13618 | 8 | 5 | 198.0 | 1350.0 | 0.0 | 378.0 | 1728.0 | 1960 | 0 | 0 | 1960 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 8 | 2 | 3.0 | 714.0 | 172 | 38 | 0 | 0 | 0 | 0 | 0 | 2006 | 320000 | 1 | 0 | . 2901 20 | 11443 | 8 | 5 | 208.0 | 1460.0 | 0.0 | 408.0 | 1868.0 | 2028 | 0 | 0 | 2028 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 7 | 2 | 3.0 | 880.0 | 326 | 66 | 0 | 0 | 0 | 0 | 0 | 2006 | 369900 | 1 | 0 | . 2902 20 | 11577 | 9 | 5 | 382.0 | 1455.0 | 0.0 | 383.0 | 1838.0 | 1838 | 0 | 0 | 1838 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 9 | 1 | 3.0 | 682.0 | 161 | 225 | 0 | 0 | 0 | 0 | 0 | 2006 | 359900 | 1 | 0 | . 2903 20 | 31250 | 1 | 3 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1600 | 0 | 0 | 1600 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 270.0 | 0 | 0 | 135 | 0 | 0 | 0 | 0 | 2006 | 81500 | 55 | 55 | . 2904 90 | 7020 | 7 | 5 | 200.0 | 1243.0 | 0.0 | 45.0 | 1288.0 | 1368 | 0 | 0 | 1368 | 2.0 | 0.0 | 2 | 0 | 2 | 2 | 8 | 0 | 4.0 | 784.0 | 0 | 48 | 0 | 0 | 0 | 0 | 0 | 2006 | 215000 | 9 | 9 | . 2905 120 | 4500 | 6 | 5 | 116.0 | 897.0 | 0.0 | 319.0 | 1216.0 | 1216 | 0 | 0 | 1216 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 402.0 | 0 | 125 | 0 | 0 | 0 | 0 | 0 | 2006 | 164000 | 8 | 8 | . 2906 120 | 4500 | 6 | 5 | 443.0 | 1201.0 | 0.0 | 36.0 | 1237.0 | 1337 | 0 | 0 | 1337 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 405.0 | 0 | 199 | 0 | 0 | 0 | 0 | 0 | 2006 | 153500 | 8 | 8 | . 2907 20 | 17217 | 5 | 5 | 0.0 | 0.0 | 0.0 | 1140.0 | 1140.0 | 1140 | 0 | 0 | 1140 | 0.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 0 | 0.0 | 0.0 | 36 | 56 | 0 | 0 | 0 | 0 | 0 | 2006 | 84500 | 0 | 0 | . 2908 160 | 2665 | 5 | 6 | 0.0 | 0.0 | 0.0 | 264.0 | 264.0 | 616 | 688 | 0 | 1304 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 1 | 1.0 | 336.0 | 141 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 104500 | 29 | 29 | . 2909 160 | 2665 | 5 | 6 | 0.0 | 548.0 | 173.0 | 36.0 | 757.0 | 925 | 550 | 0 | 1475 | 0.0 | 0.0 | 2 | 0 | 4 | 1 | 6 | 1 | 1.0 | 336.0 | 104 | 26 | 0 | 0 | 0 | 0 | 0 | 2006 | 127000 | 29 | 29 | . 2910 160 | 3964 | 6 | 4 | 0.0 | 837.0 | 0.0 | 105.0 | 942.0 | 1291 | 1230 | 0 | 2521 | 1.0 | 0.0 | 2 | 1 | 5 | 1 | 10 | 1 | 2.0 | 576.0 | 728 | 20 | 0 | 0 | 0 | 0 | 0 | 2006 | 151400 | 33 | 33 | . 2911 20 | 10172 | 5 | 7 | 0.0 | 441.0 | 0.0 | 423.0 | 864.0 | 874 | 0 | 0 | 874 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 5 | 0 | 1.0 | 288.0 | 0 | 120 | 0 | 0 | 0 | 0 | 0 | 2006 | 126500 | 38 | 3 | . 2912 90 | 11836 | 5 | 5 | 0.0 | 149.0 | 0.0 | 1503.0 | 1652.0 | 1652 | 0 | 0 | 1652 | 0.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 3.0 | 928.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 146500 | 36 | 36 | . 2913 180 | 1470 | 4 | 6 | 0.0 | 522.0 | 0.0 | 108.0 | 630.0 | 630 | 0 | 0 | 630 | 1.0 | 0.0 | 1 | 0 | 1 | 1 | 3 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 73000 | 36 | 36 | . 2914 160 | 1484 | 4 | 4 | 0.0 | 252.0 | 0.0 | 294.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 1.0 | 253.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 79400 | 34 | 34 | . 2915 20 | 13384 | 5 | 5 | 194.0 | 119.0 | 344.0 | 641.0 | 1104.0 | 1360 | 0 | 0 | 1360 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 8 | 1 | 1.0 | 336.0 | 160 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 140000 | 37 | 27 | . 2916 180 | 1533 | 5 | 7 | 0.0 | 553.0 | 0.0 | 77.0 | 630.0 | 630 | 0 | 0 | 630 | 1.0 | 0.0 | 1 | 0 | 1 | 1 | 3 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 92000 | 36 | 36 | . 2917 160 | 1533 | 4 | 5 | 0.0 | 408.0 | 0.0 | 138.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 1.0 | 286.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 87550 | 36 | 36 | . 2918 160 | 1526 | 4 | 5 | 0.0 | 0.0 | 0.0 | 546.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 0.0 | 0.0 | 0 | 34 | 0 | 0 | 0 | 0 | 0 | 2006 | 79500 | 36 | 36 | . 2919 160 | 1936 | 4 | 7 | 0.0 | 0.0 | 0.0 | 546.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 90500 | 36 | 36 | . 2920 160 | 1894 | 4 | 5 | 0.0 | 252.0 | 0.0 | 294.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 286.0 | 0 | 24 | 0 | 0 | 0 | 0 | 0 | 2006 | 71000 | 36 | 36 | . 2921 90 | 12640 | 6 | 5 | 0.0 | 936.0 | 396.0 | 396.0 | 1728.0 | 1728 | 0 | 0 | 1728 | 0.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 2.0 | 574.0 | 40 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 150900 | 30 | 30 | . 2922 90 | 9297 | 5 | 5 | 0.0 | 1606.0 | 0.0 | 122.0 | 1728.0 | 1728 | 0 | 0 | 1728 | 2.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 2.0 | 560.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 188000 | 30 | 30 | . 2923 20 | 17400 | 5 | 5 | 0.0 | 936.0 | 0.0 | 190.0 | 1126.0 | 1126 | 0 | 0 | 1126 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 5 | 1 | 2.0 | 484.0 | 295 | 41 | 0 | 0 | 0 | 0 | 0 | 2006 | 160000 | 29 | 29 | . 2924 20 | 20000 | 5 | 7 | 0.0 | 1224.0 | 0.0 | 0.0 | 1224.0 | 1224 | 0 | 0 | 1224 | 1.0 | 0.0 | 1 | 0 | 4 | 1 | 7 | 1 | 2.0 | 576.0 | 474 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 131000 | 46 | 10 | . 2925 80 | 7937 | 6 | 6 | 0.0 | 819.0 | 0.0 | 184.0 | 1003.0 | 1003 | 0 | 0 | 1003 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 0 | 2.0 | 588.0 | 120 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 142500 | 22 | 22 | . 2926 20 | 8885 | 5 | 5 | 0.0 | 301.0 | 324.0 | 239.0 | 864.0 | 902 | 0 | 0 | 902 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 0 | 2.0 | 484.0 | 164 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 131000 | 23 | 23 | . 2927 85 | 10441 | 5 | 5 | 0.0 | 337.0 | 0.0 | 575.0 | 912.0 | 970 | 0 | 0 | 970 | 0.0 | 1.0 | 1 | 0 | 3 | 1 | 6 | 0 | 0.0 | 0.0 | 80 | 32 | 0 | 0 | 0 | 0 | 700 | 2006 | 132000 | 14 | 14 | . 2928 20 | 10010 | 5 | 5 | 0.0 | 1071.0 | 123.0 | 195.0 | 1389.0 | 1389 | 0 | 0 | 1389 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 6 | 1 | 2.0 | 418.0 | 240 | 38 | 0 | 0 | 0 | 0 | 0 | 2006 | 170000 | 32 | 31 | . 2929 60 | 9627 | 7 | 5 | 94.0 | 758.0 | 0.0 | 238.0 | 996.0 | 996 | 1004 | 0 | 2000 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 9 | 1 | 3.0 | 650.0 | 190 | 48 | 0 | 0 | 0 | 0 | 0 | 2006 | 188000 | 13 | 12 | . 2927 rows × 34 columns . abs_corr_coeffs = numerical_df.corr()[&#39;SalePrice&#39;].abs().sort_values() abs_corr_coeffs . BsmtFin SF 2 0.006127 Misc Val 0.019273 Yr Sold 0.030358 3Ssn Porch 0.032268 Bsmt Half Bath 0.035875 Low Qual Fin SF 0.037629 Pool Area 0.068438 MS SubClass 0.085128 Overall Cond 0.101540 Screen Porch 0.112280 Kitchen AbvGr 0.119760 Enclosed Porch 0.128685 Bedroom AbvGr 0.143916 Bsmt Unf SF 0.182751 Lot Area 0.267520 2nd Flr SF 0.269601 Bsmt Full Bath 0.276258 Half Bath 0.284871 Open Porch SF 0.316262 Wood Deck SF 0.328183 BsmtFin SF 1 0.439284 Fireplaces 0.474831 TotRms AbvGrd 0.498574 Mas Vnr Area 0.506983 Years Since Remod 0.534985 Full Bath 0.546118 Years Before Sale 0.558979 1st Flr SF 0.635185 Garage Area 0.641425 Total Bsmt SF 0.644012 Garage Cars 0.648361 Gr Liv Area 0.717596 Overall Qual 0.801206 SalePrice 1.000000 Name: SalePrice, dtype: float64 . ## Let&#39;s only keep columns with a correlation coefficient of larger than 0.4 (arbitrary, worth experimenting later!) abs_corr_coeffs[abs_corr_coeffs &gt; 0.4] . BsmtFin SF 1 0.439284 Fireplaces 0.474831 TotRms AbvGrd 0.498574 Mas Vnr Area 0.506983 Years Since Remod 0.534985 Full Bath 0.546118 Years Before Sale 0.558979 1st Flr SF 0.635185 Garage Area 0.641425 Total Bsmt SF 0.644012 Garage Cars 0.648361 Gr Liv Area 0.717596 Overall Qual 0.801206 SalePrice 1.000000 Name: SalePrice, dtype: float64 . ## Drop columns with less than 0.4 correlation with SalePrice transform_df = transform_df.drop(abs_corr_coeffs[abs_corr_coeffs &lt; 0.4].index, axis=1) . Which categorical columns should we keep? . ## Create a list of column names from documentation that are *meant* to be categorical nominal_features = [&quot;PID&quot;, &quot;MS SubClass&quot;, &quot;MS Zoning&quot;, &quot;Street&quot;, &quot;Alley&quot;, &quot;Land Contour&quot;, &quot;Lot Config&quot;, &quot;Neighborhood&quot;, &quot;Condition 1&quot;, &quot;Condition 2&quot;, &quot;Bldg Type&quot;, &quot;House Style&quot;, &quot;Roof Style&quot;, &quot;Roof Matl&quot;, &quot;Exterior 1st&quot;, &quot;Exterior 2nd&quot;, &quot;Mas Vnr Type&quot;, &quot;Foundation&quot;, &quot;Heating&quot;, &quot;Central Air&quot;, &quot;Garage Type&quot;, &quot;Misc Feature&quot;, &quot;Sale Type&quot;, &quot;Sale Condition&quot;] . Which columns are currently numerical but need to be encoded as categorical instead (because the numbers don&#39;t have any semantic meaning)? If a categorical column has hundreds of unique values (or categories), should we keep it? When we dummy code this column, hundreds of columns will need to be added back to the data frame. . ## Which categorical columns have we still carried with us? We&#39;ll test tehse transform_cat_cols = [] for col in nominal_features: if col in transform_df.columns: transform_cat_cols.append(col) ## How many unique values in each categorical column? uniqueness_counts = transform_df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values() ## Aribtrary cutoff of 10 unique values (worth experimenting) drop_nonuniq_cols = uniqueness_counts[uniqueness_counts &gt; 10].index transform_df = transform_df.drop(drop_nonuniq_cols, axis=1) . ## Select just the remaining text columns and convert to categorical text_cols = transform_df.select_dtypes(include=[&#39;object&#39;]) for col in text_cols: transform_df[col] = transform_df[col].astype(&#39;category&#39;) ## Create dummy columns and add back to the dataframe! transform_df = pd.concat([ transform_df, pd.get_dummies(transform_df.select_dtypes(include=[&#39;category&#39;])) ], axis=1) . Update select_features() . def transform_features(df): num_missing = df.isnull().sum() drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() df = df.drop(drop_missing_cols.index, axis=1) text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] df = df.fillna(replacement_values_dict) years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod df = df.drop([1702, 2180, 2181], axis=0) df = df.drop([&quot;PID&quot;, &quot;Order&quot;, &quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis=1) return df def select_features(df, coeff_threshold=0.4, uniq_threshold=10): numerical_df = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]) abs_corr_coeffs = numerical_df.corr()[&#39;SalePrice&#39;].abs().sort_values() df = df.drop(abs_corr_coeffs[abs_corr_coeffs &lt; coeff_threshold].index, axis=1) nominal_features = [&quot;PID&quot;, &quot;MS SubClass&quot;, &quot;MS Zoning&quot;, &quot;Street&quot;, &quot;Alley&quot;, &quot;Land Contour&quot;, &quot;Lot Config&quot;, &quot;Neighborhood&quot;, &quot;Condition 1&quot;, &quot;Condition 2&quot;, &quot;Bldg Type&quot;, &quot;House Style&quot;, &quot;Roof Style&quot;, &quot;Roof Matl&quot;, &quot;Exterior 1st&quot;, &quot;Exterior 2nd&quot;, &quot;Mas Vnr Type&quot;, &quot;Foundation&quot;, &quot;Heating&quot;, &quot;Central Air&quot;, &quot;Garage Type&quot;, &quot;Misc Feature&quot;, &quot;Sale Type&quot;, &quot;Sale Condition&quot;] transform_cat_cols = [] for col in nominal_features: if col in df.columns: transform_cat_cols.append(col) uniqueness_counts = df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values() drop_nonuniq_cols = uniqueness_counts[uniqueness_counts &gt; 10].index df = df.drop(drop_nonuniq_cols, axis=1) text_cols = df.select_dtypes(include=[&#39;object&#39;]) for col in text_cols: df[col] = df[col].astype(&#39;category&#39;) df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=[&#39;category&#39;]))], axis=1) return df def train_and_test(df, k=0): numeric_df = df.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) features = numeric_df.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() if k == 0: train = df[:1460] test = df[1460:] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse if k == 1: # Randomize *all* rows (frac=1) from `df` and return shuffled_df = df.sample(frac=1, ) train = df[:1460] test = df[1460:] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions_one = lr.predict(test[features]) mse_one = mean_squared_error(test[&quot;SalePrice&quot;], predictions_one) rmse_one = np.sqrt(mse_one) lr.fit(test[features], test[&quot;SalePrice&quot;]) predictions_two = lr.predict(train[features]) mse_two = mean_squared_error(train[&quot;SalePrice&quot;], predictions_two) rmse_two = np.sqrt(mse_two) avg_rmse = np.mean([rmse_one, rmse_two]) print(rmse_one) print(rmse_two) return avg_rmse else: kf = KFold(n_splits=k, shuffle=True) rmse_values = [] for train_index, test_index, in kf.split(df): train = df.iloc[train_index] test = df.iloc[test_index] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) rmse_values.append(rmse) print(rmse_values) avg_rmse = np.mean(rmse_values) return avg_rmse df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df, k=4) rmse . [25761.875549560471, 36527.812968130842, 24956.485193881424, 28486.738135675929] . 28933.227961812168 .",
            "url": "https://phucnsp.github.io/blog/self-taught/2017/09/15/predict-house-price.html",
            "relUrl": "/self-taught/2017/09/15/predict-house-price.html",
            "date": " • Sep 15, 2017"
        }
        
    
  
    
        ,"post16": {
            "title": "Kaggle competition - titanic machine learning from disaster",
            "content": "import pandas as pd train = pd.read_csv(&quot;train.csv&quot;) holdout = pd.read_csv(&quot;test.csv&quot;) print(holdout.head()) . PassengerId Pclass Name Sex 0 892 3 Kelly, Mr. James male 1 893 3 Wilkes, Mrs. James (Ellen Needs) female 2 894 2 Myles, Mr. Thomas Francis male 3 895 3 Wirz, Mr. Albert male 4 896 3 Hirvonen, Mrs. Alexander (Helga E Lindqvist) female Age SibSp Parch Ticket Fare Cabin Embarked 0 34.5 0 0 330911 7.8292 NaN Q 1 47.0 1 0 363272 7.0000 NaN S 2 62.0 0 0 240276 9.6875 NaN Q 3 27.0 0 0 315154 8.6625 NaN S 4 22.0 1 1 3101298 12.2875 NaN S . # %load functions.py def process_missing(df): &quot;&quot;&quot;Handle various missing values from the data set Usage holdout = process_missing(holdout) &quot;&quot;&quot; df[&quot;Fare&quot;] = df[&quot;Fare&quot;].fillna(train[&quot;Fare&quot;].mean()) df[&quot;Embarked&quot;] = df[&quot;Embarked&quot;].fillna(&quot;S&quot;) return df def process_age(df): &quot;&quot;&quot;Process the Age column into pre-defined &#39;bins&#39; Usage train = process_age(train) &quot;&quot;&quot; df[&quot;Age&quot;] = df[&quot;Age&quot;].fillna(-0.5) cut_points = [-1,0,5,12,18,35,60,100] label_names = [&quot;Missing&quot;,&quot;Infant&quot;,&quot;Child&quot;,&quot;Teenager&quot;,&quot;Young Adult&quot;,&quot;Adult&quot;,&quot;Senior&quot;] df[&quot;Age_categories&quot;] = pd.cut(df[&quot;Age&quot;],cut_points,labels=label_names) return df def process_fare(df): &quot;&quot;&quot;Process the Fare column into pre-defined &#39;bins&#39; Usage train = process_fare(train) &quot;&quot;&quot; cut_points = [-1,12,50,100,1000] label_names = [&quot;0-12&quot;,&quot;12-50&quot;,&quot;50-100&quot;,&quot;100+&quot;] df[&quot;Fare_categories&quot;] = pd.cut(df[&quot;Fare&quot;],cut_points,labels=label_names) return df def process_cabin(df): &quot;&quot;&quot;Process the Cabin column into pre-defined &#39;bins&#39; Usage train process_cabin(train) &quot;&quot;&quot; df[&quot;Cabin_type&quot;] = df[&quot;Cabin&quot;].str[0] df[&quot;Cabin_type&quot;] = df[&quot;Cabin_type&quot;].fillna(&quot;Unknown&quot;) df = df.drop(&#39;Cabin&#39;,axis=1) return df def process_titles(df): &quot;&quot;&quot;Extract and categorize the title from the name column Usage train = process_titles(train) &quot;&quot;&quot; titles = { &quot;Mr&quot; : &quot;Mr&quot;, &quot;Mme&quot;: &quot;Mrs&quot;, &quot;Ms&quot;: &quot;Mrs&quot;, &quot;Mrs&quot; : &quot;Mrs&quot;, &quot;Master&quot; : &quot;Master&quot;, &quot;Mlle&quot;: &quot;Miss&quot;, &quot;Miss&quot; : &quot;Miss&quot;, &quot;Capt&quot;: &quot;Officer&quot;, &quot;Col&quot;: &quot;Officer&quot;, &quot;Major&quot;: &quot;Officer&quot;, &quot;Dr&quot;: &quot;Officer&quot;, &quot;Rev&quot;: &quot;Officer&quot;, &quot;Jonkheer&quot;: &quot;Royalty&quot;, &quot;Don&quot;: &quot;Royalty&quot;, &quot;Sir&quot; : &quot;Royalty&quot;, &quot;Countess&quot;: &quot;Royalty&quot;, &quot;Dona&quot;: &quot;Royalty&quot;, &quot;Lady&quot; : &quot;Royalty&quot; } extracted_titles = df[&quot;Name&quot;].str.extract(&#39; ([A-Za-z]+) .&#39;,expand=False) df[&quot;Title&quot;] = extracted_titles.map(titles) return df def create_dummies(df,column_name): &quot;&quot;&quot;Create Dummy Columns (One Hot Encoding) from a single Column Usage train = create_dummies(train,&quot;Age&quot;) &quot;&quot;&quot; dummies = pd.get_dummies(df[column_name],prefix=column_name) df = pd.concat([df,dummies],axis=1) return df . #preprocess the data def pre_process(df): df = process_missing(df) df = process_age(df) df = process_fare(df) df = process_titles(df) df = process_cabin(df) for col in [&quot;Age_categories&quot;,&quot;Fare_categories&quot;, &quot;Title&quot;,&quot;Cabin_type&quot;,&quot;Sex&quot;]: df = create_dummies(df,col) return df train = pre_process(train) holdout = pre_process(holdout) . Data exploration . #Inspect data type of column explore_cols = [&quot;SibSp&quot;,&quot;Parch&quot;,&quot;Survived&quot;] explore = train[explore_cols].copy() explore.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 3 columns): SibSp 891 non-null int64 Parch 891 non-null int64 Survived 891 non-null int64 dtypes: int64(3) memory usage: 21.0 KB . # Histogram to view the distribution of 2 columns: SibSp and Parch import matplotlib.pyplot as plt %matplotlib inline explore.drop(&quot;Survived&quot;,axis=1).plot.hist(alpha=0.5,bins=8) plt.xticks(range(11)) plt.show() . explore[&quot;familysize&quot;] = explore[[&quot;SibSp&quot;,&quot;Parch&quot;]].sum(axis=1) explore.drop(&quot;Survived&quot;,axis=1).plot.hist(alpha=0.5,bins=10) plt.xticks(range(11)) plt.show() . # Use pivot tables to look at the survival rate for different values of the columns import numpy as np for col in explore.columns.drop(&quot;Survived&quot;): pivot = explore.pivot_table(index=col,values=&quot;Survived&quot;) pivot.plot.bar(ylim=(0,1),yticks=np.arange(0,1,.1)) plt.axhspan(.3, .6, alpha=0.2, color=&#39;red&#39;) plt.show() . The SibSp column shows the number of siblings and/or spouses each passenger had on board, while the Parch columns shows the number of parents or children each passenger had onboard. Neither column has any missing values. . The distribution of values in both columns is skewed right, with the majority of values being zero. . You can sum these two columns to explore the total number of family members each passenger had onboard. The shape of the distribution of values in this case is similar, however there are less values at zero, and the quantity tapers off less rapidly as the values increase. . Looking at the survival rates of the the combined family members, you can see that few of the over 500 passengers with no family members survived, while greater numbers of passengers with family members survived. . Engineering new features . # Based on the observation about few surviver with no family group, let&#39;s create a binary value column where 1 is with # family and 0 is without family def feature_alone(df): df[&quot;familysize&quot;] = df[[&quot;SibSp&quot;,&quot;Parch&quot;]].sum(axis=1) df[&quot;isalone&quot;] = 0 df.loc[(df[&quot;familysize&quot;] == 0),&quot;isalone&quot;] = 1 df.drop(&quot;familysize&quot;, axis = 1) return df train = feature_alone(train) holdout = feature_alone(holdout) . Feature selection/preparation . # Select the best-performing features from sklearn.ensemble import RandomForestClassifier from sklearn.feature_selection import RFECV def select_features(df): # Remove non-numeric columns, columns that have null values df = df.select_dtypes([np.number]).dropna(axis=1) all_X = df.drop([&quot;Survived&quot;,&quot;PassengerId&quot;],axis=1) all_y = df[&quot;Survived&quot;] clf = RandomForestClassifier(random_state=1) selector = RFECV(clf,cv=10) selector.fit(all_X,all_y) best_columns = list(all_X.columns[selector.support_]) print(&quot;Best Columns n&quot;+&quot;-&quot;*12+&quot; n{}&quot;.format(best_columns)) return best_columns cols = select_features(train) . Best Columns [&#39;Pclass&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Age_categories_Adult&#39;, &#39;Age_categories_Infant&#39;, &#39;Age_categories_Missing&#39;, &#39;Age_categories_Senior&#39;, &#39;Age_categories_Teenager&#39;, &#39;Age_categories_Young Adult&#39;, &#39;Fare_categories_0-12&#39;, &#39;Fare_categories_100+&#39;, &#39;Fare_categories_12-50&#39;, &#39;Fare_categories_50-100&#39;, &#39;Title_Master&#39;, &#39;Title_Miss&#39;, &#39;Title_Mr&#39;, &#39;Title_Mrs&#39;, &#39;Title_Officer&#39;, &#39;Cabin_type_C&#39;, &#39;Cabin_type_D&#39;, &#39;Cabin_type_E&#39;, &#39;Cabin_type_Unknown&#39;, &#39;Sex_female&#39;, &#39;Sex_male&#39;, &#39;familysize&#39;, &#39;isalone&#39;] . Model selection/Tuning . # Write a function to train 3 different models. # Using grid search to train using different combinations of hyperparameters to find best performing models. from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import GridSearchCV def select_model(df,features): all_X = df[features] all_y = df[&quot;Survived&quot;] # List of dictionaries, each containing a model name, # it&#39;s estimator and a dict of hyperparameters models = [ { &quot;name&quot;: &quot;LogisticRegression&quot;, &quot;estimator&quot;: LogisticRegression(), &quot;hyperparameters&quot;: { &quot;solver&quot;: [&quot;newton-cg&quot;, &quot;lbfgs&quot;, &quot;liblinear&quot;] } }, { &quot;name&quot;: &quot;KNeighborsClassifier&quot;, &quot;estimator&quot;: KNeighborsClassifier(), &quot;hyperparameters&quot;: { &quot;n_neighbors&quot;: range(1,20,2), &quot;weights&quot;: [&quot;distance&quot;, &quot;uniform&quot;], &quot;algorithm&quot;: [&quot;ball_tree&quot;, &quot;kd_tree&quot;, &quot;brute&quot;], &quot;p&quot;: [1,2] } }, { &quot;name&quot;: &quot;RandomForestClassifier&quot;, &quot;estimator&quot;: RandomForestClassifier(random_state=1), &quot;hyperparameters&quot;: { &quot;n_estimators&quot;: [4, 6, 9], &quot;criterion&quot;: [&quot;entropy&quot;, &quot;gini&quot;], &quot;max_depth&quot;: [2, 5, 10], &quot;max_features&quot;: [&quot;log2&quot;, &quot;sqrt&quot;], &quot;min_samples_leaf&quot;: [1, 5, 8], &quot;min_samples_split&quot;: [2, 3, 5] } } ] for model in models: print(model[&#39;name&#39;]) print(&#39;-&#39;*len(model[&#39;name&#39;])) grid = GridSearchCV(model[&quot;estimator&quot;], param_grid=model[&quot;hyperparameters&quot;], cv=10) grid.fit(all_X,all_y) model[&quot;best_params&quot;] = grid.best_params_ model[&quot;best_score&quot;] = grid.best_score_ model[&quot;best_model&quot;] = grid.best_estimator_ print(&quot;Best Score: {}&quot;.format(model[&quot;best_score&quot;])) print(&quot;Best Parameters: {} n&quot;.format(model[&quot;best_params&quot;])) return models result = select_model(train,cols) . LogisticRegression Best Score: 0.8226711560044894 Best Parameters: {&#39;solver&#39;: &#39;liblinear&#39;} KNeighborsClassifier -- Best Score: 0.7833894500561167 Best Parameters: {&#39;algorithm&#39;: &#39;kd_tree&#39;, &#39;n_neighbors&#39;: 3, &#39;p&#39;: 1, &#39;weights&#39;: &#39;uniform&#39;} RandomForestClassifier - Best Score: 0.8451178451178452 Best Parameters: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 10, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 3, &#39;n_estimators&#39;: 9} . Submit to Kaggle . def save_submission_file(model,cols,filename=&quot;submission.csv&quot;): holdout_data = holdout[cols] predictions = model.predict(holdout_data) holdout_ids = holdout[&quot;PassengerId&quot;] submission_df = {&quot;PassengerId&quot;: holdout_ids, &quot;Survived&quot;: predictions} submission = pd.DataFrame(submission_df) submission.to_csv(filename,index=False) best_rf_model = result[2][&quot;best_model&quot;] save_submission_file(best_rf_model,cols) .",
            "url": "https://phucnsp.github.io/blog/kaggle/2017/08/20/kaggle-titanic-machine-learning-from-disaster.html",
            "relUrl": "/kaggle/2017/08/20/kaggle-titanic-machine-learning-from-disaster.html",
            "date": " • Aug 20, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I have been working as Data Scientist at MTI Technology Vietnam since 2018 and my journey in AI field started since 2017. In here, I mainly work with OCR (optical charcter recognition) projects where we not only have to extract texts from documents but also classify it into some specific fields defined by clients. .",
          "url": "https://phucnsp.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

}