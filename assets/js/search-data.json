{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://phucnsp.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Tutorial markdown with fastpage",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://phucnsp.github.io/blog/tutorial/2020/01/14/tutorial-markdown-and-fastpage.html",
            "relUrl": "/tutorial/2020/01/14/tutorial-markdown-and-fastpage.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://phucnsp.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Singapore, quan s√°t v√† suy ng·∫´m",
            "content": "ƒê·ª£t th√°ng m∆∞·ªùi v·ª´a r·ªìi t√¥i c√≥ d·ªãp ƒëi d·∫°o quanh anh h√†ng x√≥m Singapore. C≈©ng h∆°n hai nƒÉm r·ªìi t√¥i m·ªõi c√≥ d·ªãp ƒëi ra kh·ªèi Vietnam, t√¥i th√≠ch c√°i c·∫£m gi√°c ƒë∆∞·ª£c bay, ƒë∆∞·ª£c transit ·ªü m·ªôt s√¢n bay n√†o ƒë√≥, ng·ªìi ng·∫Øm nh·ªØng ng∆∞·ªùi xa l·∫° r·∫£o b∆∞·ªõc qua l·∫°i, c√¥ ƒë∆°n nh∆∞ng th√∫ v·ªã. Singapore n·ªïi ti·∫øng b·ªüi s·ª± hi·ªán ƒë·∫°i, b·ªüi nh·ªØng t√≤a nh√† ch·ªçc tr·ªùi, b·ªüi Universal Studio‚Ä¶nh∆∞ng ƒë·ªëi v·ªõi t√¥i th√¨ nh·ªØng c√°i ƒë√≥ ch·∫£ c√≥ √Ω nghƒ©a g√¨ c·∫£. Khi b·∫°n ƒë√£ t·ª´ng ng·ªìi cafe ·ªü ƒë·∫°i l·ªô danh v·ªçng Hollywood, ƒë√£ ƒëi loanh quanh ·ªü qu·∫£ng tr∆∞·ªùng th·ªùi ƒë·∫°i NewYork, ƒë√£ xem bi·ªÉu t√¨nh c·ªßa ng∆∞·ªùi d√¢n Syria tr∆∞·ªõc nh√† tr·∫Øng M·ªπ th√¨ li·ªáu nh·ªØng t√≤a nh√† ch·ªçc tr·ªùi ·ªü Singapore c√≥ g√¨ ƒë·ªÉ h·∫•p d·∫´n. T√¥i ƒëi Singapore ch∆°i ƒë∆°n gi·∫£n v√¨ t√¥i mu·ªën d·∫´n v·ª£ ƒëi n∆∞·ªõc ngo√†i cho bi·∫øt v√† c≈©ng v√¨ t√¥i c·∫ßn ƒëi du l·ªãch sau m·ªôt kho·∫£ng th·ªùi gian r·∫•t r·∫•t l√¢u c·∫Øm ƒë·∫ßu v√¥ c√¥ng vi·ªác. . Th·∫≠t may thay, Singapore kh√°c nh·ªØng n∆°i t√¥i t·ª´ng qua v√† c√≤n l√†m t√¥i m·∫•t m·∫•y ti·∫øng ƒë·ªÉ ng·ªìi vi·∫øt c√°i note n√†y. . Chi·ªÅu h√¥m qua t√¥i tr·ªü v·ªÅ T√¢n S√¢n Nh·∫•t sau h∆°n 1 ti·∫øng 30 ph√∫t bay, ƒë∆°n gi·∫£n l√† c·∫£m gi√°c x√≥t xa. L·∫° nh·ªâ, l·∫ßn tr∆∞·ªõc bay ·ªü Berlin v·ªÅ ch·∫£ th·∫•y g√¨, l·∫ßn n√†y t·ª± nhi√™n l·∫°i x√≥t xa. T·∫°i ng∆∞·ªùi ta ·ªü c√°i t·∫ßm cao h∆°n m√¨nh qu√°, t√¥i khen Singapore m√† t√¥i x√≥t cho Saigon, t√¥i nh√¨n anh xe √¥m, nh√¨n nh·ªØng ch·ªã tay x√°ch n√°ch mang, nh√¨n c√°i c√°ch ng∆∞·ªùi d√¢n qu√™ t√¥i bƒÉng qua ƒë∆∞·ªùng ch·ªÖnh ch·ªá b·∫•t ch·∫•p ƒë√®n t√≠n hi·ªáu, ti·∫øng la h√©t ch√®o k√©o kh√°ch, ti·∫øng b√≥p c√≤i inh ·ªèi, xa xa l√† anh c√¥ng an ng·ªìi l∆∞·ªõt ƒëi·ªán tho·∫°i ·ªü m·ªôt x√≥ c·∫°nh l·ªëi ra s√¢n bay, t·∫•t c·∫£ ƒë·ªÅu l√†m t√¥i x√≥t xa. H√¥m th·ª© nƒÉm tr∆∞·ªõc khi bay ƒëi ch∆°i c√≤n th·∫•y b√¨nh th∆∞·ªùng m√† nh·ªâ! . Changi ƒë√≥n ch√†o v·ª£ ch·ªìng t√¥i b·∫±ng c√°i th√°c n∆∞·ªõc trong nh√† cao nh·∫•t th·∫ø gi·ªõi, ƒë√≥ l√† c√°i woww ƒë·∫ßu ti√™n khi t√†u ƒëi·ªán k·∫øt n·ªëi gi·ªØa nh·ªØng terminal ƒë∆∞a ch√∫ng t√¥i ƒëi qua c√°i th√°c n∆∞·ªõc nh√¢n t·∫°o n√†y, ngay b√™n trong s√¢n bay. Ch√∫ng t√¥i v·ªÅ ƒë·∫øn hostel tr·ªùi c≈©ng ƒë√£ t·ªëi, h∆°n 8pm th√¨ ph·∫£i, n√™n c·∫£ 2 ƒë·ªÅu ƒëi ng·ªß s·ªõm ƒë·ªÉ h√¥m sau c√≥ s·ª©c m√† ƒëi ch∆°i. . . Ba ng√†y vi vu ·ªü th√†nh ph·ªë n√†y l√† ba ng√†y l√†m t√¥i suy ng·∫´m r·∫•t nhi·ªÅu, c√°i gi·∫£ ƒë·ªãnh v·ªÅ m·ªôt th√†nh ph·ªë hi·ªán ƒë·∫°i v·ªõi nh·ªØng t√≤a nh√† ch·ªçc tr·ªùi ch√°n ph√®o d·∫ßn d·∫ßn m·∫•t ƒëi khi t√¥i l·∫ßn l∆∞·ª£t ƒëi qua nh·ªØng t·ª• ƒëi·ªÉm n·ªïi ti·∫øng ·ªü ƒë√¢y, ch·ª©ng ki·∫øn c√°i c√°ch h·ªç n√¢ng t·∫ßm c√¥ng ngh·ªá l√™n th√†nh ngh√™ thu·∫≠t ƒë√£ l√†m t√¥i thay ƒë·ªïi g√≥c nh√¨n c·ªßa m√¨nh. X√¢y c√°i nh√† cao ch√≥t v√≥t l√™n th√¨ ·ªü th√†nh ph·ªë l·ªõn n√†o c≈©ng c√≥ nh∆∞ng ƒë·ªÉ khoa h·ªçc v√† ngh·ªá thu·∫≠t th·∫≠t s·ª± g·∫∑p nhau th√¨ kh√¥ng ph·∫£i n∆°i n√†o c≈©ng l√†m ƒë∆∞·ª£c v√† c≈©ng kh√¥ng ph·∫£i ng∆∞·ªùi d√¢n ·ªü ƒë√¢u c≈©ng c·∫£m th·ª• ƒë∆∞·ª£c. S√°ng s·ªõm h√¥m th·ª© b·∫£y, trong l√∫c d·∫°o b·ªô v√† ng·ªìi ngh·ªâ ng∆°i ·ªü tr·∫°m xe bus g·∫ßn khu Chinatown, t√¥i ƒë·ªçc ƒë∆∞·ª£c m·ªôt b√†i chia s·∫ª c·ªßa anh country manager b√™n Knorex, t·ª´ng h·ªçc t·∫≠p v√† l√†m vi·ªác ·ªü Singapore sau ƒë√≥ v·ªÅ qu·∫£n l√Ω branch ·ªü Ho Chi Minh. G√≥c nh√¨n c·ªßa anh ·∫•y trong c√¥ng vi·ªác th·∫≠t ra kh√¥ng m·ªõi ƒë·ªëi v·ªõi t√¥i nh∆∞ng ƒë∆°n gi·∫£n l√† n√≥ ƒë∆∞·ª£c ƒë·ªçc ƒë√∫ng l√∫c, ƒë√∫ng th·ªùi ƒëi·ªÉm. ƒê√≥ l√† g√≥c nh√¨n v·ªÅ vi·ªác h·ªçc su·ªët ƒë·ªùi, h·ªçc ·ªü b·∫•t k√¨ ho√†n c·∫£nh n√†o m√† kh√¥ng ph·∫£i ch·ªù c√≥ th·∫ßy d·∫°y m·ªõi h·ªçc ƒë∆∞·ª£c, v·ªÅ growth mindset, v·ªÅ th√°i ƒë·ªô l√†m vi·ªác cho ƒëi tr∆∞·ªõc ƒë·ªÉ nh·∫≠n l·∫°i sau, ƒë·∫∑c bi·ªát l√† v·ªÅ s·ª± to√†n c·∫ßu h√≥a nh√¢n l·ª±c c·ªßa c√°c c√¥ng ty, t·∫≠p ƒëo√†n, ƒë·ªìng nghi·ªáp c·ªßa b·∫°n ng√†y nay kh√¥ng c√≤n l√† nh·ªØng anh em b·∫°n d√¨ n·ªØa m√† c√≥ th·ªÉ ƒë·∫øn t·ª´ b·∫•t k√¨ ƒë√¢u tr√™n th·∫ø gi·ªõi n√†y, ·∫§n ƒë·ªô, Brazile, Chile, Sillicon Valley, etc. B√†i chia s·∫ª ·∫•y nh∆∞ l√† ch·∫•t x√∫c t√°c cho chuy·∫øn ƒëi c·ªßa t√¥i, t√¥i b·∫Øt ƒë·∫ßu ƒë·ªÉ √Ω h∆°n t·ªõi con ng∆∞·ªùi n∆°i ƒë√¢y, t·ª´ng chi ti·∫øt nh·ªè tr√™n ƒë∆∞·ªùng, c√°ch h·ªç ƒë√†o ƒë∆∞·ªùng l√™n v√† l·∫•p l·∫°i c·∫©n th·∫≠n m√† kh√¥ng ƒë·ªÉ nh·∫•p nh√¥, c√°ch h·ªç gi·ªØ g√¨n s·∫°ch s·∫Ω kh√¥ng ch·ªâ ph√≠a tr∆∞·ªõc m√† c·∫£ ph√≠a sau c·ªßa nh√† h√†ng cho ƒë·∫øn c√°ch h·ªç quy ho·∫°ch b·ªë tr√≠ nh√† c·ª≠a, b·ªë tr√≠ m·∫£ng xanh kh·∫Øp n∆°i‚Ä¶ t√¥i ƒë√£ c·∫£m th·∫•y th·∫≠t kh√≥ khƒÉn trong vi·ªác t√¨m ra ch·ªó ƒë·ªÉ bƒ©u m√¥i ch√™ c∆∞·ªùi. M·ªçi th·ª© ƒë∆∞·ª£c ho√†n thi·ªán m·ªôt c√°ch tuy·ªát v·ªùi ·ªü t·∫ßm qu·ªëc gia, Singapore c√≥ l·∫Ω l√† v√≠ d·ª• ƒëi·ªÉn h√¨nh nh·∫•t cho kh√°i ni·ªám qu·∫£n l√Ω ‚Äút·ª± do trong khu√¥n kh·ªï‚Äù, ng∆∞·ªùi d√¢n c√≥ kh√¥ng gian ƒë·ªÉ th·ªÉ hi·ªán c√°i ri√™ng nh∆∞ng ph·∫£i trong khu√¥n kh·ªï nh·∫•t ƒë·ªãnh ƒë·ªÉ gi·ªØ g√¨n c√°i chung. . Tr∆∞a th·ª© b·∫£y v·ª£ ch·ªìng t√¥i ƒëi d·∫°o b·ªô ·ªü khu Marina Bay th√¨ t√¨nh c·ªù ƒë∆∞·ª£c d·ª± gi·ªù m·ªôt bu·ªïi t·∫≠p h√°t c·ªßa m·∫•y em ti·ªÉu h·ªçc. T·ª•i nh·ªè bi·ªÉu di·ªÖn v·ªÅ nh·∫°c k·ªãch hay g√¨ ƒë√≥ ƒë·∫°i lo·∫°i th·∫ø, nghe kh√° xa x·ªâ ƒë·ªëi v·ªõi ƒë·∫°i ƒëa s·ªë d√¢n Vi·ªát Nam m√¨nh. Th·∫≠t bu·ªìn c∆∞·ªùi khi t√¥i khen con n√≠t ·ªü Singapore n√≥i ti·∫øng anh hay qu√°, h√°t t·ª± tin qu√°. Nh∆∞ng th·∫≠t s·ª± ch√∫ng ƒë√£ ƒë∆∞·ª£c th·ª´a h∆∞·ªüng di s·∫£n phi v·∫≠t th·ªÉ qu√° l·ªõn t·ª´ √¥ng L√Ω, ti·∫øng anh vs ti·∫øng trung, ƒë·ªÉ gi·ªù ƒë√¢y c√≥ th·ªÉ ti·∫øp c·∫≠n d·ªÖ d√†ng h∆°n v·ªõi tinh hoa th·∫ø gi·ªõi, l·∫°i th√™m c√°i ki·ªÉu gi√°o d·ª•c khai s√°ng, t·ª± do th·ªÉ hi·ªán c√°i t√¥i c√° nh√¢n th·∫ø n√†y n·ªØa th√¨ h·ª°i ∆°i, m·∫•y ƒë·ª©a ch√°u ·ªü qu√™ ƒëang ƒëung ƒë∆∞a v√µng nghe th·∫ßn t∆∞·ª£ng H√†n Qu·ªëc h√°t c·∫£ ng√†y hay ƒëang t·ª• t·∫≠p qu√°n tr√† s·ªØa ƒë·ªÉ ch√©m gi√≥, th√¨ r·ªìi c∆° may n√†o ƒë·ªÉ ch√∫ng c·∫°nh tranh trong c√°i th·∫ø gi·ªõi to√†n c·∫ßu h√≥a ƒë√¢y. C√≥ th·ªÉ nhi·ªÅu ng∆∞·ªùi s·∫Ω nghƒ© l√†m g√¨ t·ªõi n·ªói, v·∫´n c√≥ r·∫•t nhi·ªÅu nh√¢n t√†i ng∆∞·ªùi Vi·ªát h·ªçc tr∆∞·ªùng l√†ng nh·ªØng v·∫´n n·ªïi danh th·∫ø gi·ªõi ƒë√≥ th√¥i, nh∆∞ng khi ƒë√°nh gi√° c√°i t·∫ßm qu·ªëc gia th√¨ ng∆∞·ªùi ta kh√¥ng n√≥i c√¢u chuy·ªán c·ªßa m·ªôt v√†i ng∆∞·ªùi xu·∫•t ch√∫ng, ng∆∞·ªùi ta n√≥i ƒë·∫øn s·ª©c m·∫°nh c·ªßa passport index (s·ª©c m·∫°nh t·∫•m h·ªô chi·∫øu Vi·ªát Nam h√¨nh nh∆∞ ƒë·ª©ng g·∫ßn √°p ch√≥t b·∫£ng x·∫øp h·∫°ng), ng∆∞·ªùi ta ƒë√°nh gi√° th√†nh t√≠ch to√†n ƒëo√†n, c√≥ ƒë·∫•y nh·ªØng ng√¥i sao v√†ng l·∫ª loi ƒëo·∫°t huy ch∆∞∆°ng t·∫ßm th·∫ø gi·ªõi nh∆∞ng c√≥ m·∫•y b√†i b√°o ƒëƒÉng th√†nh t√≠ch c·ªßa to√†n ƒëo√†n kh√¥ng! V√† theo tr·∫£i nghi·ªám c√° nh√¢n c·ªßa t√¥i, b√¢y gi·ªù khi b·∫°n h·ªèi m·ªôt ng∆∞·ªùi n∆∞·ªõc ngo√†i bi·∫øt g√¨ v·ªÅ Vi·ªát Nam th√¨ c√¢u tr·∫£ l·ªùi kh√≥ m√† ngo√†i chi·∫øn tranh v√† gia c√¥ng qu·∫ßn √°o. . √Ä n√≥i m·ªôt t√≠ v·ªÅ kh√°i ni·ªám ‚ÄúDu l·ªãch, quan s√°t v√† suy ng·∫´m‚Äù m√† t√¥i ƒë√£ t·ª´ng ƒë·ªçc ƒë√¢u ƒë√≥, n√≥ mang t·ªõi cho b·∫°n m·ªôt g√≥c nh√¨n ‚Äúth·∫•m‚Äù h∆°n v·ªÅ n∆°i m√¨nh ƒë√£ ƒëi qua. Du l·ªãch kh√¥ng d·ª´ng l·∫°i ·ªü nh·ªØng t·∫•m h√¨nh selffie khoe tr√™n facebook m√† c√≤n c·∫£ ·ªü c·∫£m th·ª• c√° nh√¢n. Nh∆∞ng c≈©ng c√≥ l·∫Ω s·ª± c·∫£m th·ª• n√†y kh√¥ng d·ªÖ m√† c√≥ ƒë∆∞·ª£c, sau m·ªôt th·ªùi gian d√†i li√™n t·ª•c h·ªçc t·∫≠p, ƒë·ªçc s√°ch, t√≠ch l≈©y ki·∫øn th·ª©c th√¨ m·ªõi may ra ch·∫•p ch·ªõm c·∫£m nh·∫≠n ƒë∆∞·ª£c ƒëi·ªÅu n√†y. ƒê√≥ l√† l√Ω do v√¨ sao m√† ch√∫ng ta r·∫•t hay th·∫•y du kh√°ch n∆∞·ªõc ngo√†i t·ªõi Vi·ªát Nam du l·ªãch th∆∞·ªùng t·ªõi b·∫£o t√†ng, th∆∞·ªùng mua s√°ch v·ªÅ Vi·ªát Nam ƒë·ªÉ ƒë·ªçc, ƒë∆°n gi·∫£n v√¨ h·ªç ƒëang c·∫£m th·ª• m·ªôt c√°ch s√¢u h∆°n vƒÉn h√≥a, con ng∆∞·ªùi, ƒë·∫•t n∆∞·ªõc ch√∫ng ta, ch·ª© kh√¥ng ch·ªâ d·ª´ng l·∫°i b·ªÅ n·ªïi ·ªü nh·ªØng b·ª©c ·∫£nh. Ng√†y nay t·∫•t nhi√™n kh√¥ng c·∫ßn ph·∫£i t·ªõi b·∫£o t√†ng th√¨ ch√∫ng ta m·ªõi bi·∫øt v·ªÅ m·ªôt n∆°i n√†o ƒë√≥, m·ªçi th·ª© c√≥ th·ªÉ ƒë∆∞·ª£c ƒë·ªçc d·ªÖ d√†ng qua internet nh∆∞ng th·∫≠t s·ª± c·∫£m gi√°c c·ªßa vi·ªác ƒëi du l·ªãch, c·∫£m nh·∫≠n b·∫±ng ch√≠nh t·∫•t c·∫£ gi√°c quan ƒë·ªÉ r·ªìi c√≥ nh·ªØng suy ng·∫´m s√¢u s·∫Øc h∆°n v·ªÅ s·ª± v·∫≠t s·ª± vi·ªác xung quanh mang l·∫°i nh·ªØng l·ª£i √≠ch to l·ªõn. N√≥ l√† c∆°n m∆∞a m√πa h√® cho nh·ªØng b·ªô n√£o ƒë√£ b·ªã lu m·ªù b·ªüi nh·ªØng th·ª© l·∫∑p ƒëi l·∫∑p l·∫°i h·∫±ng ng√†y, th·∫≠m ch√≠ n√≥ c√≥ th·ªÉ thay ƒë·ªïi ho√†n to√†n g√≥c nh√¨n c·ªßa m√¨nh t·ª´ tr∆∞·ªõc t·ªõi nay v·ªÅ m·ªôt vi·ªác n√†o ƒë√≥. . Quay tr·ªü l·∫°i c√¢u chuy·ªán x√≥t xa Singapore, c√≥ l·∫Ω m·ªôt d·ªãp n√†o ƒë√≥ t√¥i s·∫Ω tr·ªü l·∫°i ƒë·∫•t n∆∞·ªõc n√†y ƒë·ªÉ xem c·∫£m gi√°c c√≤n nh∆∞ x∆∞a kh√¥ng. Nh∆∞ m·ªôt l·ªùi nh·∫Øn nh·ªß cho b·∫£n th√¢n, to√†n c·∫ßu h√≥a l√† c√≥ th·∫≠t v√† ·ªü ngay ƒë√≠t r·ªìi. Qu√™n ƒëi nh·ªØng x√≠ch m√≠ch, nh·ªØng c√¢u chuy·ªán v·∫∑t ·ªü xung quanh, trong l√∫c m√¨nh ng·ªìi nhi·ªÅu chuy·ªán th√¨ th·∫±ng kh√°c ·ªü ƒë√¢u ƒë√≥ v·∫´n ƒëang mi·ªát m√†i tu luy·ªán. K·ª∑ lu·∫≠t h∆°n, h·ªçc t·∫≠p ·ªü m·ªçi n∆°i, h·ªçc ·ªü b·∫•t k√¨ ai, ƒë·ª´ng ch·ªù c√≥ th·∫ßy d·∫°y th√¨ m·ªõi h·ªçc ƒë∆∞·ª£c, ƒë·ª´ng s·ª£ h·ªçc nhi·ªÅu qu√° s·∫Ω l√†m n√£o m√¨nh b·ªõt th√¥ng minh. H·ªçc ki·∫øn th·ª©c ƒë·ªÉ tƒÉng tr√≠ th√¥ng m√¨nh IQ, h·ªçc vƒÉn h√≥a ngh·ªá thu·∫≠t, h·ªçc c√°ch ·ª≠ng x·ª≠ ƒë·ªÉ tƒÉng tr√≠ th√¥ng minh EQ. L√†m vi·ªác v·ªõi m·ªôt th√°i ƒë·ªô s·∫µn s√†ng hy sinh, th√¢n t√¥i ƒë√¢y n√®, v·ª©t vi·ªác cho t√¥i ƒëi. D√π bi·∫øt r·∫±ng m·ªôt c√°nh √©n nh·ªè kh√≥ l√†m n√™n m√πa xu√¢n, t√¥i c√≥ tr·ªü th√†nh c√¥ng d√¢n to√†n c·∫ßu c≈©ng ch∆∞a ch·∫Øc Vi·ªát Nam s·∫Ω t·ªët h∆°n nh∆∞ng √≠t nh·∫•t t√¥i kh√¥ng mu·ªën l√† ng∆∞·ªùi g√≥p ph·∫ßn cho ƒë·∫•t n∆∞·ªõc m√† t√¥i y√™u qu√Ω tr·ªü n√™n t·ªá h∆°n! . T·ªëi qua v·ªÅ ƒë·∫øn nh√† con b·∫°n ng∆∞·ªùi Malay ƒëang s·ªëng ·ªü Singapore, th·∫•y h√¨nh ƒëƒÉng facebook, m·ªõi nh·∫Øn r·ªß cafe, c≈©ng h∆°i ti·∫øc kh√¥ng g·∫∑p ƒë∆∞·ª£c. N√≥ khoe m·ªõi c√≥ th·∫±ng b·ªì ng∆∞·ªùi ·∫§n, t·ª•i n√≥ v·ª´a ƒëi ƒë√°m c∆∞·ªõi b·∫°n ·ªü ƒë·∫£o Galapagos b√™n Ecuador v·ªÅ v√† r·ªß nƒÉm sau ra H√† N·ªôi xem Vietnam F1 Race. . Ch·∫Øc nƒÉm sau ra H√† N·ªôi, quan s√°t v√† ng·∫´m ti·∫øp nh·ªâ‚Ä¶ .",
            "url": "https://phucnsp.github.io/blog/travel/2019/10/07/Singpore-trip-and-remaining.html",
            "relUrl": "/travel/2019/10/07/Singpore-trip-and-remaining.html",
            "date": " ‚Ä¢ Oct 7, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Data Science workflow recommendation",
            "content": "Repository of this workflow is stored here . Production data science template . The template of this repository follows production-data-science workflow, which focuses on productionizing data scientist‚Äôs work, make the analysis or research to be reusable, applicable to production. The workflow is separated into 2 phases: . exploration phase is where data scientist explores the project, mainly work with jupyter notebook. All the work in this phase will be stored in exploration folder. | production phase is where data scientists‚Äô works are refactored into packages so it can be reuse, imported. All the work in this phase will be stored in your_package folder. | . How to setup a new repository - for maintainer . git clone https://gitlab.com/Phuc_Su/production_data_science_template.git git clone &lt;your_project_repository&gt; cd &lt;your_project_name&gt; git checkout -b product-initial-setup # open Finder, copy all content of production_data_science_template into your project repository, except .git and .idea folder conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; pip install git-lfs # in case you want to add some large file extension other than .jpg, .pdf, .csv, .xlsx git lfs track &lt;add large file path&gt; # rename &lt;your package&gt; folder and modify setup.py, most importance is require_packages. See example below # write something about your project in README.md pip install -e . pip freeze | grep -v &lt;package_name&gt; &gt; requirements.txt git add . git commit -m &quot;First commit&quot; git push -u origin HEAD . Example of setup.py . setup( name=&#39;your_project&#39;, version=&#39;v0.1&#39;, description=&#39;&#39;, long_description=readme(), classifiers=[ &#39;Programming Language :: Python :: 3&#39;, ], url=&#39;https://github.com/phucnsp/production_data_science_template&#39;, author=&#39;Phuc_Su&#39;, author_email=&#39;&#39;, license=&#39;&#39;, packages=[&#39;your_package&#39;], install_requires=[ &#39;pypandoc&gt;=1.4&#39;, &#39;watermark&gt;=1.5.0&#39;, &#39;pandas&gt;=0.20.3&#39;, &#39;scikit-learn&gt;=0.19.0&#39;, &#39;scipy&gt;=0.19.1&#39;, &#39;matplotlib&gt;=2.1.0&#39;, &#39;pytest&gt;=3.2.3&#39;, &#39;pytest-runner&gt;=2.12.1&#39;, &#39;click&gt;=6.7&#39; ], setup_requires=[&#39;pytest-runner&#39;], tests_require=[&#39;pytest&#39;], ) . and you are ready~! üéâ . Note: if you want to setup notification on slack for merge request from gitlab, reference here . How to contribute - for developers . Setup first time . bash conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; git clone &lt;repository url&gt; cd to/the/project/directory pip install -r requirements.txt pip install -e . . For a private repository accessible only through an SSH authentication, substitute https://github.com/ with git@github.com:. . Returning to work . Some rules: 1 branch/1 exploration/1 folder | branch-name convention: explore-* for exploration, refactor-* for refactor | . | . git checkout master git pull --all # if you continue to work on old branch git checkout &lt;branch&gt; # if you want to start a new exploration git checkout -b &lt;new_branch&gt; # if your branch is far behind master and you want to merge git merge master ##################### Start working ##################### git add &lt;path_to_work_files/folder&gt; git commit -m &quot;some message&quot; git push -u origin HEAD . Notes . requirements.txt helps to setup your virtual environment, to make sure all contributors working on the same environments. So whenever you have a new libraries need to install, after installing you need to add it into requirements.txt by pip freeze | grep -v &lt;package_name&gt; &gt; requirements.txt | setup.py allows you to create packages that you can redistribute. This script is meant to install your package on the end user‚Äôs system, not to prepare the development environment. packages - in-house development packages. | install_requires - packages that our development packages dependence on. | py_modules=[&#39;new_module&#39;] - in-house development modules need to install (placed in root directory) | . | pip install -e . - to install packages/modules from setup.py, in the editable mode. | If you want to add large file into working repository: pip install git-lfs git lfs install # Tell LFS to track files with given path git lfs track &quot;path_to_large_file&quot; # Tell LFS to track files with format &quot;*.jpg&quot; git lfs track &quot;*.jpg&quot; # Tell LFS to track content of the whole directory git lfs track &quot;data/*&quot; . | . How to use the package - for users . Install the library . conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; pip install -e &#39;git+https://github.com/phucnsp/production_data_science_template.git&#39; . For a private repository accessible only through an SSH authentication, substitute git+https://github.com with git+ssh://git@github.com. Note that -e argument above to make the installation editable. . Leisure read . Production Data Science tutorial | Writing a setup script | Minimum structure | gitlab slack notification service | git strategy | .",
            "url": "https://phucnsp.github.io/blog/tutorial/2019/05/10/data-science-template.html",
            "relUrl": "/tutorial/2019/05/10/data-science-template.html",
            "date": " ‚Ä¢ May 10, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "Predict house price in America",
            "content": "Introduction . import pandas as pd pd.options.display.max_columns = 999 import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import KFold from sklearn.metrics import mean_squared_error from sklearn import linear_model from sklearn.model_selection import KFold . df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) . def transform_features(df): return df def select_features(df): return df[[&quot;Gr Liv Area&quot;, &quot;SalePrice&quot;]] def train_and_test(df): train = df[:1460] test = df[1460:] ## You can use `pd.DataFrame.select_dtypes()` to specify column types ## and return only those columns as a data frame. numeric_train = train.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) numeric_test = test.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) ## You can use `pd.Series.drop()` to drop a value. features = numeric_train.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df) rmse . 57088.251612639091 . Feature Engineering . Handle missing values: All columns: Drop any with 5% or more missing values for now. Text columns: Drop any with 1 or more missing values for now. Numerical columns: For columns with missing values, fill in with the most common value in that column . 1: All columns: Drop any with 5% or more missing values for now. . ## Series object: column name -&gt; number of missing values num_missing = df.isnull().sum() . # Filter Series to columns containing &gt;5% missing values drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() # Drop those columns from the data frame. Note the use of the .index accessor df = df.drop(drop_missing_cols.index, axis=1) . ## Series object: column name -&gt; number of missing values text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) ## Filter Series to columns containing *any* missing values drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) . ## Compute column-wise missing value counts num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() fixable_numeric_cols . BsmtFin SF 1 1 BsmtFin SF 2 1 Bsmt Unf SF 1 Total Bsmt SF 1 Garage Cars 1 Garage Area 1 Bsmt Full Bath 2 Bsmt Half Bath 2 Mas Vnr Area 23 dtype: int64 . ## Compute the most common value for each column in `fixable_nmeric_missing_cols`. replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] replacement_values_dict . {&#39;Bsmt Full Bath&#39;: 0.0, &#39;Bsmt Half Bath&#39;: 0.0, &#39;Bsmt Unf SF&#39;: 0.0, &#39;BsmtFin SF 1&#39;: 0.0, &#39;BsmtFin SF 2&#39;: 0.0, &#39;Garage Area&#39;: 0.0, &#39;Garage Cars&#39;: 2.0, &#39;Mas Vnr Area&#39;: 0.0, &#39;Total Bsmt SF&#39;: 0.0} . ## Use `pd.DataFrame.fillna()` to replace missing values. df = df.fillna(replacement_values_dict) . ## Verify that every column has 0 missing values df.isnull().sum().value_counts() . 0 64 dtype: int64 . years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_sold[years_sold &lt; 0] . 2180 -1 dtype: int64 . years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] years_since_remod[years_since_remod &lt; 0] . 1702 -1 2180 -2 2181 -1 dtype: int64 . ## Create new columns df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod ## Drop rows with negative values for both of these new features df = df.drop([1702, 2180, 2181], axis=0) ## No longer need original year columns df = df.drop([&quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis = 1) . Drop columns that: a. that aren&#39;t useful for ML b. leak data about the final sale . ## Drop columns that aren&#39;t useful for ML df = df.drop([&quot;PID&quot;, &quot;Order&quot;], axis=1) ## Drop columns that leak info about the final sale df = df.drop([&quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Yr Sold&quot;], axis=1) . Let&#39;s update transform_features() . def transform_features(df): num_missing = df.isnull().sum() drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() df = df.drop(drop_missing_cols.index, axis=1) text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] df = df.fillna(replacement_values_dict) years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod df = df.drop([1702, 2180, 2181], axis=0) df = df.drop([&quot;PID&quot;, &quot;Order&quot;, &quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis=1) return df def select_features(df): return df[[&quot;Gr Liv Area&quot;, &quot;SalePrice&quot;]] def train_and_test(df): train = df[:1460] test = df[1460:] ## You can use `pd.DataFrame.select_dtypes()` to specify column types ## and return only those columns as a data frame. numeric_train = train.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) numeric_test = test.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) ## You can use `pd.Series.drop()` to drop a value. features = numeric_train.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df) rmse . 55275.367312413066 . Feature Selection . numerical_df = transform_df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]) numerical_df . MS SubClass Lot Area Overall Qual Overall Cond Mas Vnr Area BsmtFin SF 1 BsmtFin SF 2 Bsmt Unf SF Total Bsmt SF 1st Flr SF 2nd Flr SF Low Qual Fin SF Gr Liv Area Bsmt Full Bath Bsmt Half Bath Full Bath Half Bath Bedroom AbvGr Kitchen AbvGr TotRms AbvGrd Fireplaces Garage Cars Garage Area Wood Deck SF Open Porch SF Enclosed Porch 3Ssn Porch Screen Porch Pool Area Misc Val Yr Sold SalePrice Years Before Sale Years Since Remod . 0 20 | 31770 | 6 | 5 | 112.0 | 639.0 | 0.0 | 441.0 | 1080.0 | 1656 | 0 | 0 | 1656 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 7 | 2 | 2.0 | 528.0 | 210 | 62 | 0 | 0 | 0 | 0 | 0 | 2010 | 215000 | 50 | 50 | . 1 20 | 11622 | 5 | 6 | 0.0 | 468.0 | 144.0 | 270.0 | 882.0 | 896 | 0 | 0 | 896 | 0.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 0 | 1.0 | 730.0 | 140 | 0 | 0 | 0 | 120 | 0 | 0 | 2010 | 105000 | 49 | 49 | . 2 20 | 14267 | 6 | 6 | 108.0 | 923.0 | 0.0 | 406.0 | 1329.0 | 1329 | 0 | 0 | 1329 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 312.0 | 393 | 36 | 0 | 0 | 0 | 0 | 12500 | 2010 | 172000 | 52 | 52 | . 3 20 | 11160 | 7 | 5 | 0.0 | 1065.0 | 0.0 | 1045.0 | 2110.0 | 2110 | 0 | 0 | 2110 | 1.0 | 0.0 | 2 | 1 | 3 | 1 | 8 | 2 | 2.0 | 522.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 244000 | 42 | 42 | . 4 60 | 13830 | 5 | 5 | 0.0 | 791.0 | 0.0 | 137.0 | 928.0 | 928 | 701 | 0 | 1629 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 6 | 1 | 2.0 | 482.0 | 212 | 34 | 0 | 0 | 0 | 0 | 0 | 2010 | 189900 | 13 | 12 | . 5 60 | 9978 | 6 | 6 | 20.0 | 602.0 | 0.0 | 324.0 | 926.0 | 926 | 678 | 0 | 1604 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 470.0 | 360 | 36 | 0 | 0 | 0 | 0 | 0 | 2010 | 195500 | 12 | 12 | . 6 120 | 4920 | 8 | 5 | 0.0 | 616.0 | 0.0 | 722.0 | 1338.0 | 1338 | 0 | 0 | 1338 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 6 | 0 | 2.0 | 582.0 | 0 | 0 | 170 | 0 | 0 | 0 | 0 | 2010 | 213500 | 9 | 9 | . 7 120 | 5005 | 8 | 5 | 0.0 | 263.0 | 0.0 | 1017.0 | 1280.0 | 1280 | 0 | 0 | 1280 | 0.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 506.0 | 0 | 82 | 0 | 0 | 144 | 0 | 0 | 2010 | 191500 | 18 | 18 | . 8 120 | 5389 | 8 | 5 | 0.0 | 1180.0 | 0.0 | 415.0 | 1595.0 | 1616 | 0 | 0 | 1616 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 1 | 2.0 | 608.0 | 237 | 152 | 0 | 0 | 0 | 0 | 0 | 2010 | 236500 | 15 | 14 | . 9 60 | 7500 | 7 | 5 | 0.0 | 0.0 | 0.0 | 994.0 | 994.0 | 1028 | 776 | 0 | 1804 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 442.0 | 140 | 60 | 0 | 0 | 0 | 0 | 0 | 2010 | 189000 | 11 | 11 | . 10 60 | 10000 | 6 | 5 | 0.0 | 0.0 | 0.0 | 763.0 | 763.0 | 763 | 892 | 0 | 1655 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 440.0 | 157 | 84 | 0 | 0 | 0 | 0 | 0 | 2010 | 175900 | 17 | 16 | . 11 20 | 7980 | 6 | 7 | 0.0 | 935.0 | 0.0 | 233.0 | 1168.0 | 1187 | 0 | 0 | 1187 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 6 | 0 | 2.0 | 420.0 | 483 | 21 | 0 | 0 | 0 | 0 | 500 | 2010 | 185000 | 18 | 3 | . 12 60 | 8402 | 6 | 5 | 0.0 | 0.0 | 0.0 | 789.0 | 789.0 | 789 | 676 | 0 | 1465 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 393.0 | 0 | 75 | 0 | 0 | 0 | 0 | 0 | 2010 | 180400 | 12 | 12 | . 13 20 | 10176 | 7 | 5 | 0.0 | 637.0 | 0.0 | 663.0 | 1300.0 | 1341 | 0 | 0 | 1341 | 1.0 | 0.0 | 1 | 1 | 2 | 1 | 5 | 1 | 2.0 | 506.0 | 192 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 171500 | 20 | 20 | . 14 120 | 6820 | 8 | 5 | 0.0 | 368.0 | 1120.0 | 0.0 | 1488.0 | 1502 | 0 | 0 | 1502 | 1.0 | 0.0 | 1 | 1 | 1 | 1 | 4 | 0 | 2.0 | 528.0 | 0 | 54 | 0 | 0 | 140 | 0 | 0 | 2010 | 212000 | 25 | 25 | . 15 60 | 53504 | 8 | 5 | 603.0 | 1416.0 | 0.0 | 234.0 | 1650.0 | 1690 | 1589 | 0 | 3279 | 1.0 | 0.0 | 3 | 1 | 4 | 1 | 12 | 1 | 3.0 | 841.0 | 503 | 36 | 0 | 0 | 210 | 0 | 0 | 2010 | 538000 | 7 | 7 | . 16 50 | 12134 | 8 | 7 | 0.0 | 427.0 | 0.0 | 132.0 | 559.0 | 1080 | 672 | 0 | 1752 | 0.0 | 0.0 | 2 | 0 | 4 | 1 | 8 | 0 | 2.0 | 492.0 | 325 | 12 | 0 | 0 | 0 | 0 | 0 | 2010 | 164000 | 22 | 5 | . 17 20 | 11394 | 9 | 2 | 350.0 | 1445.0 | 0.0 | 411.0 | 1856.0 | 1856 | 0 | 0 | 1856 | 1.0 | 0.0 | 1 | 1 | 1 | 1 | 8 | 1 | 3.0 | 834.0 | 113 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 394432 | 0 | 0 | . 18 20 | 19138 | 4 | 5 | 0.0 | 120.0 | 0.0 | 744.0 | 864.0 | 864 | 0 | 0 | 864 | 0.0 | 0.0 | 1 | 0 | 2 | 1 | 4 | 0 | 2.0 | 400.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 141000 | 59 | 59 | . 19 20 | 13175 | 6 | 6 | 119.0 | 790.0 | 163.0 | 589.0 | 1542.0 | 2073 | 0 | 0 | 2073 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 7 | 2 | 2.0 | 500.0 | 349 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 210000 | 32 | 22 | . 20 20 | 11751 | 6 | 6 | 480.0 | 705.0 | 0.0 | 1139.0 | 1844.0 | 1844 | 0 | 0 | 1844 | 0.0 | 0.0 | 2 | 0 | 3 | 1 | 7 | 1 | 2.0 | 546.0 | 0 | 122 | 0 | 0 | 0 | 0 | 0 | 2010 | 190000 | 33 | 33 | . 21 85 | 10625 | 7 | 6 | 81.0 | 885.0 | 168.0 | 0.0 | 1053.0 | 1173 | 0 | 0 | 1173 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 6 | 2 | 2.0 | 528.0 | 0 | 120 | 0 | 0 | 0 | 0 | 0 | 2010 | 170000 | 36 | 36 | . 22 60 | 7500 | 7 | 5 | 0.0 | 533.0 | 0.0 | 281.0 | 814.0 | 814 | 860 | 0 | 1674 | 1.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 0 | 2.0 | 663.0 | 0 | 96 | 0 | 0 | 0 | 0 | 0 | 2010 | 216000 | 10 | 10 | . 23 20 | 11241 | 6 | 7 | 180.0 | 578.0 | 0.0 | 426.0 | 1004.0 | 1004 | 0 | 0 | 1004 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 1 | 2.0 | 480.0 | 0 | 0 | 0 | 0 | 0 | 0 | 700 | 2010 | 149000 | 40 | 40 | . 24 20 | 12537 | 5 | 6 | 0.0 | 734.0 | 0.0 | 344.0 | 1078.0 | 1078 | 0 | 0 | 1078 | 1.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 1 | 2.0 | 500.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 149900 | 39 | 2 | . 25 20 | 8450 | 5 | 6 | 0.0 | 775.0 | 0.0 | 281.0 | 1056.0 | 1056 | 0 | 0 | 1056 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 1 | 1.0 | 304.0 | 0 | 85 | 184 | 0 | 0 | 0 | 0 | 2010 | 142000 | 42 | 42 | . 26 20 | 8400 | 4 | 5 | 0.0 | 804.0 | 78.0 | 0.0 | 882.0 | 882 | 0 | 0 | 882 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 4 | 0 | 2.0 | 525.0 | 240 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 126000 | 40 | 40 | . 27 20 | 10500 | 4 | 5 | 0.0 | 432.0 | 0.0 | 432.0 | 864.0 | 864 | 0 | 0 | 864 | 0.0 | 0.0 | 1 | 0 | 3 | 1 | 5 | 1 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 115000 | 39 | 39 | . 28 120 | 5858 | 7 | 5 | 0.0 | 1051.0 | 0.0 | 354.0 | 1405.0 | 1337 | 0 | 0 | 1337 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 1 | 2.0 | 511.0 | 203 | 68 | 0 | 0 | 0 | 0 | 0 | 2010 | 184000 | 11 | 11 | . 29 160 | 1680 | 6 | 5 | 504.0 | 156.0 | 0.0 | 327.0 | 483.0 | 483 | 504 | 0 | 987 | 0.0 | 0.0 | 1 | 1 | 2 | 1 | 5 | 0 | 1.0 | 264.0 | 275 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 96000 | 39 | 39 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2900 20 | 13618 | 8 | 5 | 198.0 | 1350.0 | 0.0 | 378.0 | 1728.0 | 1960 | 0 | 0 | 1960 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 8 | 2 | 3.0 | 714.0 | 172 | 38 | 0 | 0 | 0 | 0 | 0 | 2006 | 320000 | 1 | 0 | . 2901 20 | 11443 | 8 | 5 | 208.0 | 1460.0 | 0.0 | 408.0 | 1868.0 | 2028 | 0 | 0 | 2028 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 7 | 2 | 3.0 | 880.0 | 326 | 66 | 0 | 0 | 0 | 0 | 0 | 2006 | 369900 | 1 | 0 | . 2902 20 | 11577 | 9 | 5 | 382.0 | 1455.0 | 0.0 | 383.0 | 1838.0 | 1838 | 0 | 0 | 1838 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 9 | 1 | 3.0 | 682.0 | 161 | 225 | 0 | 0 | 0 | 0 | 0 | 2006 | 359900 | 1 | 0 | . 2903 20 | 31250 | 1 | 3 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1600 | 0 | 0 | 1600 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 270.0 | 0 | 0 | 135 | 0 | 0 | 0 | 0 | 2006 | 81500 | 55 | 55 | . 2904 90 | 7020 | 7 | 5 | 200.0 | 1243.0 | 0.0 | 45.0 | 1288.0 | 1368 | 0 | 0 | 1368 | 2.0 | 0.0 | 2 | 0 | 2 | 2 | 8 | 0 | 4.0 | 784.0 | 0 | 48 | 0 | 0 | 0 | 0 | 0 | 2006 | 215000 | 9 | 9 | . 2905 120 | 4500 | 6 | 5 | 116.0 | 897.0 | 0.0 | 319.0 | 1216.0 | 1216 | 0 | 0 | 1216 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 402.0 | 0 | 125 | 0 | 0 | 0 | 0 | 0 | 2006 | 164000 | 8 | 8 | . 2906 120 | 4500 | 6 | 5 | 443.0 | 1201.0 | 0.0 | 36.0 | 1237.0 | 1337 | 0 | 0 | 1337 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 405.0 | 0 | 199 | 0 | 0 | 0 | 0 | 0 | 2006 | 153500 | 8 | 8 | . 2907 20 | 17217 | 5 | 5 | 0.0 | 0.0 | 0.0 | 1140.0 | 1140.0 | 1140 | 0 | 0 | 1140 | 0.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 0 | 0.0 | 0.0 | 36 | 56 | 0 | 0 | 0 | 0 | 0 | 2006 | 84500 | 0 | 0 | . 2908 160 | 2665 | 5 | 6 | 0.0 | 0.0 | 0.0 | 264.0 | 264.0 | 616 | 688 | 0 | 1304 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 1 | 1.0 | 336.0 | 141 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 104500 | 29 | 29 | . 2909 160 | 2665 | 5 | 6 | 0.0 | 548.0 | 173.0 | 36.0 | 757.0 | 925 | 550 | 0 | 1475 | 0.0 | 0.0 | 2 | 0 | 4 | 1 | 6 | 1 | 1.0 | 336.0 | 104 | 26 | 0 | 0 | 0 | 0 | 0 | 2006 | 127000 | 29 | 29 | . 2910 160 | 3964 | 6 | 4 | 0.0 | 837.0 | 0.0 | 105.0 | 942.0 | 1291 | 1230 | 0 | 2521 | 1.0 | 0.0 | 2 | 1 | 5 | 1 | 10 | 1 | 2.0 | 576.0 | 728 | 20 | 0 | 0 | 0 | 0 | 0 | 2006 | 151400 | 33 | 33 | . 2911 20 | 10172 | 5 | 7 | 0.0 | 441.0 | 0.0 | 423.0 | 864.0 | 874 | 0 | 0 | 874 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 5 | 0 | 1.0 | 288.0 | 0 | 120 | 0 | 0 | 0 | 0 | 0 | 2006 | 126500 | 38 | 3 | . 2912 90 | 11836 | 5 | 5 | 0.0 | 149.0 | 0.0 | 1503.0 | 1652.0 | 1652 | 0 | 0 | 1652 | 0.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 3.0 | 928.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 146500 | 36 | 36 | . 2913 180 | 1470 | 4 | 6 | 0.0 | 522.0 | 0.0 | 108.0 | 630.0 | 630 | 0 | 0 | 630 | 1.0 | 0.0 | 1 | 0 | 1 | 1 | 3 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 73000 | 36 | 36 | . 2914 160 | 1484 | 4 | 4 | 0.0 | 252.0 | 0.0 | 294.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 1.0 | 253.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 79400 | 34 | 34 | . 2915 20 | 13384 | 5 | 5 | 194.0 | 119.0 | 344.0 | 641.0 | 1104.0 | 1360 | 0 | 0 | 1360 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 8 | 1 | 1.0 | 336.0 | 160 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 140000 | 37 | 27 | . 2916 180 | 1533 | 5 | 7 | 0.0 | 553.0 | 0.0 | 77.0 | 630.0 | 630 | 0 | 0 | 630 | 1.0 | 0.0 | 1 | 0 | 1 | 1 | 3 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 92000 | 36 | 36 | . 2917 160 | 1533 | 4 | 5 | 0.0 | 408.0 | 0.0 | 138.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 1.0 | 286.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 87550 | 36 | 36 | . 2918 160 | 1526 | 4 | 5 | 0.0 | 0.0 | 0.0 | 546.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 0.0 | 0.0 | 0 | 34 | 0 | 0 | 0 | 0 | 0 | 2006 | 79500 | 36 | 36 | . 2919 160 | 1936 | 4 | 7 | 0.0 | 0.0 | 0.0 | 546.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 90500 | 36 | 36 | . 2920 160 | 1894 | 4 | 5 | 0.0 | 252.0 | 0.0 | 294.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 286.0 | 0 | 24 | 0 | 0 | 0 | 0 | 0 | 2006 | 71000 | 36 | 36 | . 2921 90 | 12640 | 6 | 5 | 0.0 | 936.0 | 396.0 | 396.0 | 1728.0 | 1728 | 0 | 0 | 1728 | 0.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 2.0 | 574.0 | 40 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 150900 | 30 | 30 | . 2922 90 | 9297 | 5 | 5 | 0.0 | 1606.0 | 0.0 | 122.0 | 1728.0 | 1728 | 0 | 0 | 1728 | 2.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 2.0 | 560.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 188000 | 30 | 30 | . 2923 20 | 17400 | 5 | 5 | 0.0 | 936.0 | 0.0 | 190.0 | 1126.0 | 1126 | 0 | 0 | 1126 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 5 | 1 | 2.0 | 484.0 | 295 | 41 | 0 | 0 | 0 | 0 | 0 | 2006 | 160000 | 29 | 29 | . 2924 20 | 20000 | 5 | 7 | 0.0 | 1224.0 | 0.0 | 0.0 | 1224.0 | 1224 | 0 | 0 | 1224 | 1.0 | 0.0 | 1 | 0 | 4 | 1 | 7 | 1 | 2.0 | 576.0 | 474 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 131000 | 46 | 10 | . 2925 80 | 7937 | 6 | 6 | 0.0 | 819.0 | 0.0 | 184.0 | 1003.0 | 1003 | 0 | 0 | 1003 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 0 | 2.0 | 588.0 | 120 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 142500 | 22 | 22 | . 2926 20 | 8885 | 5 | 5 | 0.0 | 301.0 | 324.0 | 239.0 | 864.0 | 902 | 0 | 0 | 902 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 0 | 2.0 | 484.0 | 164 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 131000 | 23 | 23 | . 2927 85 | 10441 | 5 | 5 | 0.0 | 337.0 | 0.0 | 575.0 | 912.0 | 970 | 0 | 0 | 970 | 0.0 | 1.0 | 1 | 0 | 3 | 1 | 6 | 0 | 0.0 | 0.0 | 80 | 32 | 0 | 0 | 0 | 0 | 700 | 2006 | 132000 | 14 | 14 | . 2928 20 | 10010 | 5 | 5 | 0.0 | 1071.0 | 123.0 | 195.0 | 1389.0 | 1389 | 0 | 0 | 1389 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 6 | 1 | 2.0 | 418.0 | 240 | 38 | 0 | 0 | 0 | 0 | 0 | 2006 | 170000 | 32 | 31 | . 2929 60 | 9627 | 7 | 5 | 94.0 | 758.0 | 0.0 | 238.0 | 996.0 | 996 | 1004 | 0 | 2000 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 9 | 1 | 3.0 | 650.0 | 190 | 48 | 0 | 0 | 0 | 0 | 0 | 2006 | 188000 | 13 | 12 | . 2927 rows √ó 34 columns . abs_corr_coeffs = numerical_df.corr()[&#39;SalePrice&#39;].abs().sort_values() abs_corr_coeffs . BsmtFin SF 2 0.006127 Misc Val 0.019273 Yr Sold 0.030358 3Ssn Porch 0.032268 Bsmt Half Bath 0.035875 Low Qual Fin SF 0.037629 Pool Area 0.068438 MS SubClass 0.085128 Overall Cond 0.101540 Screen Porch 0.112280 Kitchen AbvGr 0.119760 Enclosed Porch 0.128685 Bedroom AbvGr 0.143916 Bsmt Unf SF 0.182751 Lot Area 0.267520 2nd Flr SF 0.269601 Bsmt Full Bath 0.276258 Half Bath 0.284871 Open Porch SF 0.316262 Wood Deck SF 0.328183 BsmtFin SF 1 0.439284 Fireplaces 0.474831 TotRms AbvGrd 0.498574 Mas Vnr Area 0.506983 Years Since Remod 0.534985 Full Bath 0.546118 Years Before Sale 0.558979 1st Flr SF 0.635185 Garage Area 0.641425 Total Bsmt SF 0.644012 Garage Cars 0.648361 Gr Liv Area 0.717596 Overall Qual 0.801206 SalePrice 1.000000 Name: SalePrice, dtype: float64 . ## Let&#39;s only keep columns with a correlation coefficient of larger than 0.4 (arbitrary, worth experimenting later!) abs_corr_coeffs[abs_corr_coeffs &gt; 0.4] . BsmtFin SF 1 0.439284 Fireplaces 0.474831 TotRms AbvGrd 0.498574 Mas Vnr Area 0.506983 Years Since Remod 0.534985 Full Bath 0.546118 Years Before Sale 0.558979 1st Flr SF 0.635185 Garage Area 0.641425 Total Bsmt SF 0.644012 Garage Cars 0.648361 Gr Liv Area 0.717596 Overall Qual 0.801206 SalePrice 1.000000 Name: SalePrice, dtype: float64 . ## Drop columns with less than 0.4 correlation with SalePrice transform_df = transform_df.drop(abs_corr_coeffs[abs_corr_coeffs &lt; 0.4].index, axis=1) . Which categorical columns should we keep? . ## Create a list of column names from documentation that are *meant* to be categorical nominal_features = [&quot;PID&quot;, &quot;MS SubClass&quot;, &quot;MS Zoning&quot;, &quot;Street&quot;, &quot;Alley&quot;, &quot;Land Contour&quot;, &quot;Lot Config&quot;, &quot;Neighborhood&quot;, &quot;Condition 1&quot;, &quot;Condition 2&quot;, &quot;Bldg Type&quot;, &quot;House Style&quot;, &quot;Roof Style&quot;, &quot;Roof Matl&quot;, &quot;Exterior 1st&quot;, &quot;Exterior 2nd&quot;, &quot;Mas Vnr Type&quot;, &quot;Foundation&quot;, &quot;Heating&quot;, &quot;Central Air&quot;, &quot;Garage Type&quot;, &quot;Misc Feature&quot;, &quot;Sale Type&quot;, &quot;Sale Condition&quot;] . Which columns are currently numerical but need to be encoded as categorical instead (because the numbers don&#39;t have any semantic meaning)? If a categorical column has hundreds of unique values (or categories), should we keep it? When we dummy code this column, hundreds of columns will need to be added back to the data frame. . ## Which categorical columns have we still carried with us? We&#39;ll test tehse transform_cat_cols = [] for col in nominal_features: if col in transform_df.columns: transform_cat_cols.append(col) ## How many unique values in each categorical column? uniqueness_counts = transform_df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values() ## Aribtrary cutoff of 10 unique values (worth experimenting) drop_nonuniq_cols = uniqueness_counts[uniqueness_counts &gt; 10].index transform_df = transform_df.drop(drop_nonuniq_cols, axis=1) . ## Select just the remaining text columns and convert to categorical text_cols = transform_df.select_dtypes(include=[&#39;object&#39;]) for col in text_cols: transform_df[col] = transform_df[col].astype(&#39;category&#39;) ## Create dummy columns and add back to the dataframe! transform_df = pd.concat([ transform_df, pd.get_dummies(transform_df.select_dtypes(include=[&#39;category&#39;])) ], axis=1) . Update select_features() . def transform_features(df): num_missing = df.isnull().sum() drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() df = df.drop(drop_missing_cols.index, axis=1) text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] df = df.fillna(replacement_values_dict) years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod df = df.drop([1702, 2180, 2181], axis=0) df = df.drop([&quot;PID&quot;, &quot;Order&quot;, &quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis=1) return df def select_features(df, coeff_threshold=0.4, uniq_threshold=10): numerical_df = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]) abs_corr_coeffs = numerical_df.corr()[&#39;SalePrice&#39;].abs().sort_values() df = df.drop(abs_corr_coeffs[abs_corr_coeffs &lt; coeff_threshold].index, axis=1) nominal_features = [&quot;PID&quot;, &quot;MS SubClass&quot;, &quot;MS Zoning&quot;, &quot;Street&quot;, &quot;Alley&quot;, &quot;Land Contour&quot;, &quot;Lot Config&quot;, &quot;Neighborhood&quot;, &quot;Condition 1&quot;, &quot;Condition 2&quot;, &quot;Bldg Type&quot;, &quot;House Style&quot;, &quot;Roof Style&quot;, &quot;Roof Matl&quot;, &quot;Exterior 1st&quot;, &quot;Exterior 2nd&quot;, &quot;Mas Vnr Type&quot;, &quot;Foundation&quot;, &quot;Heating&quot;, &quot;Central Air&quot;, &quot;Garage Type&quot;, &quot;Misc Feature&quot;, &quot;Sale Type&quot;, &quot;Sale Condition&quot;] transform_cat_cols = [] for col in nominal_features: if col in df.columns: transform_cat_cols.append(col) uniqueness_counts = df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values() drop_nonuniq_cols = uniqueness_counts[uniqueness_counts &gt; 10].index df = df.drop(drop_nonuniq_cols, axis=1) text_cols = df.select_dtypes(include=[&#39;object&#39;]) for col in text_cols: df[col] = df[col].astype(&#39;category&#39;) df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=[&#39;category&#39;]))], axis=1) return df def train_and_test(df, k=0): numeric_df = df.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) features = numeric_df.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() if k == 0: train = df[:1460] test = df[1460:] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse if k == 1: # Randomize *all* rows (frac=1) from `df` and return shuffled_df = df.sample(frac=1, ) train = df[:1460] test = df[1460:] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions_one = lr.predict(test[features]) mse_one = mean_squared_error(test[&quot;SalePrice&quot;], predictions_one) rmse_one = np.sqrt(mse_one) lr.fit(test[features], test[&quot;SalePrice&quot;]) predictions_two = lr.predict(train[features]) mse_two = mean_squared_error(train[&quot;SalePrice&quot;], predictions_two) rmse_two = np.sqrt(mse_two) avg_rmse = np.mean([rmse_one, rmse_two]) print(rmse_one) print(rmse_two) return avg_rmse else: kf = KFold(n_splits=k, shuffle=True) rmse_values = [] for train_index, test_index, in kf.split(df): train = df.iloc[train_index] test = df.iloc[test_index] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) rmse_values.append(rmse) print(rmse_values) avg_rmse = np.mean(rmse_values) return avg_rmse df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df, k=4) rmse . [25761.875549560471, 36527.812968130842, 24956.485193881424, 28486.738135675929] . 28933.227961812168 .",
            "url": "https://phucnsp.github.io/blog/jupyter/2017/09/15/predict-house-price.html",
            "relUrl": "/jupyter/2017/09/15/predict-house-price.html",
            "date": " ‚Ä¢ Sep 15, 2017"
        }
        
    
  
    
        ,"post6": {
            "title": "Kaggle competition - titanic machine learning from disaster",
            "content": "import pandas as pd train = pd.read_csv(&quot;train.csv&quot;) holdout = pd.read_csv(&quot;test.csv&quot;) print(holdout.head()) . PassengerId Pclass Name Sex 0 892 3 Kelly, Mr. James male 1 893 3 Wilkes, Mrs. James (Ellen Needs) female 2 894 2 Myles, Mr. Thomas Francis male 3 895 3 Wirz, Mr. Albert male 4 896 3 Hirvonen, Mrs. Alexander (Helga E Lindqvist) female Age SibSp Parch Ticket Fare Cabin Embarked 0 34.5 0 0 330911 7.8292 NaN Q 1 47.0 1 0 363272 7.0000 NaN S 2 62.0 0 0 240276 9.6875 NaN Q 3 27.0 0 0 315154 8.6625 NaN S 4 22.0 1 1 3101298 12.2875 NaN S . # %load functions.py def process_missing(df): &quot;&quot;&quot;Handle various missing values from the data set Usage holdout = process_missing(holdout) &quot;&quot;&quot; df[&quot;Fare&quot;] = df[&quot;Fare&quot;].fillna(train[&quot;Fare&quot;].mean()) df[&quot;Embarked&quot;] = df[&quot;Embarked&quot;].fillna(&quot;S&quot;) return df def process_age(df): &quot;&quot;&quot;Process the Age column into pre-defined &#39;bins&#39; Usage train = process_age(train) &quot;&quot;&quot; df[&quot;Age&quot;] = df[&quot;Age&quot;].fillna(-0.5) cut_points = [-1,0,5,12,18,35,60,100] label_names = [&quot;Missing&quot;,&quot;Infant&quot;,&quot;Child&quot;,&quot;Teenager&quot;,&quot;Young Adult&quot;,&quot;Adult&quot;,&quot;Senior&quot;] df[&quot;Age_categories&quot;] = pd.cut(df[&quot;Age&quot;],cut_points,labels=label_names) return df def process_fare(df): &quot;&quot;&quot;Process the Fare column into pre-defined &#39;bins&#39; Usage train = process_fare(train) &quot;&quot;&quot; cut_points = [-1,12,50,100,1000] label_names = [&quot;0-12&quot;,&quot;12-50&quot;,&quot;50-100&quot;,&quot;100+&quot;] df[&quot;Fare_categories&quot;] = pd.cut(df[&quot;Fare&quot;],cut_points,labels=label_names) return df def process_cabin(df): &quot;&quot;&quot;Process the Cabin column into pre-defined &#39;bins&#39; Usage train process_cabin(train) &quot;&quot;&quot; df[&quot;Cabin_type&quot;] = df[&quot;Cabin&quot;].str[0] df[&quot;Cabin_type&quot;] = df[&quot;Cabin_type&quot;].fillna(&quot;Unknown&quot;) df = df.drop(&#39;Cabin&#39;,axis=1) return df def process_titles(df): &quot;&quot;&quot;Extract and categorize the title from the name column Usage train = process_titles(train) &quot;&quot;&quot; titles = { &quot;Mr&quot; : &quot;Mr&quot;, &quot;Mme&quot;: &quot;Mrs&quot;, &quot;Ms&quot;: &quot;Mrs&quot;, &quot;Mrs&quot; : &quot;Mrs&quot;, &quot;Master&quot; : &quot;Master&quot;, &quot;Mlle&quot;: &quot;Miss&quot;, &quot;Miss&quot; : &quot;Miss&quot;, &quot;Capt&quot;: &quot;Officer&quot;, &quot;Col&quot;: &quot;Officer&quot;, &quot;Major&quot;: &quot;Officer&quot;, &quot;Dr&quot;: &quot;Officer&quot;, &quot;Rev&quot;: &quot;Officer&quot;, &quot;Jonkheer&quot;: &quot;Royalty&quot;, &quot;Don&quot;: &quot;Royalty&quot;, &quot;Sir&quot; : &quot;Royalty&quot;, &quot;Countess&quot;: &quot;Royalty&quot;, &quot;Dona&quot;: &quot;Royalty&quot;, &quot;Lady&quot; : &quot;Royalty&quot; } extracted_titles = df[&quot;Name&quot;].str.extract(&#39; ([A-Za-z]+) .&#39;,expand=False) df[&quot;Title&quot;] = extracted_titles.map(titles) return df def create_dummies(df,column_name): &quot;&quot;&quot;Create Dummy Columns (One Hot Encoding) from a single Column Usage train = create_dummies(train,&quot;Age&quot;) &quot;&quot;&quot; dummies = pd.get_dummies(df[column_name],prefix=column_name) df = pd.concat([df,dummies],axis=1) return df . #preprocess the data def pre_process(df): df = process_missing(df) df = process_age(df) df = process_fare(df) df = process_titles(df) df = process_cabin(df) for col in [&quot;Age_categories&quot;,&quot;Fare_categories&quot;, &quot;Title&quot;,&quot;Cabin_type&quot;,&quot;Sex&quot;]: df = create_dummies(df,col) return df train = pre_process(train) holdout = pre_process(holdout) . Data exploration . #Inspect data type of column explore_cols = [&quot;SibSp&quot;,&quot;Parch&quot;,&quot;Survived&quot;] explore = train[explore_cols].copy() explore.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 3 columns): SibSp 891 non-null int64 Parch 891 non-null int64 Survived 891 non-null int64 dtypes: int64(3) memory usage: 21.0 KB . # Histogram to view the distribution of 2 columns: SibSp and Parch import matplotlib.pyplot as plt %matplotlib inline explore.drop(&quot;Survived&quot;,axis=1).plot.hist(alpha=0.5,bins=8) plt.xticks(range(11)) plt.show() . explore[&quot;familysize&quot;] = explore[[&quot;SibSp&quot;,&quot;Parch&quot;]].sum(axis=1) explore.drop(&quot;Survived&quot;,axis=1).plot.hist(alpha=0.5,bins=10) plt.xticks(range(11)) plt.show() . # Use pivot tables to look at the survival rate for different values of the columns import numpy as np for col in explore.columns.drop(&quot;Survived&quot;): pivot = explore.pivot_table(index=col,values=&quot;Survived&quot;) pivot.plot.bar(ylim=(0,1),yticks=np.arange(0,1,.1)) plt.axhspan(.3, .6, alpha=0.2, color=&#39;red&#39;) plt.show() . The SibSp column shows the number of siblings and/or spouses each passenger had on board, while the Parch columns shows the number of parents or children each passenger had onboard. Neither column has any missing values. . The distribution of values in both columns is skewed right, with the majority of values being zero. . You can sum these two columns to explore the total number of family members each passenger had onboard. The shape of the distribution of values in this case is similar, however there are less values at zero, and the quantity tapers off less rapidly as the values increase. . Looking at the survival rates of the the combined family members, you can see that few of the over 500 passengers with no family members survived, while greater numbers of passengers with family members survived. . Engineering new features . # Based on the observation about few surviver with no family group, let&#39;s create a binary value column where 1 is with # family and 0 is without family def feature_alone(df): df[&quot;familysize&quot;] = df[[&quot;SibSp&quot;,&quot;Parch&quot;]].sum(axis=1) df[&quot;isalone&quot;] = 0 df.loc[(df[&quot;familysize&quot;] == 0),&quot;isalone&quot;] = 1 df.drop(&quot;familysize&quot;, axis = 1) return df train = feature_alone(train) holdout = feature_alone(holdout) . Feature selection/preparation . # Select the best-performing features from sklearn.ensemble import RandomForestClassifier from sklearn.feature_selection import RFECV def select_features(df): # Remove non-numeric columns, columns that have null values df = df.select_dtypes([np.number]).dropna(axis=1) all_X = df.drop([&quot;Survived&quot;,&quot;PassengerId&quot;],axis=1) all_y = df[&quot;Survived&quot;] clf = RandomForestClassifier(random_state=1) selector = RFECV(clf,cv=10) selector.fit(all_X,all_y) best_columns = list(all_X.columns[selector.support_]) print(&quot;Best Columns n&quot;+&quot;-&quot;*12+&quot; n{}&quot;.format(best_columns)) return best_columns cols = select_features(train) . Best Columns [&#39;Pclass&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Age_categories_Adult&#39;, &#39;Age_categories_Infant&#39;, &#39;Age_categories_Missing&#39;, &#39;Age_categories_Senior&#39;, &#39;Age_categories_Teenager&#39;, &#39;Age_categories_Young Adult&#39;, &#39;Fare_categories_0-12&#39;, &#39;Fare_categories_100+&#39;, &#39;Fare_categories_12-50&#39;, &#39;Fare_categories_50-100&#39;, &#39;Title_Master&#39;, &#39;Title_Miss&#39;, &#39;Title_Mr&#39;, &#39;Title_Mrs&#39;, &#39;Title_Officer&#39;, &#39;Cabin_type_C&#39;, &#39;Cabin_type_D&#39;, &#39;Cabin_type_E&#39;, &#39;Cabin_type_Unknown&#39;, &#39;Sex_female&#39;, &#39;Sex_male&#39;, &#39;familysize&#39;, &#39;isalone&#39;] . Model selection/Tuning . # Write a function to train 3 different models. # Using grid search to train using different combinations of hyperparameters to find best performing models. from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import GridSearchCV def select_model(df,features): all_X = df[features] all_y = df[&quot;Survived&quot;] # List of dictionaries, each containing a model name, # it&#39;s estimator and a dict of hyperparameters models = [ { &quot;name&quot;: &quot;LogisticRegression&quot;, &quot;estimator&quot;: LogisticRegression(), &quot;hyperparameters&quot;: { &quot;solver&quot;: [&quot;newton-cg&quot;, &quot;lbfgs&quot;, &quot;liblinear&quot;] } }, { &quot;name&quot;: &quot;KNeighborsClassifier&quot;, &quot;estimator&quot;: KNeighborsClassifier(), &quot;hyperparameters&quot;: { &quot;n_neighbors&quot;: range(1,20,2), &quot;weights&quot;: [&quot;distance&quot;, &quot;uniform&quot;], &quot;algorithm&quot;: [&quot;ball_tree&quot;, &quot;kd_tree&quot;, &quot;brute&quot;], &quot;p&quot;: [1,2] } }, { &quot;name&quot;: &quot;RandomForestClassifier&quot;, &quot;estimator&quot;: RandomForestClassifier(random_state=1), &quot;hyperparameters&quot;: { &quot;n_estimators&quot;: [4, 6, 9], &quot;criterion&quot;: [&quot;entropy&quot;, &quot;gini&quot;], &quot;max_depth&quot;: [2, 5, 10], &quot;max_features&quot;: [&quot;log2&quot;, &quot;sqrt&quot;], &quot;min_samples_leaf&quot;: [1, 5, 8], &quot;min_samples_split&quot;: [2, 3, 5] } } ] for model in models: print(model[&#39;name&#39;]) print(&#39;-&#39;*len(model[&#39;name&#39;])) grid = GridSearchCV(model[&quot;estimator&quot;], param_grid=model[&quot;hyperparameters&quot;], cv=10) grid.fit(all_X,all_y) model[&quot;best_params&quot;] = grid.best_params_ model[&quot;best_score&quot;] = grid.best_score_ model[&quot;best_model&quot;] = grid.best_estimator_ print(&quot;Best Score: {}&quot;.format(model[&quot;best_score&quot;])) print(&quot;Best Parameters: {} n&quot;.format(model[&quot;best_params&quot;])) return models result = select_model(train,cols) . LogisticRegression Best Score: 0.8226711560044894 Best Parameters: {&#39;solver&#39;: &#39;liblinear&#39;} KNeighborsClassifier -- Best Score: 0.7833894500561167 Best Parameters: {&#39;algorithm&#39;: &#39;kd_tree&#39;, &#39;n_neighbors&#39;: 3, &#39;p&#39;: 1, &#39;weights&#39;: &#39;uniform&#39;} RandomForestClassifier - Best Score: 0.8451178451178452 Best Parameters: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 10, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 3, &#39;n_estimators&#39;: 9} . Submit to Kaggle . def save_submission_file(model,cols,filename=&quot;submission.csv&quot;): holdout_data = holdout[cols] predictions = model.predict(holdout_data) holdout_ids = holdout[&quot;PassengerId&quot;] submission_df = {&quot;PassengerId&quot;: holdout_ids, &quot;Survived&quot;: predictions} submission = pd.DataFrame(submission_df) submission.to_csv(filename,index=False) best_rf_model = result[2][&quot;best_model&quot;] save_submission_file(best_rf_model,cols) .",
            "url": "https://phucnsp.github.io/blog/jupyter/2017/08/20/kaggle-titanic-machine-learning-from-disaster.html",
            "relUrl": "/jupyter/2017/08/20/kaggle-titanic-machine-learning-from-disaster.html",
            "date": " ‚Ä¢ Aug 20, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I have been working as Data Scientist at MTI Technology Vietnam since 2018 and my journey in AI field started since 2017. In here, I mainly work with OCR (optical charcter recognition) projects where we not only have to extract texts from documents but also classify it into some specific fields defined by clients. .",
          "url": "https://phucnsp.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}