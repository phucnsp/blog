{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://phucnsp.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Tutorial markdown with fastpage",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://phucnsp.github.io/blog/tutorial/2020/01/14/tutorial-markdown-and-fastpage.html",
            "relUrl": "/tutorial/2020/01/14/tutorial-markdown-and-fastpage.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://phucnsp.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Singapore, quan sát và suy ngẫm",
            "content": "Đợt tháng mười vừa rồi tôi có dịp đi dạo quanh anh hàng xóm Singapore. Cũng hơn hai năm rồi tôi mới có dịp đi ra khỏi Vietnam, tôi thích cái cảm giác được bay, được transit ở một sân bay nào đó, ngồi ngắm những người xa lạ rảo bước qua lại, cô đơn nhưng thú vị. Singapore nổi tiếng bởi sự hiện đại, bởi những tòa nhà chọc trời, bởi Universal Studio…nhưng đối với tôi thì những cái đó chả có ý nghĩa gì cả. Khi bạn đã từng ngồi cafe ở đại lộ danh vọng Hollywood, đã đi loanh quanh ở quảng trường thời đại NewYork, đã xem biểu tình của người dân Syria trước nhà trắng Mỹ thì liệu những tòa nhà chọc trời ở Singapore có gì để hấp dẫn. Tôi đi Singapore chơi đơn giản vì tôi muốn dẫn vợ đi nước ngoài cho biết và cũng vì tôi cần đi du lịch sau một khoảng thời gian rất rất lâu cắm đầu vô công việc. . Thật may thay, Singapore khác những nơi tôi từng qua và còn làm tôi mất mấy tiếng để ngồi viết cái note này. . Chiều hôm qua tôi trở về Tân Sân Nhất sau hơn 1 tiếng 30 phút bay, đơn giản là cảm giác xót xa. Lạ nhỉ, lần trước bay ở Berlin về chả thấy gì, lần này tự nhiên lại xót xa. Tại người ta ở cái tầm cao hơn mình quá, tôi khen Singapore mà tôi xót cho Saigon, tôi nhìn anh xe ôm, nhìn những chị tay xách nách mang, nhìn cái cách người dân quê tôi băng qua đường chễnh chệ bất chấp đèn tín hiệu, tiếng la hét chèo kéo khách, tiếng bóp còi inh ỏi, xa xa là anh công an ngồi lướt điện thoại ở một xó cạnh lối ra sân bay, tất cả đều làm tôi xót xa. Hôm thứ năm trước khi bay đi chơi còn thấy bình thường mà nhỉ! . Changi đón chào vợ chồng tôi bằng cái thác nước trong nhà cao nhất thế giới, đó là cái woww đầu tiên khi tàu điện kết nối giữa những terminal đưa chúng tôi đi qua cái thác nước nhân tạo này, ngay bên trong sân bay. Chúng tôi về đến hostel trời cũng đã tối, hơn 8pm thì phải, nên cả 2 đều đi ngủ sớm để hôm sau có sức mà đi chơi. . . Ba ngày vi vu ở thành phố này là ba ngày làm tôi suy ngẫm rất nhiều, cái giả định về một thành phố hiện đại với những tòa nhà chọc trời chán phèo dần dần mất đi khi tôi lần lượt đi qua những tụ điểm nổi tiếng ở đây, chứng kiến cái cách họ nâng tầm công nghệ lên thành nghê thuật đã làm tôi thay đổi góc nhìn của mình. Xây cái nhà cao chót vót lên thì ở thành phố lớn nào cũng có nhưng để khoa học và nghệ thuật thật sự gặp nhau thì không phải nơi nào cũng làm được và cũng không phải người dân ở đâu cũng cảm thụ được. Sáng sớm hôm thứ bảy, trong lúc dạo bộ và ngồi nghỉ ngơi ở trạm xe bus gần khu Chinatown, tôi đọc được một bài chia sẻ của anh country manager bên Knorex, từng học tập và làm việc ở Singapore sau đó về quản lý branch ở Ho Chi Minh. Góc nhìn của anh ấy trong công việc thật ra không mới đối với tôi nhưng đơn giản là nó được đọc đúng lúc, đúng thời điểm. Đó là góc nhìn về việc học suốt đời, học ở bất kì hoàn cảnh nào mà không phải chờ có thầy dạy mới học được, về growth mindset, về thái độ làm việc cho đi trước để nhận lại sau, đặc biệt là về sự toàn cầu hóa nhân lực của các công ty, tập đoàn, đồng nghiệp của bạn ngày nay không còn là những anh em bạn dì nữa mà có thể đến từ bất kì đâu trên thế giới này, Ấn độ, Brazile, Chile, Sillicon Valley, etc. Bài chia sẻ ấy như là chất xúc tác cho chuyến đi của tôi, tôi bắt đầu để ý hơn tới con người nơi đây, từng chi tiết nhỏ trên đường, cách họ đào đường lên và lấp lại cẩn thận mà không để nhấp nhô, cách họ giữ gìn sạch sẽ không chỉ phía trước mà cả phía sau của nhà hàng cho đến cách họ quy hoạch bố trí nhà cửa, bố trí mảng xanh khắp nơi… tôi đã cảm thấy thật khó khăn trong việc tìm ra chỗ để bĩu môi chê cười. Mọi thứ được hoàn thiện một cách tuyệt vời ở tầm quốc gia, Singapore có lẽ là ví dụ điển hình nhất cho khái niệm quản lý “tự do trong khuôn khổ”, người dân có không gian để thể hiện cái riêng nhưng phải trong khuôn khổ nhất định để giữ gìn cái chung. . Trưa thứ bảy vợ chồng tôi đi dạo bộ ở khu Marina Bay thì tình cờ được dự giờ một buổi tập hát của mấy em tiểu học. Tụi nhỏ biểu diễn về nhạc kịch hay gì đó đại loại thế, nghe khá xa xỉ đối với đại đa số dân Việt Nam mình. Thật buồn cười khi tôi khen con nít ở Singapore nói tiếng anh hay quá, hát tự tin quá. Nhưng thật sự chúng đã được thừa hưởng di sản phi vật thể quá lớn từ ông Lý, tiếng anh vs tiếng trung, để giờ đây có thể tiếp cận dễ dàng hơn với tinh hoa thế giới, lại thêm cái kiểu giáo dục khai sáng, tự do thể hiện cái tôi cá nhân thế này nữa thì hỡi ơi, mấy đứa cháu ở quê đang đung đưa võng nghe thần tượng Hàn Quốc hát cả ngày hay đang tụ tập quán trà sữa để chém gió, thì rồi cơ may nào để chúng cạnh tranh trong cái thế giới toàn cầu hóa đây. Có thể nhiều người sẽ nghĩ làm gì tới nỗi, vẫn có rất nhiều nhân tài người Việt học trường làng những vẫn nổi danh thế giới đó thôi, nhưng khi đánh giá cái tầm quốc gia thì người ta không nói câu chuyện của một vài người xuất chúng, người ta nói đến sức mạnh của passport index (sức mạnh tấm hộ chiếu Việt Nam hình như đứng gần áp chót bảng xếp hạng), người ta đánh giá thành tích toàn đoàn, có đấy những ngôi sao vàng lẻ loi đoạt huy chương tầm thế giới nhưng có mấy bài báo đăng thành tích của toàn đoàn không! Và theo trải nghiệm cá nhân của tôi, bây giờ khi bạn hỏi một người nước ngoài biết gì về Việt Nam thì câu trả lời khó mà ngoài chiến tranh và gia công quần áo. . À nói một tí về khái niệm “Du lịch, quan sát và suy ngẫm” mà tôi đã từng đọc đâu đó, nó mang tới cho bạn một góc nhìn “thấm” hơn về nơi mình đã đi qua. Du lịch không dừng lại ở những tấm hình selffie khoe trên facebook mà còn cả ở cảm thụ cá nhân. Nhưng cũng có lẽ sự cảm thụ này không dễ mà có được, sau một thời gian dài liên tục học tập, đọc sách, tích lũy kiến thức thì mới may ra chấp chớm cảm nhận được điều này. Đó là lý do vì sao mà chúng ta rất hay thấy du khách nước ngoài tới Việt Nam du lịch thường tới bảo tàng, thường mua sách về Việt Nam để đọc, đơn giản vì họ đang cảm thụ một cách sâu hơn văn hóa, con người, đất nước chúng ta, chứ không chỉ dừng lại bề nổi ở những bức ảnh. Ngày nay tất nhiên không cần phải tới bảo tàng thì chúng ta mới biết về một nơi nào đó, mọi thứ có thể được đọc dễ dàng qua internet nhưng thật sự cảm giác của việc đi du lịch, cảm nhận bằng chính tất cả giác quan để rồi có những suy ngẫm sâu sắc hơn về sự vật sự việc xung quanh mang lại những lợi ích to lớn. Nó là cơn mưa mùa hè cho những bộ não đã bị lu mờ bởi những thứ lặp đi lặp lại hằng ngày, thậm chí nó có thể thay đổi hoàn toàn góc nhìn của mình từ trước tới nay về một việc nào đó. . Quay trở lại câu chuyện xót xa Singapore, có lẽ một dịp nào đó tôi sẽ trở lại đất nước này để xem cảm giác còn như xưa không. Như một lời nhắn nhủ cho bản thân, toàn cầu hóa là có thật và ở ngay đít rồi. Quên đi những xích mích, những câu chuyện vặt ở xung quanh, trong lúc mình ngồi nhiều chuyện thì thằng khác ở đâu đó vẫn đang miệt mài tu luyện. Kỷ luật hơn, học tập ở mọi nơi, học ở bất kì ai, đừng chờ có thầy dạy thì mới học được, đừng sợ học nhiều quá sẽ làm não mình bớt thông minh. Học kiến thức để tăng trí thông mình IQ, học văn hóa nghệ thuật, học cách ửng xử để tăng trí thông minh EQ. Làm việc với một thái độ sẵn sàng hy sinh, thân tôi đây nè, vứt việc cho tôi đi. Dù biết rằng một cánh én nhỏ khó làm nên mùa xuân, tôi có trở thành công dân toàn cầu cũng chưa chắc Việt Nam sẽ tốt hơn nhưng ít nhất tôi không muốn là người góp phần cho đất nước mà tôi yêu quý trở nên tệ hơn! . Tối qua về đến nhà con bạn người Malay đang sống ở Singapore, thấy hình đăng facebook, mới nhắn rủ cafe, cũng hơi tiếc không gặp được. Nó khoe mới có thằng bồ người Ấn, tụi nó vừa đi đám cưới bạn ở đảo Galapagos bên Ecuador về và rủ năm sau ra Hà Nội xem Vietnam F1 Race. . Chắc năm sau ra Hà Nội, quan sát và ngẫm tiếp nhỉ… .",
            "url": "https://phucnsp.github.io/blog/travel/2019/10/07/Singpore-trip-and-remaining.html",
            "relUrl": "/travel/2019/10/07/Singpore-trip-and-remaining.html",
            "date": " • Oct 7, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Data Science workflow recommendation",
            "content": "Repository of this workflow is stored here . Production data science template . The template of this repository follows production-data-science workflow, which focuses on productionizing data scientist’s work, make the analysis or research to be reusable, applicable to production. The workflow is separated into 2 phases: . exploration phase is where data scientist explores the project, mainly work with jupyter notebook. All the work in this phase will be stored in exploration folder. | production phase is where data scientists’ works are refactored into packages so it can be reuse, imported. All the work in this phase will be stored in your_package folder. | . How to setup a new repository - for maintainer . git clone https://gitlab.com/Phuc_Su/production_data_science_template.git git clone &lt;your_project_repository&gt; cd &lt;your_project_name&gt; git checkout -b product-initial-setup # open Finder, copy all content of production_data_science_template into your project repository, except .git and .idea folder conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; pip install git-lfs # in case you want to add some large file extension other than .jpg, .pdf, .csv, .xlsx git lfs track &lt;add large file path&gt; # rename &lt;your package&gt; folder and modify setup.py, most importance is require_packages. See example below # write something about your project in README.md pip install -e . pip freeze | grep -v &lt;package_name&gt; &gt; requirements.txt git add . git commit -m &quot;First commit&quot; git push -u origin HEAD . Example of setup.py . setup( name=&#39;your_project&#39;, version=&#39;v0.1&#39;, description=&#39;&#39;, long_description=readme(), classifiers=[ &#39;Programming Language :: Python :: 3&#39;, ], url=&#39;https://github.com/phucnsp/production_data_science_template&#39;, author=&#39;Phuc_Su&#39;, author_email=&#39;&#39;, license=&#39;&#39;, packages=[&#39;your_package&#39;], install_requires=[ &#39;pypandoc&gt;=1.4&#39;, &#39;watermark&gt;=1.5.0&#39;, &#39;pandas&gt;=0.20.3&#39;, &#39;scikit-learn&gt;=0.19.0&#39;, &#39;scipy&gt;=0.19.1&#39;, &#39;matplotlib&gt;=2.1.0&#39;, &#39;pytest&gt;=3.2.3&#39;, &#39;pytest-runner&gt;=2.12.1&#39;, &#39;click&gt;=6.7&#39; ], setup_requires=[&#39;pytest-runner&#39;], tests_require=[&#39;pytest&#39;], ) . and you are ready~! 🎉 . Note: if you want to setup notification on slack for merge request from gitlab, reference here . How to contribute - for developers . Setup first time . bash conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; git clone &lt;repository url&gt; cd to/the/project/directory pip install -r requirements.txt pip install -e . . For a private repository accessible only through an SSH authentication, substitute https://github.com/ with git@github.com:. . Returning to work . Some rules: 1 branch/1 exploration/1 folder | branch-name convention: explore-* for exploration, refactor-* for refactor | . | . git checkout master git pull --all # if you continue to work on old branch git checkout &lt;branch&gt; # if you want to start a new exploration git checkout -b &lt;new_branch&gt; # if your branch is far behind master and you want to merge git merge master ##################### Start working ##################### git add &lt;path_to_work_files/folder&gt; git commit -m &quot;some message&quot; git push -u origin HEAD . Notes . requirements.txt helps to setup your virtual environment, to make sure all contributors working on the same environments. So whenever you have a new libraries need to install, after installing you need to add it into requirements.txt by pip freeze | grep -v &lt;package_name&gt; &gt; requirements.txt | setup.py allows you to create packages that you can redistribute. This script is meant to install your package on the end user’s system, not to prepare the development environment. packages - in-house development packages. | install_requires - packages that our development packages dependence on. | py_modules=[&#39;new_module&#39;] - in-house development modules need to install (placed in root directory) | . | pip install -e . - to install packages/modules from setup.py, in the editable mode. | If you want to add large file into working repository: pip install git-lfs git lfs install # Tell LFS to track files with given path git lfs track &quot;path_to_large_file&quot; # Tell LFS to track files with format &quot;*.jpg&quot; git lfs track &quot;*.jpg&quot; # Tell LFS to track content of the whole directory git lfs track &quot;data/*&quot; . | . How to use the package - for users . Install the library . conda create --name &lt;environment_name&gt; python=3.6 source activate &lt;environment_name&gt; pip install -e &#39;git+https://github.com/phucnsp/production_data_science_template.git&#39; . For a private repository accessible only through an SSH authentication, substitute git+https://github.com with git+ssh://git@github.com. Note that -e argument above to make the installation editable. . Leisure read . Production Data Science tutorial | Writing a setup script | Minimum structure | gitlab slack notification service | git strategy | .",
            "url": "https://phucnsp.github.io/blog/tutorial/2019/05/10/data-science-template.html",
            "relUrl": "/tutorial/2019/05/10/data-science-template.html",
            "date": " • May 10, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "Predict house price in America",
            "content": "Introduction . import pandas as pd pd.options.display.max_columns = 999 import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import KFold from sklearn.metrics import mean_squared_error from sklearn import linear_model from sklearn.model_selection import KFold . df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) . def transform_features(df): return df def select_features(df): return df[[&quot;Gr Liv Area&quot;, &quot;SalePrice&quot;]] def train_and_test(df): train = df[:1460] test = df[1460:] ## You can use `pd.DataFrame.select_dtypes()` to specify column types ## and return only those columns as a data frame. numeric_train = train.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) numeric_test = test.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) ## You can use `pd.Series.drop()` to drop a value. features = numeric_train.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df) rmse . 57088.251612639091 . Feature Engineering . Handle missing values: All columns: Drop any with 5% or more missing values for now. Text columns: Drop any with 1 or more missing values for now. Numerical columns: For columns with missing values, fill in with the most common value in that column . 1: All columns: Drop any with 5% or more missing values for now. . ## Series object: column name -&gt; number of missing values num_missing = df.isnull().sum() . # Filter Series to columns containing &gt;5% missing values drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() # Drop those columns from the data frame. Note the use of the .index accessor df = df.drop(drop_missing_cols.index, axis=1) . ## Series object: column name -&gt; number of missing values text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) ## Filter Series to columns containing *any* missing values drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) . ## Compute column-wise missing value counts num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() fixable_numeric_cols . BsmtFin SF 1 1 BsmtFin SF 2 1 Bsmt Unf SF 1 Total Bsmt SF 1 Garage Cars 1 Garage Area 1 Bsmt Full Bath 2 Bsmt Half Bath 2 Mas Vnr Area 23 dtype: int64 . ## Compute the most common value for each column in `fixable_nmeric_missing_cols`. replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] replacement_values_dict . {&#39;Bsmt Full Bath&#39;: 0.0, &#39;Bsmt Half Bath&#39;: 0.0, &#39;Bsmt Unf SF&#39;: 0.0, &#39;BsmtFin SF 1&#39;: 0.0, &#39;BsmtFin SF 2&#39;: 0.0, &#39;Garage Area&#39;: 0.0, &#39;Garage Cars&#39;: 2.0, &#39;Mas Vnr Area&#39;: 0.0, &#39;Total Bsmt SF&#39;: 0.0} . ## Use `pd.DataFrame.fillna()` to replace missing values. df = df.fillna(replacement_values_dict) . ## Verify that every column has 0 missing values df.isnull().sum().value_counts() . 0 64 dtype: int64 . years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_sold[years_sold &lt; 0] . 2180 -1 dtype: int64 . years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] years_since_remod[years_since_remod &lt; 0] . 1702 -1 2180 -2 2181 -1 dtype: int64 . ## Create new columns df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod ## Drop rows with negative values for both of these new features df = df.drop([1702, 2180, 2181], axis=0) ## No longer need original year columns df = df.drop([&quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis = 1) . Drop columns that: a. that aren&#39;t useful for ML b. leak data about the final sale . ## Drop columns that aren&#39;t useful for ML df = df.drop([&quot;PID&quot;, &quot;Order&quot;], axis=1) ## Drop columns that leak info about the final sale df = df.drop([&quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Yr Sold&quot;], axis=1) . Let&#39;s update transform_features() . def transform_features(df): num_missing = df.isnull().sum() drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() df = df.drop(drop_missing_cols.index, axis=1) text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] df = df.fillna(replacement_values_dict) years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod df = df.drop([1702, 2180, 2181], axis=0) df = df.drop([&quot;PID&quot;, &quot;Order&quot;, &quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis=1) return df def select_features(df): return df[[&quot;Gr Liv Area&quot;, &quot;SalePrice&quot;]] def train_and_test(df): train = df[:1460] test = df[1460:] ## You can use `pd.DataFrame.select_dtypes()` to specify column types ## and return only those columns as a data frame. numeric_train = train.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) numeric_test = test.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) ## You can use `pd.Series.drop()` to drop a value. features = numeric_train.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df) rmse . 55275.367312413066 . Feature Selection . numerical_df = transform_df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]) numerical_df . MS SubClass Lot Area Overall Qual Overall Cond Mas Vnr Area BsmtFin SF 1 BsmtFin SF 2 Bsmt Unf SF Total Bsmt SF 1st Flr SF 2nd Flr SF Low Qual Fin SF Gr Liv Area Bsmt Full Bath Bsmt Half Bath Full Bath Half Bath Bedroom AbvGr Kitchen AbvGr TotRms AbvGrd Fireplaces Garage Cars Garage Area Wood Deck SF Open Porch SF Enclosed Porch 3Ssn Porch Screen Porch Pool Area Misc Val Yr Sold SalePrice Years Before Sale Years Since Remod . 0 20 | 31770 | 6 | 5 | 112.0 | 639.0 | 0.0 | 441.0 | 1080.0 | 1656 | 0 | 0 | 1656 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 7 | 2 | 2.0 | 528.0 | 210 | 62 | 0 | 0 | 0 | 0 | 0 | 2010 | 215000 | 50 | 50 | . 1 20 | 11622 | 5 | 6 | 0.0 | 468.0 | 144.0 | 270.0 | 882.0 | 896 | 0 | 0 | 896 | 0.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 0 | 1.0 | 730.0 | 140 | 0 | 0 | 0 | 120 | 0 | 0 | 2010 | 105000 | 49 | 49 | . 2 20 | 14267 | 6 | 6 | 108.0 | 923.0 | 0.0 | 406.0 | 1329.0 | 1329 | 0 | 0 | 1329 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 312.0 | 393 | 36 | 0 | 0 | 0 | 0 | 12500 | 2010 | 172000 | 52 | 52 | . 3 20 | 11160 | 7 | 5 | 0.0 | 1065.0 | 0.0 | 1045.0 | 2110.0 | 2110 | 0 | 0 | 2110 | 1.0 | 0.0 | 2 | 1 | 3 | 1 | 8 | 2 | 2.0 | 522.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 244000 | 42 | 42 | . 4 60 | 13830 | 5 | 5 | 0.0 | 791.0 | 0.0 | 137.0 | 928.0 | 928 | 701 | 0 | 1629 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 6 | 1 | 2.0 | 482.0 | 212 | 34 | 0 | 0 | 0 | 0 | 0 | 2010 | 189900 | 13 | 12 | . 5 60 | 9978 | 6 | 6 | 20.0 | 602.0 | 0.0 | 324.0 | 926.0 | 926 | 678 | 0 | 1604 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 470.0 | 360 | 36 | 0 | 0 | 0 | 0 | 0 | 2010 | 195500 | 12 | 12 | . 6 120 | 4920 | 8 | 5 | 0.0 | 616.0 | 0.0 | 722.0 | 1338.0 | 1338 | 0 | 0 | 1338 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 6 | 0 | 2.0 | 582.0 | 0 | 0 | 170 | 0 | 0 | 0 | 0 | 2010 | 213500 | 9 | 9 | . 7 120 | 5005 | 8 | 5 | 0.0 | 263.0 | 0.0 | 1017.0 | 1280.0 | 1280 | 0 | 0 | 1280 | 0.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 506.0 | 0 | 82 | 0 | 0 | 144 | 0 | 0 | 2010 | 191500 | 18 | 18 | . 8 120 | 5389 | 8 | 5 | 0.0 | 1180.0 | 0.0 | 415.0 | 1595.0 | 1616 | 0 | 0 | 1616 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 1 | 2.0 | 608.0 | 237 | 152 | 0 | 0 | 0 | 0 | 0 | 2010 | 236500 | 15 | 14 | . 9 60 | 7500 | 7 | 5 | 0.0 | 0.0 | 0.0 | 994.0 | 994.0 | 1028 | 776 | 0 | 1804 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 442.0 | 140 | 60 | 0 | 0 | 0 | 0 | 0 | 2010 | 189000 | 11 | 11 | . 10 60 | 10000 | 6 | 5 | 0.0 | 0.0 | 0.0 | 763.0 | 763.0 | 763 | 892 | 0 | 1655 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 440.0 | 157 | 84 | 0 | 0 | 0 | 0 | 0 | 2010 | 175900 | 17 | 16 | . 11 20 | 7980 | 6 | 7 | 0.0 | 935.0 | 0.0 | 233.0 | 1168.0 | 1187 | 0 | 0 | 1187 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 6 | 0 | 2.0 | 420.0 | 483 | 21 | 0 | 0 | 0 | 0 | 500 | 2010 | 185000 | 18 | 3 | . 12 60 | 8402 | 6 | 5 | 0.0 | 0.0 | 0.0 | 789.0 | 789.0 | 789 | 676 | 0 | 1465 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 1 | 2.0 | 393.0 | 0 | 75 | 0 | 0 | 0 | 0 | 0 | 2010 | 180400 | 12 | 12 | . 13 20 | 10176 | 7 | 5 | 0.0 | 637.0 | 0.0 | 663.0 | 1300.0 | 1341 | 0 | 0 | 1341 | 1.0 | 0.0 | 1 | 1 | 2 | 1 | 5 | 1 | 2.0 | 506.0 | 192 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 171500 | 20 | 20 | . 14 120 | 6820 | 8 | 5 | 0.0 | 368.0 | 1120.0 | 0.0 | 1488.0 | 1502 | 0 | 0 | 1502 | 1.0 | 0.0 | 1 | 1 | 1 | 1 | 4 | 0 | 2.0 | 528.0 | 0 | 54 | 0 | 0 | 140 | 0 | 0 | 2010 | 212000 | 25 | 25 | . 15 60 | 53504 | 8 | 5 | 603.0 | 1416.0 | 0.0 | 234.0 | 1650.0 | 1690 | 1589 | 0 | 3279 | 1.0 | 0.0 | 3 | 1 | 4 | 1 | 12 | 1 | 3.0 | 841.0 | 503 | 36 | 0 | 0 | 210 | 0 | 0 | 2010 | 538000 | 7 | 7 | . 16 50 | 12134 | 8 | 7 | 0.0 | 427.0 | 0.0 | 132.0 | 559.0 | 1080 | 672 | 0 | 1752 | 0.0 | 0.0 | 2 | 0 | 4 | 1 | 8 | 0 | 2.0 | 492.0 | 325 | 12 | 0 | 0 | 0 | 0 | 0 | 2010 | 164000 | 22 | 5 | . 17 20 | 11394 | 9 | 2 | 350.0 | 1445.0 | 0.0 | 411.0 | 1856.0 | 1856 | 0 | 0 | 1856 | 1.0 | 0.0 | 1 | 1 | 1 | 1 | 8 | 1 | 3.0 | 834.0 | 113 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 394432 | 0 | 0 | . 18 20 | 19138 | 4 | 5 | 0.0 | 120.0 | 0.0 | 744.0 | 864.0 | 864 | 0 | 0 | 864 | 0.0 | 0.0 | 1 | 0 | 2 | 1 | 4 | 0 | 2.0 | 400.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 141000 | 59 | 59 | . 19 20 | 13175 | 6 | 6 | 119.0 | 790.0 | 163.0 | 589.0 | 1542.0 | 2073 | 0 | 0 | 2073 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 7 | 2 | 2.0 | 500.0 | 349 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 210000 | 32 | 22 | . 20 20 | 11751 | 6 | 6 | 480.0 | 705.0 | 0.0 | 1139.0 | 1844.0 | 1844 | 0 | 0 | 1844 | 0.0 | 0.0 | 2 | 0 | 3 | 1 | 7 | 1 | 2.0 | 546.0 | 0 | 122 | 0 | 0 | 0 | 0 | 0 | 2010 | 190000 | 33 | 33 | . 21 85 | 10625 | 7 | 6 | 81.0 | 885.0 | 168.0 | 0.0 | 1053.0 | 1173 | 0 | 0 | 1173 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 6 | 2 | 2.0 | 528.0 | 0 | 120 | 0 | 0 | 0 | 0 | 0 | 2010 | 170000 | 36 | 36 | . 22 60 | 7500 | 7 | 5 | 0.0 | 533.0 | 0.0 | 281.0 | 814.0 | 814 | 860 | 0 | 1674 | 1.0 | 0.0 | 2 | 1 | 3 | 1 | 7 | 0 | 2.0 | 663.0 | 0 | 96 | 0 | 0 | 0 | 0 | 0 | 2010 | 216000 | 10 | 10 | . 23 20 | 11241 | 6 | 7 | 180.0 | 578.0 | 0.0 | 426.0 | 1004.0 | 1004 | 0 | 0 | 1004 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 1 | 2.0 | 480.0 | 0 | 0 | 0 | 0 | 0 | 0 | 700 | 2010 | 149000 | 40 | 40 | . 24 20 | 12537 | 5 | 6 | 0.0 | 734.0 | 0.0 | 344.0 | 1078.0 | 1078 | 0 | 0 | 1078 | 1.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 1 | 2.0 | 500.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 149900 | 39 | 2 | . 25 20 | 8450 | 5 | 6 | 0.0 | 775.0 | 0.0 | 281.0 | 1056.0 | 1056 | 0 | 0 | 1056 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 1 | 1.0 | 304.0 | 0 | 85 | 184 | 0 | 0 | 0 | 0 | 2010 | 142000 | 42 | 42 | . 26 20 | 8400 | 4 | 5 | 0.0 | 804.0 | 78.0 | 0.0 | 882.0 | 882 | 0 | 0 | 882 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 4 | 0 | 2.0 | 525.0 | 240 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 126000 | 40 | 40 | . 27 20 | 10500 | 4 | 5 | 0.0 | 432.0 | 0.0 | 432.0 | 864.0 | 864 | 0 | 0 | 864 | 0.0 | 0.0 | 1 | 0 | 3 | 1 | 5 | 1 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 115000 | 39 | 39 | . 28 120 | 5858 | 7 | 5 | 0.0 | 1051.0 | 0.0 | 354.0 | 1405.0 | 1337 | 0 | 0 | 1337 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 1 | 2.0 | 511.0 | 203 | 68 | 0 | 0 | 0 | 0 | 0 | 2010 | 184000 | 11 | 11 | . 29 160 | 1680 | 6 | 5 | 504.0 | 156.0 | 0.0 | 327.0 | 483.0 | 483 | 504 | 0 | 987 | 0.0 | 0.0 | 1 | 1 | 2 | 1 | 5 | 0 | 1.0 | 264.0 | 275 | 0 | 0 | 0 | 0 | 0 | 0 | 2010 | 96000 | 39 | 39 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2900 20 | 13618 | 8 | 5 | 198.0 | 1350.0 | 0.0 | 378.0 | 1728.0 | 1960 | 0 | 0 | 1960 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 8 | 2 | 3.0 | 714.0 | 172 | 38 | 0 | 0 | 0 | 0 | 0 | 2006 | 320000 | 1 | 0 | . 2901 20 | 11443 | 8 | 5 | 208.0 | 1460.0 | 0.0 | 408.0 | 1868.0 | 2028 | 0 | 0 | 2028 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 7 | 2 | 3.0 | 880.0 | 326 | 66 | 0 | 0 | 0 | 0 | 0 | 2006 | 369900 | 1 | 0 | . 2902 20 | 11577 | 9 | 5 | 382.0 | 1455.0 | 0.0 | 383.0 | 1838.0 | 1838 | 0 | 0 | 1838 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 9 | 1 | 3.0 | 682.0 | 161 | 225 | 0 | 0 | 0 | 0 | 0 | 2006 | 359900 | 1 | 0 | . 2903 20 | 31250 | 1 | 3 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1600 | 0 | 0 | 1600 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 270.0 | 0 | 0 | 135 | 0 | 0 | 0 | 0 | 2006 | 81500 | 55 | 55 | . 2904 90 | 7020 | 7 | 5 | 200.0 | 1243.0 | 0.0 | 45.0 | 1288.0 | 1368 | 0 | 0 | 1368 | 2.0 | 0.0 | 2 | 0 | 2 | 2 | 8 | 0 | 4.0 | 784.0 | 0 | 48 | 0 | 0 | 0 | 0 | 0 | 2006 | 215000 | 9 | 9 | . 2905 120 | 4500 | 6 | 5 | 116.0 | 897.0 | 0.0 | 319.0 | 1216.0 | 1216 | 0 | 0 | 1216 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 402.0 | 0 | 125 | 0 | 0 | 0 | 0 | 0 | 2006 | 164000 | 8 | 8 | . 2906 120 | 4500 | 6 | 5 | 443.0 | 1201.0 | 0.0 | 36.0 | 1237.0 | 1337 | 0 | 0 | 1337 | 1.0 | 0.0 | 2 | 0 | 2 | 1 | 5 | 0 | 2.0 | 405.0 | 0 | 199 | 0 | 0 | 0 | 0 | 0 | 2006 | 153500 | 8 | 8 | . 2907 20 | 17217 | 5 | 5 | 0.0 | 0.0 | 0.0 | 1140.0 | 1140.0 | 1140 | 0 | 0 | 1140 | 0.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 0 | 0.0 | 0.0 | 36 | 56 | 0 | 0 | 0 | 0 | 0 | 2006 | 84500 | 0 | 0 | . 2908 160 | 2665 | 5 | 6 | 0.0 | 0.0 | 0.0 | 264.0 | 264.0 | 616 | 688 | 0 | 1304 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 1 | 1.0 | 336.0 | 141 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 104500 | 29 | 29 | . 2909 160 | 2665 | 5 | 6 | 0.0 | 548.0 | 173.0 | 36.0 | 757.0 | 925 | 550 | 0 | 1475 | 0.0 | 0.0 | 2 | 0 | 4 | 1 | 6 | 1 | 1.0 | 336.0 | 104 | 26 | 0 | 0 | 0 | 0 | 0 | 2006 | 127000 | 29 | 29 | . 2910 160 | 3964 | 6 | 4 | 0.0 | 837.0 | 0.0 | 105.0 | 942.0 | 1291 | 1230 | 0 | 2521 | 1.0 | 0.0 | 2 | 1 | 5 | 1 | 10 | 1 | 2.0 | 576.0 | 728 | 20 | 0 | 0 | 0 | 0 | 0 | 2006 | 151400 | 33 | 33 | . 2911 20 | 10172 | 5 | 7 | 0.0 | 441.0 | 0.0 | 423.0 | 864.0 | 874 | 0 | 0 | 874 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 5 | 0 | 1.0 | 288.0 | 0 | 120 | 0 | 0 | 0 | 0 | 0 | 2006 | 126500 | 38 | 3 | . 2912 90 | 11836 | 5 | 5 | 0.0 | 149.0 | 0.0 | 1503.0 | 1652.0 | 1652 | 0 | 0 | 1652 | 0.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 3.0 | 928.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 146500 | 36 | 36 | . 2913 180 | 1470 | 4 | 6 | 0.0 | 522.0 | 0.0 | 108.0 | 630.0 | 630 | 0 | 0 | 630 | 1.0 | 0.0 | 1 | 0 | 1 | 1 | 3 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 73000 | 36 | 36 | . 2914 160 | 1484 | 4 | 4 | 0.0 | 252.0 | 0.0 | 294.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 1.0 | 253.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 79400 | 34 | 34 | . 2915 20 | 13384 | 5 | 5 | 194.0 | 119.0 | 344.0 | 641.0 | 1104.0 | 1360 | 0 | 0 | 1360 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 8 | 1 | 1.0 | 336.0 | 160 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 140000 | 37 | 27 | . 2916 180 | 1533 | 5 | 7 | 0.0 | 553.0 | 0.0 | 77.0 | 630.0 | 630 | 0 | 0 | 630 | 1.0 | 0.0 | 1 | 0 | 1 | 1 | 3 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 92000 | 36 | 36 | . 2917 160 | 1533 | 4 | 5 | 0.0 | 408.0 | 0.0 | 138.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 1.0 | 286.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 87550 | 36 | 36 | . 2918 160 | 1526 | 4 | 5 | 0.0 | 0.0 | 0.0 | 546.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 0.0 | 0.0 | 0 | 34 | 0 | 0 | 0 | 0 | 0 | 2006 | 79500 | 36 | 36 | . 2919 160 | 1936 | 4 | 7 | 0.0 | 0.0 | 0.0 | 546.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 5 | 0 | 0.0 | 0.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 90500 | 36 | 36 | . 2920 160 | 1894 | 4 | 5 | 0.0 | 252.0 | 0.0 | 294.0 | 546.0 | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | 6 | 0 | 1.0 | 286.0 | 0 | 24 | 0 | 0 | 0 | 0 | 0 | 2006 | 71000 | 36 | 36 | . 2921 90 | 12640 | 6 | 5 | 0.0 | 936.0 | 396.0 | 396.0 | 1728.0 | 1728 | 0 | 0 | 1728 | 0.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 2.0 | 574.0 | 40 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 150900 | 30 | 30 | . 2922 90 | 9297 | 5 | 5 | 0.0 | 1606.0 | 0.0 | 122.0 | 1728.0 | 1728 | 0 | 0 | 1728 | 2.0 | 0.0 | 2 | 0 | 4 | 2 | 8 | 0 | 2.0 | 560.0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 188000 | 30 | 30 | . 2923 20 | 17400 | 5 | 5 | 0.0 | 936.0 | 0.0 | 190.0 | 1126.0 | 1126 | 0 | 0 | 1126 | 1.0 | 0.0 | 2 | 0 | 3 | 1 | 5 | 1 | 2.0 | 484.0 | 295 | 41 | 0 | 0 | 0 | 0 | 0 | 2006 | 160000 | 29 | 29 | . 2924 20 | 20000 | 5 | 7 | 0.0 | 1224.0 | 0.0 | 0.0 | 1224.0 | 1224 | 0 | 0 | 1224 | 1.0 | 0.0 | 1 | 0 | 4 | 1 | 7 | 1 | 2.0 | 576.0 | 474 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 131000 | 46 | 10 | . 2925 80 | 7937 | 6 | 6 | 0.0 | 819.0 | 0.0 | 184.0 | 1003.0 | 1003 | 0 | 0 | 1003 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | 6 | 0 | 2.0 | 588.0 | 120 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 142500 | 22 | 22 | . 2926 20 | 8885 | 5 | 5 | 0.0 | 301.0 | 324.0 | 239.0 | 864.0 | 902 | 0 | 0 | 902 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 5 | 0 | 2.0 | 484.0 | 164 | 0 | 0 | 0 | 0 | 0 | 0 | 2006 | 131000 | 23 | 23 | . 2927 85 | 10441 | 5 | 5 | 0.0 | 337.0 | 0.0 | 575.0 | 912.0 | 970 | 0 | 0 | 970 | 0.0 | 1.0 | 1 | 0 | 3 | 1 | 6 | 0 | 0.0 | 0.0 | 80 | 32 | 0 | 0 | 0 | 0 | 700 | 2006 | 132000 | 14 | 14 | . 2928 20 | 10010 | 5 | 5 | 0.0 | 1071.0 | 123.0 | 195.0 | 1389.0 | 1389 | 0 | 0 | 1389 | 1.0 | 0.0 | 1 | 0 | 2 | 1 | 6 | 1 | 2.0 | 418.0 | 240 | 38 | 0 | 0 | 0 | 0 | 0 | 2006 | 170000 | 32 | 31 | . 2929 60 | 9627 | 7 | 5 | 94.0 | 758.0 | 0.0 | 238.0 | 996.0 | 996 | 1004 | 0 | 2000 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | 9 | 1 | 3.0 | 650.0 | 190 | 48 | 0 | 0 | 0 | 0 | 0 | 2006 | 188000 | 13 | 12 | . 2927 rows × 34 columns . abs_corr_coeffs = numerical_df.corr()[&#39;SalePrice&#39;].abs().sort_values() abs_corr_coeffs . BsmtFin SF 2 0.006127 Misc Val 0.019273 Yr Sold 0.030358 3Ssn Porch 0.032268 Bsmt Half Bath 0.035875 Low Qual Fin SF 0.037629 Pool Area 0.068438 MS SubClass 0.085128 Overall Cond 0.101540 Screen Porch 0.112280 Kitchen AbvGr 0.119760 Enclosed Porch 0.128685 Bedroom AbvGr 0.143916 Bsmt Unf SF 0.182751 Lot Area 0.267520 2nd Flr SF 0.269601 Bsmt Full Bath 0.276258 Half Bath 0.284871 Open Porch SF 0.316262 Wood Deck SF 0.328183 BsmtFin SF 1 0.439284 Fireplaces 0.474831 TotRms AbvGrd 0.498574 Mas Vnr Area 0.506983 Years Since Remod 0.534985 Full Bath 0.546118 Years Before Sale 0.558979 1st Flr SF 0.635185 Garage Area 0.641425 Total Bsmt SF 0.644012 Garage Cars 0.648361 Gr Liv Area 0.717596 Overall Qual 0.801206 SalePrice 1.000000 Name: SalePrice, dtype: float64 . ## Let&#39;s only keep columns with a correlation coefficient of larger than 0.4 (arbitrary, worth experimenting later!) abs_corr_coeffs[abs_corr_coeffs &gt; 0.4] . BsmtFin SF 1 0.439284 Fireplaces 0.474831 TotRms AbvGrd 0.498574 Mas Vnr Area 0.506983 Years Since Remod 0.534985 Full Bath 0.546118 Years Before Sale 0.558979 1st Flr SF 0.635185 Garage Area 0.641425 Total Bsmt SF 0.644012 Garage Cars 0.648361 Gr Liv Area 0.717596 Overall Qual 0.801206 SalePrice 1.000000 Name: SalePrice, dtype: float64 . ## Drop columns with less than 0.4 correlation with SalePrice transform_df = transform_df.drop(abs_corr_coeffs[abs_corr_coeffs &lt; 0.4].index, axis=1) . Which categorical columns should we keep? . ## Create a list of column names from documentation that are *meant* to be categorical nominal_features = [&quot;PID&quot;, &quot;MS SubClass&quot;, &quot;MS Zoning&quot;, &quot;Street&quot;, &quot;Alley&quot;, &quot;Land Contour&quot;, &quot;Lot Config&quot;, &quot;Neighborhood&quot;, &quot;Condition 1&quot;, &quot;Condition 2&quot;, &quot;Bldg Type&quot;, &quot;House Style&quot;, &quot;Roof Style&quot;, &quot;Roof Matl&quot;, &quot;Exterior 1st&quot;, &quot;Exterior 2nd&quot;, &quot;Mas Vnr Type&quot;, &quot;Foundation&quot;, &quot;Heating&quot;, &quot;Central Air&quot;, &quot;Garage Type&quot;, &quot;Misc Feature&quot;, &quot;Sale Type&quot;, &quot;Sale Condition&quot;] . Which columns are currently numerical but need to be encoded as categorical instead (because the numbers don&#39;t have any semantic meaning)? If a categorical column has hundreds of unique values (or categories), should we keep it? When we dummy code this column, hundreds of columns will need to be added back to the data frame. . ## Which categorical columns have we still carried with us? We&#39;ll test tehse transform_cat_cols = [] for col in nominal_features: if col in transform_df.columns: transform_cat_cols.append(col) ## How many unique values in each categorical column? uniqueness_counts = transform_df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values() ## Aribtrary cutoff of 10 unique values (worth experimenting) drop_nonuniq_cols = uniqueness_counts[uniqueness_counts &gt; 10].index transform_df = transform_df.drop(drop_nonuniq_cols, axis=1) . ## Select just the remaining text columns and convert to categorical text_cols = transform_df.select_dtypes(include=[&#39;object&#39;]) for col in text_cols: transform_df[col] = transform_df[col].astype(&#39;category&#39;) ## Create dummy columns and add back to the dataframe! transform_df = pd.concat([ transform_df, pd.get_dummies(transform_df.select_dtypes(include=[&#39;category&#39;])) ], axis=1) . Update select_features() . def transform_features(df): num_missing = df.isnull().sum() drop_missing_cols = num_missing[(num_missing &gt; len(df)/20)].sort_values() df = df.drop(drop_missing_cols.index, axis=1) text_mv_counts = df.select_dtypes(include=[&#39;object&#39;]).isnull().sum().sort_values(ascending=False) drop_missing_cols_2 = text_mv_counts[text_mv_counts &gt; 0] df = df.drop(drop_missing_cols_2.index, axis=1) num_missing = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]).isnull().sum() fixable_numeric_cols = num_missing[(num_missing &lt; len(df)/20) &amp; (num_missing &gt; 0)].sort_values() replacement_values_dict = df[fixable_numeric_cols.index].mode().to_dict(orient=&#39;records&#39;)[0] df = df.fillna(replacement_values_dict) years_sold = df[&#39;Yr Sold&#39;] - df[&#39;Year Built&#39;] years_since_remod = df[&#39;Yr Sold&#39;] - df[&#39;Year Remod/Add&#39;] df[&#39;Years Before Sale&#39;] = years_sold df[&#39;Years Since Remod&#39;] = years_since_remod df = df.drop([1702, 2180, 2181], axis=0) df = df.drop([&quot;PID&quot;, &quot;Order&quot;, &quot;Mo Sold&quot;, &quot;Sale Condition&quot;, &quot;Sale Type&quot;, &quot;Year Built&quot;, &quot;Year Remod/Add&quot;], axis=1) return df def select_features(df, coeff_threshold=0.4, uniq_threshold=10): numerical_df = df.select_dtypes(include=[&#39;int&#39;, &#39;float&#39;]) abs_corr_coeffs = numerical_df.corr()[&#39;SalePrice&#39;].abs().sort_values() df = df.drop(abs_corr_coeffs[abs_corr_coeffs &lt; coeff_threshold].index, axis=1) nominal_features = [&quot;PID&quot;, &quot;MS SubClass&quot;, &quot;MS Zoning&quot;, &quot;Street&quot;, &quot;Alley&quot;, &quot;Land Contour&quot;, &quot;Lot Config&quot;, &quot;Neighborhood&quot;, &quot;Condition 1&quot;, &quot;Condition 2&quot;, &quot;Bldg Type&quot;, &quot;House Style&quot;, &quot;Roof Style&quot;, &quot;Roof Matl&quot;, &quot;Exterior 1st&quot;, &quot;Exterior 2nd&quot;, &quot;Mas Vnr Type&quot;, &quot;Foundation&quot;, &quot;Heating&quot;, &quot;Central Air&quot;, &quot;Garage Type&quot;, &quot;Misc Feature&quot;, &quot;Sale Type&quot;, &quot;Sale Condition&quot;] transform_cat_cols = [] for col in nominal_features: if col in df.columns: transform_cat_cols.append(col) uniqueness_counts = df[transform_cat_cols].apply(lambda col: len(col.value_counts())).sort_values() drop_nonuniq_cols = uniqueness_counts[uniqueness_counts &gt; 10].index df = df.drop(drop_nonuniq_cols, axis=1) text_cols = df.select_dtypes(include=[&#39;object&#39;]) for col in text_cols: df[col] = df[col].astype(&#39;category&#39;) df = pd.concat([df, pd.get_dummies(df.select_dtypes(include=[&#39;category&#39;]))], axis=1) return df def train_and_test(df, k=0): numeric_df = df.select_dtypes(include=[&#39;integer&#39;, &#39;float&#39;]) features = numeric_df.columns.drop(&quot;SalePrice&quot;) lr = linear_model.LinearRegression() if k == 0: train = df[:1460] test = df[1460:] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) return rmse if k == 1: # Randomize *all* rows (frac=1) from `df` and return shuffled_df = df.sample(frac=1, ) train = df[:1460] test = df[1460:] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions_one = lr.predict(test[features]) mse_one = mean_squared_error(test[&quot;SalePrice&quot;], predictions_one) rmse_one = np.sqrt(mse_one) lr.fit(test[features], test[&quot;SalePrice&quot;]) predictions_two = lr.predict(train[features]) mse_two = mean_squared_error(train[&quot;SalePrice&quot;], predictions_two) rmse_two = np.sqrt(mse_two) avg_rmse = np.mean([rmse_one, rmse_two]) print(rmse_one) print(rmse_two) return avg_rmse else: kf = KFold(n_splits=k, shuffle=True) rmse_values = [] for train_index, test_index, in kf.split(df): train = df.iloc[train_index] test = df.iloc[test_index] lr.fit(train[features], train[&quot;SalePrice&quot;]) predictions = lr.predict(test[features]) mse = mean_squared_error(test[&quot;SalePrice&quot;], predictions) rmse = np.sqrt(mse) rmse_values.append(rmse) print(rmse_values) avg_rmse = np.mean(rmse_values) return avg_rmse df = pd.read_csv(&quot;AmesHousing.tsv&quot;, delimiter=&quot; t&quot;) transform_df = transform_features(df) filtered_df = select_features(transform_df) rmse = train_and_test(filtered_df, k=4) rmse . [25761.875549560471, 36527.812968130842, 24956.485193881424, 28486.738135675929] . 28933.227961812168 .",
            "url": "https://phucnsp.github.io/blog/jupyter/2017/09/15/predict-house-price.html",
            "relUrl": "/jupyter/2017/09/15/predict-house-price.html",
            "date": " • Sep 15, 2017"
        }
        
    
  
    
        ,"post6": {
            "title": "Kaggle competition - titanic machine learning from disaster",
            "content": "import pandas as pd train = pd.read_csv(&quot;train.csv&quot;) holdout = pd.read_csv(&quot;test.csv&quot;) print(holdout.head()) . PassengerId Pclass Name Sex 0 892 3 Kelly, Mr. James male 1 893 3 Wilkes, Mrs. James (Ellen Needs) female 2 894 2 Myles, Mr. Thomas Francis male 3 895 3 Wirz, Mr. Albert male 4 896 3 Hirvonen, Mrs. Alexander (Helga E Lindqvist) female Age SibSp Parch Ticket Fare Cabin Embarked 0 34.5 0 0 330911 7.8292 NaN Q 1 47.0 1 0 363272 7.0000 NaN S 2 62.0 0 0 240276 9.6875 NaN Q 3 27.0 0 0 315154 8.6625 NaN S 4 22.0 1 1 3101298 12.2875 NaN S . # %load functions.py def process_missing(df): &quot;&quot;&quot;Handle various missing values from the data set Usage holdout = process_missing(holdout) &quot;&quot;&quot; df[&quot;Fare&quot;] = df[&quot;Fare&quot;].fillna(train[&quot;Fare&quot;].mean()) df[&quot;Embarked&quot;] = df[&quot;Embarked&quot;].fillna(&quot;S&quot;) return df def process_age(df): &quot;&quot;&quot;Process the Age column into pre-defined &#39;bins&#39; Usage train = process_age(train) &quot;&quot;&quot; df[&quot;Age&quot;] = df[&quot;Age&quot;].fillna(-0.5) cut_points = [-1,0,5,12,18,35,60,100] label_names = [&quot;Missing&quot;,&quot;Infant&quot;,&quot;Child&quot;,&quot;Teenager&quot;,&quot;Young Adult&quot;,&quot;Adult&quot;,&quot;Senior&quot;] df[&quot;Age_categories&quot;] = pd.cut(df[&quot;Age&quot;],cut_points,labels=label_names) return df def process_fare(df): &quot;&quot;&quot;Process the Fare column into pre-defined &#39;bins&#39; Usage train = process_fare(train) &quot;&quot;&quot; cut_points = [-1,12,50,100,1000] label_names = [&quot;0-12&quot;,&quot;12-50&quot;,&quot;50-100&quot;,&quot;100+&quot;] df[&quot;Fare_categories&quot;] = pd.cut(df[&quot;Fare&quot;],cut_points,labels=label_names) return df def process_cabin(df): &quot;&quot;&quot;Process the Cabin column into pre-defined &#39;bins&#39; Usage train process_cabin(train) &quot;&quot;&quot; df[&quot;Cabin_type&quot;] = df[&quot;Cabin&quot;].str[0] df[&quot;Cabin_type&quot;] = df[&quot;Cabin_type&quot;].fillna(&quot;Unknown&quot;) df = df.drop(&#39;Cabin&#39;,axis=1) return df def process_titles(df): &quot;&quot;&quot;Extract and categorize the title from the name column Usage train = process_titles(train) &quot;&quot;&quot; titles = { &quot;Mr&quot; : &quot;Mr&quot;, &quot;Mme&quot;: &quot;Mrs&quot;, &quot;Ms&quot;: &quot;Mrs&quot;, &quot;Mrs&quot; : &quot;Mrs&quot;, &quot;Master&quot; : &quot;Master&quot;, &quot;Mlle&quot;: &quot;Miss&quot;, &quot;Miss&quot; : &quot;Miss&quot;, &quot;Capt&quot;: &quot;Officer&quot;, &quot;Col&quot;: &quot;Officer&quot;, &quot;Major&quot;: &quot;Officer&quot;, &quot;Dr&quot;: &quot;Officer&quot;, &quot;Rev&quot;: &quot;Officer&quot;, &quot;Jonkheer&quot;: &quot;Royalty&quot;, &quot;Don&quot;: &quot;Royalty&quot;, &quot;Sir&quot; : &quot;Royalty&quot;, &quot;Countess&quot;: &quot;Royalty&quot;, &quot;Dona&quot;: &quot;Royalty&quot;, &quot;Lady&quot; : &quot;Royalty&quot; } extracted_titles = df[&quot;Name&quot;].str.extract(&#39; ([A-Za-z]+) .&#39;,expand=False) df[&quot;Title&quot;] = extracted_titles.map(titles) return df def create_dummies(df,column_name): &quot;&quot;&quot;Create Dummy Columns (One Hot Encoding) from a single Column Usage train = create_dummies(train,&quot;Age&quot;) &quot;&quot;&quot; dummies = pd.get_dummies(df[column_name],prefix=column_name) df = pd.concat([df,dummies],axis=1) return df . #preprocess the data def pre_process(df): df = process_missing(df) df = process_age(df) df = process_fare(df) df = process_titles(df) df = process_cabin(df) for col in [&quot;Age_categories&quot;,&quot;Fare_categories&quot;, &quot;Title&quot;,&quot;Cabin_type&quot;,&quot;Sex&quot;]: df = create_dummies(df,col) return df train = pre_process(train) holdout = pre_process(holdout) . Data exploration . #Inspect data type of column explore_cols = [&quot;SibSp&quot;,&quot;Parch&quot;,&quot;Survived&quot;] explore = train[explore_cols].copy() explore.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 3 columns): SibSp 891 non-null int64 Parch 891 non-null int64 Survived 891 non-null int64 dtypes: int64(3) memory usage: 21.0 KB . # Histogram to view the distribution of 2 columns: SibSp and Parch import matplotlib.pyplot as plt %matplotlib inline explore.drop(&quot;Survived&quot;,axis=1).plot.hist(alpha=0.5,bins=8) plt.xticks(range(11)) plt.show() . explore[&quot;familysize&quot;] = explore[[&quot;SibSp&quot;,&quot;Parch&quot;]].sum(axis=1) explore.drop(&quot;Survived&quot;,axis=1).plot.hist(alpha=0.5,bins=10) plt.xticks(range(11)) plt.show() . # Use pivot tables to look at the survival rate for different values of the columns import numpy as np for col in explore.columns.drop(&quot;Survived&quot;): pivot = explore.pivot_table(index=col,values=&quot;Survived&quot;) pivot.plot.bar(ylim=(0,1),yticks=np.arange(0,1,.1)) plt.axhspan(.3, .6, alpha=0.2, color=&#39;red&#39;) plt.show() . The SibSp column shows the number of siblings and/or spouses each passenger had on board, while the Parch columns shows the number of parents or children each passenger had onboard. Neither column has any missing values. . The distribution of values in both columns is skewed right, with the majority of values being zero. . You can sum these two columns to explore the total number of family members each passenger had onboard. The shape of the distribution of values in this case is similar, however there are less values at zero, and the quantity tapers off less rapidly as the values increase. . Looking at the survival rates of the the combined family members, you can see that few of the over 500 passengers with no family members survived, while greater numbers of passengers with family members survived. . Engineering new features . # Based on the observation about few surviver with no family group, let&#39;s create a binary value column where 1 is with # family and 0 is without family def feature_alone(df): df[&quot;familysize&quot;] = df[[&quot;SibSp&quot;,&quot;Parch&quot;]].sum(axis=1) df[&quot;isalone&quot;] = 0 df.loc[(df[&quot;familysize&quot;] == 0),&quot;isalone&quot;] = 1 df.drop(&quot;familysize&quot;, axis = 1) return df train = feature_alone(train) holdout = feature_alone(holdout) . Feature selection/preparation . # Select the best-performing features from sklearn.ensemble import RandomForestClassifier from sklearn.feature_selection import RFECV def select_features(df): # Remove non-numeric columns, columns that have null values df = df.select_dtypes([np.number]).dropna(axis=1) all_X = df.drop([&quot;Survived&quot;,&quot;PassengerId&quot;],axis=1) all_y = df[&quot;Survived&quot;] clf = RandomForestClassifier(random_state=1) selector = RFECV(clf,cv=10) selector.fit(all_X,all_y) best_columns = list(all_X.columns[selector.support_]) print(&quot;Best Columns n&quot;+&quot;-&quot;*12+&quot; n{}&quot;.format(best_columns)) return best_columns cols = select_features(train) . Best Columns [&#39;Pclass&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;, &#39;Age_categories_Adult&#39;, &#39;Age_categories_Infant&#39;, &#39;Age_categories_Missing&#39;, &#39;Age_categories_Senior&#39;, &#39;Age_categories_Teenager&#39;, &#39;Age_categories_Young Adult&#39;, &#39;Fare_categories_0-12&#39;, &#39;Fare_categories_100+&#39;, &#39;Fare_categories_12-50&#39;, &#39;Fare_categories_50-100&#39;, &#39;Title_Master&#39;, &#39;Title_Miss&#39;, &#39;Title_Mr&#39;, &#39;Title_Mrs&#39;, &#39;Title_Officer&#39;, &#39;Cabin_type_C&#39;, &#39;Cabin_type_D&#39;, &#39;Cabin_type_E&#39;, &#39;Cabin_type_Unknown&#39;, &#39;Sex_female&#39;, &#39;Sex_male&#39;, &#39;familysize&#39;, &#39;isalone&#39;] . Model selection/Tuning . # Write a function to train 3 different models. # Using grid search to train using different combinations of hyperparameters to find best performing models. from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.model_selection import GridSearchCV def select_model(df,features): all_X = df[features] all_y = df[&quot;Survived&quot;] # List of dictionaries, each containing a model name, # it&#39;s estimator and a dict of hyperparameters models = [ { &quot;name&quot;: &quot;LogisticRegression&quot;, &quot;estimator&quot;: LogisticRegression(), &quot;hyperparameters&quot;: { &quot;solver&quot;: [&quot;newton-cg&quot;, &quot;lbfgs&quot;, &quot;liblinear&quot;] } }, { &quot;name&quot;: &quot;KNeighborsClassifier&quot;, &quot;estimator&quot;: KNeighborsClassifier(), &quot;hyperparameters&quot;: { &quot;n_neighbors&quot;: range(1,20,2), &quot;weights&quot;: [&quot;distance&quot;, &quot;uniform&quot;], &quot;algorithm&quot;: [&quot;ball_tree&quot;, &quot;kd_tree&quot;, &quot;brute&quot;], &quot;p&quot;: [1,2] } }, { &quot;name&quot;: &quot;RandomForestClassifier&quot;, &quot;estimator&quot;: RandomForestClassifier(random_state=1), &quot;hyperparameters&quot;: { &quot;n_estimators&quot;: [4, 6, 9], &quot;criterion&quot;: [&quot;entropy&quot;, &quot;gini&quot;], &quot;max_depth&quot;: [2, 5, 10], &quot;max_features&quot;: [&quot;log2&quot;, &quot;sqrt&quot;], &quot;min_samples_leaf&quot;: [1, 5, 8], &quot;min_samples_split&quot;: [2, 3, 5] } } ] for model in models: print(model[&#39;name&#39;]) print(&#39;-&#39;*len(model[&#39;name&#39;])) grid = GridSearchCV(model[&quot;estimator&quot;], param_grid=model[&quot;hyperparameters&quot;], cv=10) grid.fit(all_X,all_y) model[&quot;best_params&quot;] = grid.best_params_ model[&quot;best_score&quot;] = grid.best_score_ model[&quot;best_model&quot;] = grid.best_estimator_ print(&quot;Best Score: {}&quot;.format(model[&quot;best_score&quot;])) print(&quot;Best Parameters: {} n&quot;.format(model[&quot;best_params&quot;])) return models result = select_model(train,cols) . LogisticRegression Best Score: 0.8226711560044894 Best Parameters: {&#39;solver&#39;: &#39;liblinear&#39;} KNeighborsClassifier -- Best Score: 0.7833894500561167 Best Parameters: {&#39;algorithm&#39;: &#39;kd_tree&#39;, &#39;n_neighbors&#39;: 3, &#39;p&#39;: 1, &#39;weights&#39;: &#39;uniform&#39;} RandomForestClassifier - Best Score: 0.8451178451178452 Best Parameters: {&#39;criterion&#39;: &#39;entropy&#39;, &#39;max_depth&#39;: 10, &#39;max_features&#39;: &#39;sqrt&#39;, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 3, &#39;n_estimators&#39;: 9} . Submit to Kaggle . def save_submission_file(model,cols,filename=&quot;submission.csv&quot;): holdout_data = holdout[cols] predictions = model.predict(holdout_data) holdout_ids = holdout[&quot;PassengerId&quot;] submission_df = {&quot;PassengerId&quot;: holdout_ids, &quot;Survived&quot;: predictions} submission = pd.DataFrame(submission_df) submission.to_csv(filename,index=False) best_rf_model = result[2][&quot;best_model&quot;] save_submission_file(best_rf_model,cols) .",
            "url": "https://phucnsp.github.io/blog/jupyter/2017/08/20/kaggle-titanic-machine-learning-from-disaster.html",
            "relUrl": "/jupyter/2017/08/20/kaggle-titanic-machine-learning-from-disaster.html",
            "date": " • Aug 20, 2017"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I have been working as Data Scientist at MTI Technology Vietnam since 2018 and my journey in AI field started since 2017. In here, I mainly work with OCR (optical charcter recognition) projects where we not only have to extract texts from documents but also classify it into some specific fields defined by clients. .",
          "url": "https://phucnsp.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}