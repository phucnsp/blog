{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch part 1 - tensor and Pytorch tensor\n",
    "> This notebook introduces fundamental pieces of neural network such as tensors, Pytorch tensor operation, GPU, CUDA.\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [self-taught]\n",
    "- image: images/pytorch_ava.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introducing Pytorch, CUDA and GPU\n",
    "\n",
    "PyTorch is a `deep learning framework` and a `scientific computing package`.   \n",
    "The scientific computing aspect of PyTorch is primarily a result PyTorch’s tensor library and associated tensor operations. That means you can take advantage of Pytorch for many computing tasks, thanks to its supporting tensor operation, without touching deep learning modules.\n",
    "\n",
    "Important to note that PyTorch tensors and their associated operations are very similar to numpy n-dimensional arrays. A tensor is actually an n-dimensional array.   \n",
    "\n",
    "Pytorch build its library around Object Oriented Programming(OOP) concept. With object oriented programming, we orient our program design and structure around `objects` (take a look at `Random topics` for more information). The tensor in Pytorch is presented by the object `torch.Tensor` which is created from numpy ndarray objects. Two objects `share memory`. This makes the transition between PyTorch and NumPy very cheap from a performance perspective.\n",
    "\n",
    "With PyTorch tensors, GPU support is built-in. It’s very easy with PyTorch to move tensors to and from a GPU if we have one installed on our system.\n",
    "Tensors are super important for deep learning and neural networks because they are the data structure that we ultimately use for building and training our neural networks.\n",
    "\n",
    "Talking a bit about history.  \n",
    "The initial release of PyTorch was in October of 2016, and before PyTorch was created, there was and still is, another framework called Torch which is also a machine learning framework but is based on the Lua programming language. The connection between PyTorch and this Lua version, called Torch, exists because many of the developers who maintain the Lua version are the individuals who created PyTorch. And they have been working for Facebook since then till now.\n",
    "\n",
    "> Note: Facebook Created PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the primary PyTorch modules we’ll be learning about and using as we build neural networks along the way.\n",
    "\n",
    "|Package|Description &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|\n",
    "|:--------------------|:----------------------------------------------------------------------------------------|\n",
    "| torch               |The top-level PyTorch package and tensor library.   |\n",
    "| torch.nn            | A subpackage that contains modules and extensible classes for building neural networks. |\n",
    "| torch.autograd      | A subpackage that supports all the differentiable Tensor operations in PyTorch. |\n",
    "| torch.nn.functional | A functional interface that contains operations used for building neural net like loss, activation, layer operations... |\n",
    "| torch.optim         | A subpackage that contains standard optimization operations like SGD and Adam.   |\n",
    "| torch.utils         | A subpackage that contains utility classes like data sets and data loaders that make data preprocessing easier.                                                  |\n",
    "| torchvision         | A package that provides access to popular datasets, models, and image transformations for computer vision.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use PyTorch for deep learning?\n",
    "- PyTorch’s design is modern, Pythonic. When we build neural networks with PyTorch, we are super close to programming neural networks from scratch. When we write PyTorch code, we are just writing and extending standard Python classes, and when we debug PyTorch code, we are using the standard Python debugger. It’s written mostly in Python, and only drops into C++ and CUDA code for operations that are performance bottlenecks.\n",
    "- It is a thin framework, which makes it more likely that PyTorch will be capable of adapting to the rapidly evolving deep learning environment as things change quickly over time.\n",
    "- Stays out of the way and this makes it so that we can focus on neural networks and less on the actual framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why PyTorch is great for deep learning research\n",
    "\n",
    "The reason for this research suitability is that Pytorch use dynamic computational graph, in contrast with tensorfow which uses static computational graph, in order to calculate derivatives.\n",
    "\n",
    "Computational graphs are used to graph the function operations that occur on tensors inside neural networks.\n",
    "These graphs are then used to compute the derivatives needed to optimize the neural network. Dynamic computational graph means that the graph is generated on the fly as the operations are created. Static graphs that are fully determined before the actual operations occur.\n",
    "\n",
    "It just so happens that many of the cutting edge research topics in deep learning are requiring or benefiting greatly from dynamic graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing PyTorch \n",
    "The recommended best option is to use the Anaconda Python package manager. With Anaconda, it's easy to get and manage Python, Jupyter Notebook, and other commonly used packages for scientific computing and data science, like PyTorch!\n",
    "\n",
    "Let’s go over the steps:\n",
    "\n",
    "- Download and install [Anaconda](https://www.anaconda.com/distribution/) (choose the latest Python version).\n",
    "- Go to [PyTorch's site](https://pytorch.org/) and find the get started locally section.\n",
    "- Specify the appropriate configuration options for your particular environment.\n",
    "- Run the presented command in the terminal to install PyTorch\n",
    "\n",
    "For the example: \n",
    "`conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`\n",
    "\n",
    "Notice that we are installing both PyTorch and torchvision. Also, there is no need to install CUDA separately. The needed CUDA software comes installed with PyTorch if a CUDA version is selected in step (3). All we need to do is select a version of CUDA if we have a supported Nvidia GPU on our system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:38:33.162327Z",
     "start_time": "2020-04-13T04:38:27.338652Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /Users/phucnsp/anaconda3/envs/fastai2:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "pytorch                   1.4.0                   py3.7_0    pytorch\r\n",
      "torchsummary              1.5.1                    pypi_0    pypi\r\n",
      "torchvision               0.5.0                  py37_cpu    pytorch\r\n"
     ]
    }
   ],
   "source": [
    "!conda list torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:38:59.319747Z",
     "start_time": "2020-04-13T04:38:59.315359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__ # to verify pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:39:06.656741Z",
     "start_time": "2020-04-13T04:39:06.652363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # to verify our GPU capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why deep learning uses GPUs\n",
    "To understand CUDA, we need to have a working knowledge of graphics processing units (GPUs). A GPU is a processor that is good at handling specialized computations. This is in contrast to a central processing unit (CPU), which is a processor that is good at handling general computations. CPUs are the processors that power most of the typical computations on our electronic devices.\n",
    "\n",
    "A GPU can be much faster at computing than a CPU. However, this is not always the case. The speed of a GPU relative to a CPU depends on the type of computation being performed. `The type of computation most suitable for a GPU is a computation that can be done in parallel.`\n",
    "\n",
    "Parallel computing is a type of computation where by a particular computation is broken into independent smaller computations that can be carried out simultaneously. The resulting computations are then recombined, or synchronized, to form the result of the original larger computation. The number of tasks that a larger task can be broken into depends on the number of cores contained on a particular piece of hardware. Cores are the units that actually do the computation within a given processor, and CPUs typically have four, eight, or sixteen cores while GPUs have potentially thousands.\n",
    "\n",
    "So why deep learning uses them - `Neural networks are embarrassingly parallel`.\n",
    "Tasks that embarrassingly parallel are ones where it’s easy to see that the set of smaller tasks are independent with respect to each other. Many of the computations that we do with neural networks can be easily broken into smaller computations in such a way that the set of smaller computations do not depend on one another. One such example is a convolution.\n",
    "![](data/conv.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU, CUDA and Nvidia \n",
    "\n",
    "**GPU computing**  \n",
    "In the beginning, the main tasks that were accelerated using GPUs were computer graphics. That's why we have the name graphics processing unit. But `in recent years, many more varieties parallel tasks have emerged`. One such task as we have seen is `deep learning.`  \n",
    "Deep learning along with many other scientific computing tasks that use parallel programming techniques are leading to a new type of programming model called [GPGPU](https://arxiv.org/abs/1408.6923) or general purpose GPU computing.  \n",
    "\n",
    "GPU computing practically began with the introduction of CUDA by NVIDIA and Stream by AMD. These are APIs designed by the GPU vendors to be used together with the hardware that they provide.   \n",
    "Nvidia is a technology company that designs GPUs, and they have created `CUDA as a software platform` that pairs with their `GPU hardware` making it easier for developers to build software that accelerates computations using the parallel processing power of Nvidia GPUs.\n",
    "\n",
    "**GPU computing stack concept**  \n",
    "The stack comprises of:\n",
    "- GPU as the hardware on the bottom\n",
    "- CUDA as the software architecture on top of the GPU\n",
    "- And finally libraries like cuDNN on top of CUDA. \n",
    "\n",
    "Sitting on top of CUDA and cuDNN is PyTorch, which is the framework were we’ll be working that ultimately supports applications on top.\n",
    "Developers use CUDA by downloading the `CUDA toolkit` which comes with specialized libraries like `cuDNN` - the CUDA Deep Neural Network library. With PyTorch, CUDA comes baked in from the start. There are no additional downloads required. All we need is to have a supported Nvidia GPU, and we can leverage CUDA using PyTorch. We don’t need to know how to use the CUDA API directly.\n",
    "\n",
    "After all, PyTorch is written in all of these: Python, C++, CUDA\n",
    "\n",
    "![](data/dl_stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:52:02.092128Z",
     "start_time": "2020-04-13T04:52:02.088536Z"
    }
   },
   "outputs": [],
   "source": [
    "t = torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor object created in this way is on the CPU by default. As a result, any operations that we do using this tensor object will be carried out on the CPU.\n",
    "Now, to move the tensor onto the GPU, we just write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:52:20.885587Z",
     "start_time": "2020-04-13T04:52:20.882666Z"
    }
   },
   "outputs": [],
   "source": [
    "t = t.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ability makes PyTorch very flexible because computations can be selectively carried out either on the CPU or on the GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: GPU Can Be Slower Than CPU. The answer is that a GPU is only faster for particular (specialized) tasks. For example, moving data from the CPU to the GPU is costly, so in this case, the overall performance might be slower if the computation task is a simple one. Moving relatively small computational tasks to the GPU won’t speed us up very much and may indeed slow us down. Remember, the GPU works well for tasks that can be broken into many smaller tasks, and if a compute task is already small, we won’t have much to gain by moving the task to the GPU.For this reason, it’s often acceptable to simply use a CPU when just starting out, and as we tackle larger more complicated problems, begin using the GPU more heavily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Introducing Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors - Data Structures of Deep Learning\n",
    "\n",
    "A tensor is the primary data structure used by neural networks. The inputs, outputs, and transformations within neural networks are all represented using tensors, and as a result, neural network programming utilizes tensors heavily.\n",
    "\n",
    "The below concepts, that we met in math or computer science, are all refered to `tensor` in deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| indexes required |  math  | computer science |\n",
    "|:----------------:|:------:|------------------|\n",
    "| 0                | scalar | number           |\n",
    "| 1                | vector | array            |\n",
    "| 2                | matrix | 2d-array         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship within each of these pairs, for example `vector` and `array`, is that both elements require the same number of indexes to refer to a specific element within the data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T03:56:56.843744Z",
     "start_time": "2020-04-13T03:56:56.839595Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an array or a vector requires 1 index to access its element\n",
    "a = [1,2,3,4]\n",
    "a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T03:57:42.727931Z",
     "start_time": "2020-04-13T03:57:42.723040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an matrix or 2d-array requires 2 index to access its element\n",
    "a = [\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8]\n",
    "]\n",
    "a[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When more than two indexes are required to access a specific element, we stop giving specific names to the structures, and we begin using more general language.\n",
    "- In mathematics, we stop using words like scalar, vector, and matrix, and we start using the word `tensor` or `nd-tensor`. The n tells us the number of indexes required to access a specific element within the structure.\n",
    "- In computer science, we stop using words like, number, array, 2d-array, and start using the word `multidimensional array` or `nd-array`. The n tells us the number of indexes required to access a specific element within the structure.\n",
    "\n",
    "The reason we say a `tensor` is a generalization form is because we use the word tensor for all values of n like so:\n",
    "- A scalar is a 0 dimensional tensor\n",
    "- A vector is a 1 dimensional tensor\n",
    "- A matrix is a 2 dimensional tensor\n",
    "- A nd-array is an n dimensional tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Tensors and nd-arrays are the same thing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental tensor attributes for deep learning  - Rank, Axes, and Shape.\n",
    "\n",
    "These concepts build on one another starting with rank, then axes, and building up to shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rank` of a tensor refers to `the number of dimensions` present within the tensor. A rank-2 tensor means all of the following: `a matrix`, `a 2d-array`, `a 2d-tensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `axis` of a tensor is `a specific dimension` of a tensor.   \n",
    "Let's get an example how to access elements of an axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:09:36.955138Z",
     "start_time": "2020-04-13T04:09:36.951955Z"
    }
   },
   "outputs": [],
   "source": [
    "dd = [\n",
    "[1,2,3],\n",
    "[4,5,6],\n",
    "[7,8,9]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element along the first axis, is an array:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:09:59.266138Z",
     "start_time": "2020-04-13T04:09:59.261296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3], [4, 5, 6], [7, 8, 9])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[0], dd[1], dd[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element along the second axis, is a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:10:22.174494Z",
     "start_time": "2020-04-13T04:10:22.170115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 7)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[0][0], dd[1][0], dd[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: with tensors, the elements of the last axis are always numbers. Every other axis will contain n-dimensional arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shape` of a tensor gives us the `length of each axis` of the tensor. \n",
    ">Note: The shape of a tensor is important because it encodes all of the relevant information about axes, rank, and therefore indexes. Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called `reshaping`.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:13:01.670957Z",
     "start_time": "2020-04-13T04:13:01.665545Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1,2,3],\n",
    "    [5,6,7]\n",
    "], dtype=torch.float)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: size and shape of a tensor are the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Pytorch Tensors\n",
    "\n",
    "`PyTorch tensors are the data structures we'll be using when programming neural networks in PyTorch.`  \n",
    "\n",
    "The tensor in Pytorch is presented by the object `torch.Tensor` which is created from numpy ndarray objects. Two objects `share memory`. This makes the transition between PyTorch and NumPy very cheap from a performance perspective.\n",
    "\n",
    "When programming neural networks, data preprocessing is often one of the first steps in the overall process, and one goal of data preprocessing is to transform the raw input data into tensor form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.Tensor class and its attributes\n",
    "PyTorch tensors are instances of the `torch.Tensor` Python class.   \n",
    "First, let’s look at a few torch.Tensor's tensor attributes.\n",
    "- tensor.dtype: \n",
    "- tensor.device\n",
    "- tensor.layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T00:58:09.796913Z",
     "start_time": "2020-04-21T00:58:09.786261Z"
    }
   },
   "outputs": [],
   "source": [
    "t = torch.Tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:28:26.341835Z",
     "start_time": "2020-04-13T01:28:26.199513Z"
    }
   },
   "source": [
    "The `dtype` specifies the type of the data that is contained within the tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T00:58:33.288817Z",
     "start_time": "2020-04-21T00:58:33.272390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:28:26.341835Z",
     "start_time": "2020-04-13T01:28:26.199513Z"
    }
   },
   "source": [
    "Table below shows all the tensor types that Pytorch supports. Each type has a CPU and GPU version. Tensors contain uniform (of the same type) numerical data.\n",
    "\n",
    "![pytorch datatype](data/pytorch_dtype.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `device` specifies the device (`CPU` or `GPU`) where the tensor's data is allocated. This determines where tensor computations for the given tensor will be performed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T00:58:50.669249Z",
     "start_time": "2020-04-21T00:58:50.664198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch supports the use of multiple devices, and they are specified using an index like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:35:52.603803Z",
     "start_time": "2020-04-13T01:35:52.593007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a device like above, we can create a tensor on the device by passing the device to the tensor’s constructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T00:57:05.383520Z",
     "start_time": "2020-04-21T00:57:05.381051Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "t = torch.tensor([1,2,3], dtype=torch.int, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `layout` specifies how the tensor is stored in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T00:59:16.648157Z",
     "start_time": "2020-04-21T00:59:16.644086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.strided"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about stride check [here](https://en.wikipedia.org/wiki/Stride_of_an_array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new tensor using data\n",
    "\n",
    "These are the primary ways of creating tensor objects (instances of the `torch.Tensor` class), with data (array-like) in PyTorch:\n",
    "1. **torch.Tensor(data)** is the constructor of the `torch.Tensor` class\n",
    "2. **torch.tensor(data)**: is the `factory function` that constructs `torch.Tensor` objects. \n",
    "3. **torch.as_tensor(data)**\n",
    "4. **torch.from_numpy(data)**\n",
    "\n",
    "Let’s look at each of these. They all accept some form of data and give us an instance of the `torch.Tensor` class. Sometimes when there are multiple ways to achieve the same result, things can get confusing, so let’s break this down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T03:37:58.401115Z",
     "start_time": "2020-04-13T03:37:58.394648Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.array([1,2,3])\n",
    "\n",
    "o1 = torch.Tensor(data)\n",
    "o2 = torch.tensor(data)\n",
    "o3 = torch.as_tensor(data)\n",
    "o4 = torch.from_numpy(data)\n",
    "\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(o3)\n",
    "print(o4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below compare 4 options and propose which one to use. \n",
    "- `torch.tensor` is best option to go daily ^^. It copy data which help us prevent hidden mistake caused by sharing data. In addition, it is better than `torch.Tensor` thanks to better doc and more config options.\n",
    "- `torch.as_tensor` is recommened in case we want to improve the performance and want to levarage data share memory characteristic of Pytorch. However, it is always better to start with copy data to make sure your program works first and go to performance improvement later. This option is better than `torch.from_numpy` because it accepts a wide variety of array-like objects including other  Pytorch tensor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          method         |         which one to use  &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;   | dtype   &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;     | data in memory |\n",
    "|:-----------------------|:------------------------------|:-----------------------------|:--------------|\n",
    "| torch.Tensor(data)      | |infer from default dtype. |copy|\n",
    "| **torch.tensor(data)**    |best option to go |inferred from input or explicitly set.|copy|\n",
    "| *torch.as_tensor(data)* | use for improve performance |inferred from input or explicitly set.|share|\n",
    "| torch.from_numpy(data) | |inferred from input or explicitly set.|share|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data in memory is shared means that the actual data in memory exists in a single place. As a result, any changes that occur in the underlying data will be reflected in both objects, the `torch.Tensor` and the `numpy.ndarray`.  \n",
    "Sharing data is more efficient and uses less memory than copying data because the data is not written to two locations in memory.\n",
    "However, there are something to keep in mind about memory sharing:\n",
    "- Since `numpy.ndarray` objects are allocated on the CPU, the `as_tensor()` function must copy the data from the CPU to the GPU when a GPU is being used.\n",
    "- The memory sharing of `as_tensor()` doesn’t work with built-in Python data structures like `list`.\n",
    "- The `as_tensor()` call requires developer knowledge of the sharing feature. This is necessary so we don’t inadvertently make an unwanted change in the underlying data without realizing the change impacts multiple objects.\n",
    "- The `as_tensor()` performance improvement will be greater if there are a lot of back and forth operations between `numpy.ndarray` objects and tensor objects. However, if there is just a single load operation, there shouldn’t be much impact from a performance perspective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips, In order to convert multiple arrays to tensor we can use `map`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T01:06:26.164720Z",
     "start_time": "2020-04-21T01:06:26.131862Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3]), tensor([3, 4, 5]), tensor([1]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "a = np.array([1,2,3])\n",
    "b = np.array([3,4,5])\n",
    "c = np.array([1])\n",
    "\n",
    "a, b, c = map(torch.tensor, (a, b, c))\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new tensor without data\n",
    "Here are some other creation options that are available.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:46:11.612741Z",
     "start_time": "2020-04-13T01:46:11.608093Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create identity matrix\n",
    "torch.eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:46:22.493974Z",
     "start_time": "2020-04-13T01:46:22.487993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor of zeros with the shape of specified shape argument\n",
    "torch.zeros([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:47:03.594806Z",
     "start_time": "2020-04-13T01:47:03.583642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor of ones with the shape of specified shape argument\n",
    "torch.ones([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:47:32.090530Z",
     "start_time": "2020-04-13T01:47:32.076459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3088, 0.4226],\n",
       "        [0.8102, 0.9129]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a tensor filled with random numbers from a uniform distribution on the interval [0, 1)\n",
    "# The shape of the tensor is defined by the variable argument size.\n",
    "torch.rand([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:45:32.402471Z",
     "start_time": "2020-04-17T07:45:26.790791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1778, -1.0388, -0.1459],\n",
       "        [-0.1746,  0.5764, -1.1491]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 \n",
    "# (also called the standard normal distribution).\n",
    "torch.randn(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a small subset of the available creation functions that don’t require data. Check with the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for the full list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Pytorch Tensor Operation Types\n",
    "We have the following high-level categories of tensor operations:\n",
    "- **Reshaping operation type:**  gave us the ability to position our elements along particular axes. \n",
    "- **Element-wise operation type:**  allow us to perform operations on elements between two tensors.\n",
    "- **Reduction operation type:**  allow us to perform operations on elements within a single tensor.\n",
    "- **Access operation type** allow us to access to each numerical elements within a single tensor.\n",
    "\n",
    "There are a lot of individual operations out there, so much so that it can sometimes be intimidating when you're just beginning, but grouping similar operations into categories based on their likeness can help make learning about tensor operations more manageable.  \n",
    "\n",
    "> Important: Tensor operations between tensors must happen between tensors with the same type of data and on the same device\n",
    "\n",
    "Before going to each category, firstly we will take a look on the concept of `broadcasting`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Broadcasting Tensors\n",
    "\n",
    "To understand this concept, let's take a look at an example. Suppose we have the following tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T01:10:07.869979Z",
     "start_time": "2020-04-21T01:10:07.865152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2]), torch.Size([2]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,1],\n",
    "    [1,1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "t2 = torch.tensor([2,4], dtype=torch.float32)\n",
    "\n",
    "t1.shape, t2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will be the result of this element-wise addition operation, t1 + t2 ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T01:10:18.520920Z",
     "start_time": "2020-04-21T01:10:18.491110Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 5.],\n",
       "        [3., 5.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Even though these two tenors have differing shapes, the element-wise operation is possible, and broadcasting is what makes the operation possible. The lower rank tensor t2 will be transformed via broadcasting to match the shape of the higher rank tensor t1, and the element-wise operation will be performed as usual.\n",
    "\n",
    "we can check the broadcast transformation using the `broadcast_to()` numpy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:48:23.362778Z",
     "start_time": "2020-04-12T07:48:23.350517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 4.],\n",
       "       [2., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.broadcast_to(t2.numpy(), t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:48:28.826126Z",
     "start_time": "2020-04-12T07:48:28.821405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 5.],\n",
       "        [3., 5.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After broadcasting, the addition operation between these two tensors is a regular element-wise operation between tensors of the same shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Tensor reshape operation type\n",
    "\n",
    "Reshaping operations are perhaps the most important type of tensor operations because the shape of a tensor gives us something concrete we can use to shape an intuition for our tensors.\n",
    ">Note: Reshaping changes the shape but not the underlying data elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `reshape()` function, we can specify the `row x column` shape that we are seeking. Notice that the product of the shape's components has to be equal to the number of elements in the original tensor.\n",
    "\n",
    "Pytorch has another function called `view()` that does the same thing as `reshape` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T01:14:00.045915Z",
     "start_time": "2020-04-21T01:14:00.042097Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T01:14:00.976003Z",
     "start_time": "2020-04-21T01:14:00.970417Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common reshape type operation is `squeeze`, `unsqueeze`. Those operations change the shape of our tensors is by squeezing and unsqueezing them.\n",
    "- Squeezing a tensor removes the dimensions or axes that have a length of one.\n",
    "- Unsqueezing a tensor adds a dimension with a length of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T01:16:44.638365Z",
     "start_time": "2020-04-21T01:16:44.633702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([1,12]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:01:22.117373Z",
     "start_time": "2020-04-13T01:01:22.111978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([1,12]).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:01:58.409610Z",
     "start_time": "2020-04-13T01:01:58.395995Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([1,12]).unsqueeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common reshape type operation is `flattening`.  \n",
    "A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flatted out before the fully connected layer will accept the input.  \n",
    "A flatten operation on a tensor reshapes the tensor to have a shape that is equal to the number of elements contained in the tensor. This is the same thing as a 1d-array of elements.\n",
    "\n",
    "\n",
    "\n",
    "These are some ways to flatten a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T01:14:02.725859Z",
     "start_time": "2020-04-21T01:14:02.720234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T01:14:03.332795Z",
     "start_time": "2020-04-21T01:14:03.327306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T01:14:04.384346Z",
     "start_time": "2020-04-21T01:14:04.377903Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(t.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T01:14:09.211579Z",
     "start_time": "2020-04-21T01:14:09.205932Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it is possible to flatten only specific parts of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:08:34.734018Z",
     "start_time": "2020-04-13T01:08:34.727426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "]], dtype=torch.float32)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:08:37.978364Z",
     "start_time": "2020-04-13T01:08:37.972845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a deeper look inside the flatten operation, it is actually a composition of reshape and squeeze operation.  \n",
    "> Note: flatten operation = reshape operation + squeeze operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:05:56.847349Z",
     "start_time": "2020-04-13T01:05:56.840315Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_ex(t):\n",
    "    t = t.reshape(1, -1)\n",
    "    t = t.squeeze()\n",
    "    return t\n",
    "flatten_ex(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor element-wise operation type\n",
    "\n",
    "An element-wise operation is an operation between two tensors that operates on corresponding elements within the respective tensors. Two tensors must have the same shape in order to perform element-wise operations on them.\n",
    "\n",
    "Some common element-wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:50.391690Z",
     "start_time": "2020-04-12T07:49:50.388169Z"
    }
   },
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1,2],\n",
    "                   [3,4]], dtype=torch.float32)\n",
    "t2 = torch.tensor([[9,8],\n",
    "                   [7,6]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:50.612975Z",
     "start_time": "2020-04-12T07:49:50.607796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10.],\n",
       "        [10., 10.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + t2 # equivalent with t1.add(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:50.835664Z",
     "start_time": "2020-04-12T07:49:50.830431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + 2 # equivalent with t1.add(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:51.052223Z",
     "start_time": "2020-04-12T07:49:51.046968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  0.],\n",
       "        [ 1.,  2.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 - 2 # equivalent with t1.sub(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:51.264834Z",
     "start_time": "2020-04-12T07:49:51.259196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [6., 8.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 * 2 # equivalent with t1.mul(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:51.476863Z",
     "start_time": "2020-04-12T07:49:51.471819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 1.0000],\n",
       "        [1.5000, 2.0000]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 / 2 # equivalent with t1.div(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Comparison Operation is element-wise type operation`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:51.909639Z",
     "start_time": "2020-04-12T07:49:51.904742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True],\n",
       "        [False,  True, False],\n",
       "        [ True, False,  True]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:52.134464Z",
     "start_time": "2020-04-12T07:49:52.128893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False],\n",
       "        [ True, False,  True],\n",
       "        [False,  True, False]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.gt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:52.406219Z",
     "start_time": "2020-04-12T07:49:52.401338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.lt(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With element-wise operations that are functions, it’s fine to assume that the function is applied to each element of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:52.841982Z",
     "start_time": "2020-04-12T07:49:52.837285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [2., 0., 2.],\n",
       "        [0., 3., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:53.060024Z",
     "start_time": "2020-04-12T07:49:53.054915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.0000, 0.0000],\n",
       "        [1.4142, 0.0000, 1.4142],\n",
       "        [0.0000, 1.7321, 0.0000]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:53.559141Z",
     "start_time": "2020-04-12T07:49:53.554168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0., -1., -0.],\n",
       "        [-2., -0., -2.],\n",
       "        [-0., -3., -0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:53.644620Z",
     "start_time": "2020-04-12T07:49:53.639000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [2., 0., 2.],\n",
       "        [0., 3., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg().abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Element-wise = Component-wise = Point-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor reduction operations type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor.\n",
    "Tensors give us the ability to manage our data. The tensor can be reduced to a single scalar value or reduced along an axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at common tensor reduction operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:54.256198Z",
     "start_time": "2020-04-12T07:49:54.252620Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([\n",
    "    [0,1,0],\n",
    "    [2,0,2],\n",
    "    [0,3,0]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing to a tensor with a single element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:54.468754Z",
     "start_time": "2020-04-12T07:49:54.463253Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(8.), tensor(0.), tensor(0.8889), tensor(1.1667))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(), t.prod(), t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing tensors By Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:57.259599Z",
     "start_time": "2020-04-12T07:49:57.253925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 2.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Argmax` tensor reduction operation is very common in neural network.  \n",
    "This operation returns the index location of the maximum value inside a tensor.\n",
    "In practice, we often use the `argmax()` function on a network’s output prediction tensor, to determine which category has the highest prediction value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:58.048395Z",
     "start_time": "2020-04-12T07:49:58.042858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1., 2., 3.]),\n",
       "indices=tensor([1, 2, 1]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:58.418903Z",
     "start_time": "2020-04-12T07:49:58.414338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor access operation type\n",
    "\n",
    "This operation provides the ability to access data within the tensor.   \n",
    "Common tensor access operations are `item(), tolist(), numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:59.799175Z",
     "start_time": "2020-04-12T07:49:59.794095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888955116272"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:50:00.018601Z",
     "start_time": "2020-04-12T07:50:00.013310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6666666865348816, 1.3333333730697632, 0.6666666865348816]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(dim=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:50:00.217873Z",
     "start_time": "2020-04-12T07:50:00.210173Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6666667, 1.3333334, 0.6666667], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Object Oriented Programming and Why Pytorch select it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When we’re writing programs or building software, there are two key components, `code` and `data`. With object oriented programming, we orient our program design and structure around `objects`.  \n",
    "`Objects` are defined in code using `classes`. A class defines the object's specification or spec, which specifies what `data` and `code` each object of the class should have.  \n",
    "When we create an object of a class, we call the object an instance of the class, and all instances of a given class have two core components:\n",
    "- Methods(code)\n",
    "- Attributes(data)\n",
    "\n",
    "In a given program, many objects, a.k.a instances of a given class have the same available attributes and the same available methods. The difference between objects of the same class is the values contained within the object for each attribute. Each object has its own attribute values. These values determine the internal state of the object. The code and data of each object is said to be encapsulated within the object.\n",
    "\n",
    "Let’s build a simple class to demonstrate how classes encapsulate data and code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-10T07:49:03.941817Z",
     "start_time": "2020-04-10T07:49:03.924418Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Sample: #class declaration\n",
    "    def __init__(self, name): #class constructor (code)\n",
    "        self.name = name #attribute (data)\n",
    "    \n",
    "    def set_name(self, name): #method declaration (code)\n",
    "        self.name = name #method implementation (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's switch gears now and look at how object oriented programming fits in with PyTorch.\n",
    "\n",
    "The primary component we'll need to build a neural network is a layer, and so, as we might expect, PyTorch's neural network library contains classes that aid us in constructing layers.\n",
    "As we know, deep neural networks are built using multiple layers. This is what makes the network deep. Each layer in a neural network has two primary components:\n",
    "- A transformation (code)\n",
    "- A collection of weights (data)\n",
    "\n",
    "Like many things in life, this fact makes layers great candidates to be represented as objects using Object Oriented Programming - OOP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## References\n",
    "Some good sources:\n",
    "- [pytorch zero to all](https://github.com/hunkim/PyTorchZeroToAll)\n",
    "- [deeplizard](https://deeplizard.com/learn/video/v5cngxo4mIg)\n",
    "- [effective pytorch](https://github.com/vahidk/EffectivePyTorch?fbclid=IwAR1MhsjnjccWy6dIVtibFOCZbWhLtAj5pSTobnkUDxw_gHgfEswnVzqrKQ0#torchscript)\n",
    "- [what is torch.nn really?](https://pytorch.org/tutorials/beginner/nn_tutorial.html)\n",
    "- [recommend walk with pytorch](https://forums.fast.ai/t/getting-comfortable-with-pytorch-projects/28371)\n",
    "- [official tutorial](https://pytorch.org/tutorials/)\n",
    "- [DL(with Pytorch)](https://github.com/Atcold/pytorch-Deep-Learning)\n",
    "- [Pytorch project template](https://github.com/moemen95/PyTorch-Project-Template)\n",
    "- [nlp turorial with pytorch](https://github.com/graykode/nlp-tutorial)\n",
    "- [UDACITY course](https://www.udacity.com/course/deep-learning-pytorch--ud188)\n",
    "- [awesome pytorch list](https://github.com/bharathgs/Awesome-pytorch-list)\n",
    "- [deep learning with pytorch](https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf)\n",
    "- others:\n",
    "    - https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc\n",
    "    - Grokking Algorithms: An illustrated guide for programmers and other curious people 1st Edition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai2] *",
   "language": "python",
   "name": "conda-env-fastai2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
