{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch part 1 - tensor and Pytorch tensor\n",
    "> This notebook covers the fundamental part of deep learning -tensor and overview of Pytorch as well.\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [self-taught]\n",
    "- image: images/pytorch_ava.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Section 1: Introducing Pytorch\n",
    "\n",
    "PyTorch is a `deep learning framework` and a `scientific computing package`.   \n",
    "The scientific computing aspect of PyTorch is primarily a result PyTorch’s tensor library and associated tensor operations.\n",
    "\n",
    "PyTorch tensors and their associated operations are very similar to numpy n-dimensional arrays. A tensor is actually an n-dimensional array. For example, PyTorch `torch.Tensor` objects that are created from numpy ndarray objects, share memory. This makes the transition between PyTorch and NumPy very cheap from a performance perspective.\n",
    "\n",
    "With PyTorch tensors, GPU support is built-in. It’s very easy with PyTorch to move tensors to and from a GPU if we have one installed on our system.\n",
    "Tensors are super important for deep learning and neural networks because they are the data structure that we ultimately use for building and training our neural networks.\n",
    "\n",
    "The initial release of PyTorch was in October of 2016, and before PyTorch was created, there was and still is, another framework called Torch which is also a machine learning framework but is based on the Lua programming language. The connection between PyTorch and this Lua version, called Torch, exists because many of the developers who maintain the Lua version are the individuals who created PyTorch.\n",
    "\n",
    "> Note: Facebook Created PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "These are the primary PyTorch components we’ll be learning about and using as we build neural networks in this series.\n",
    "\n",
    "| Package             |                                                                            Description                                                                           |\n",
    "|:---------------------|:----------------------------------------------|\n",
    "| torch               |                                                         The top-level PyTorch package and tensor library.                                                        |\n",
    "| torch.nn            | A subpackage that contains modules and extensible classes for building neural networks.                                                                          |\n",
    "| torch.autograd      | A subpackage that supports all the differentiable Tensor operations in PyTorch.                                                                                  |\n",
    "| torch.nn.functional | A functional interface that contains typical operations used for building neural networks like loss functions, activation functions, and convolution operations. |\n",
    "| torch.optim         | A subpackage that contains standard optimization operations like SGD and Adam.                                                                                   |\n",
    "| torch.utils         | A subpackage that contains utility classes like data sets and data loaders that make data preprocessing easier.                                                  |\n",
    "| torchvision         | A package that provides access to popular datasets, model architectures, and image transformations for computer vision.                                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Why Use PyTorch For Deep Learning?\n",
    "- it is a thin framework, which makes it more likely that PyTorch will be capable of adapting to the rapidly evolving deep learning environment as things change over time.\n",
    "- stays out of the way and this makes it so that we can focus on neural networks and less on the actual framework. When we build neural networks with PyTorch, we are super close to programming neural networks from scratch. When we write PyTorch code, we are just writing and extending standard Python classes, and when we debug PyTorch code, we are using the standard Python debugger.\n",
    "\n",
    "PyTorch's development is guided by the following list:\n",
    "- Stay out of the way\n",
    "- Cater to the impatient\n",
    "- Promote linear code-flow\n",
    "- Full interop with the Python ecosystem\n",
    "- Be as fast as anything else\n",
    "\n",
    "\n",
    "PyTorch’s design is modern, Pythonic, and thin. The source code is easy to read for Python developers because it’s written mostly in Python, and only drops into C++ and CUDA code for operations that are performance bottlenecks.\n",
    "\n",
    "Overall, PyTorch is a great tool for deepening our understanding of deep learning and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Why PyTorch is great for deep learning research\n",
    "\n",
    "The reason for this research suitability is that Pytorch use dynamic computational graph, in contrast with tensorfow which uses static computational graph, in order to calculate derivatives.\n",
    "\n",
    "Computational graphs are used to graph the function operations that occur on tensors inside neural networks.\n",
    "These graphs are then used to compute the derivatives needed to optimize the neural network. Dynamic computational graph means that the graph is generated on the fly as the operations are created. Static graphs that are fully determined before the actual operations occur.\n",
    "\n",
    "It just so happens that many of the cutting edge research topics in deep learning are requiring or benefiting greatly from dynamic graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Installing PyTorch \n",
    "The recommended best option is to use the Anaconda Python package manager. With Anaconda, it's easy to get and manage Python, Jupyter Notebook, and other commonly used packages for scientific computing and data science, like PyTorch!\n",
    "\n",
    "Let’s go over the steps:\n",
    "\n",
    "- Download and install [Anaconda](https://www.anaconda.com/distribution/) (choose the latest Python version).\n",
    "- Go to [PyTorch's site](https://pytorch.org/) and find the get started locally section.\n",
    "- Specify the appropriate configuration options for your particular environment.\n",
    "- Run the presented command in the terminal to install PyTorch\n",
    "\n",
    "For the example: \n",
    "`conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`\n",
    "\n",
    "Notice that we are installing both PyTorch and torchvision. Also, there is no need to install CUDA separately. The needed CUDA software comes installed with PyTorch if a CUDA version is selected in step (3). All we need to do is select a version of CUDA if we have a supported Nvidia GPU on our system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:38:33.162327Z",
     "start_time": "2020-04-13T04:38:27.338652Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /Users/phucnsp/anaconda3/envs/fastai2:\r\n",
      "#\r\n",
      "# Name                    Version                   Build  Channel\r\n",
      "pytorch                   1.4.0                   py3.7_0    pytorch\r\n",
      "torchsummary              1.5.1                    pypi_0    pypi\r\n",
      "torchvision               0.5.0                  py37_cpu    pytorch\r\n"
     ]
    }
   ],
   "source": [
    "!conda list torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:38:59.319747Z",
     "start_time": "2020-04-13T04:38:59.315359Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__ # to verify pytorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:39:06.656741Z",
     "start_time": "2020-04-13T04:39:06.652363Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # to verify our GPU capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### CUDA - Why Deep Learning Uses GPUs\n",
    "To understand CUDA, we need to have a working knowledge of graphics processing units (GPUs). A GPU is a processor that is good at handling specialized computations. This is in contrast to a central processing unit (CPU), which is a processor that is good at handling general computations. CPUs are the processors that power most of the typical computations on our electronic devices.\n",
    "\n",
    "A GPU can be much faster at computing than a CPU. However, this is not always the case. The speed of a GPU relative to a CPU depends on the type of computation being performed. The type of computation most suitable for a GPU is a computation that can be done in parallel.\n",
    "\n",
    "Parallel computing is a type of computation where by a particular computation is broken into independent smaller computations that can be carried out simultaneously. The resulting computations are then recombined, or synchronized, to form the result of the original larger computation.\n",
    "\n",
    "The number of tasks that a larger task can be broken into depends on the number of cores contained on a particular piece of hardware. Cores are the units that actually do the computation within a given processor, and CPUs typically have four, eight, or sixteen cores while GPUs have potentially thousands.\n",
    "\n",
    "So why deep learning uses them - `Neural networks are embarrassingly parallel`.\n",
    "Tasks that embarrassingly parallel are ones where it’s easy to see that the set of smaller tasks are independent with respect to each other. Many of the computations that we do with neural networks can be easily broken into smaller computations in such a way that the set of smaller computations do not depend on one another. One such example is a convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Nvidia Hardware (GPU) And Software (CUDA)\n",
    "Nvidia is a technology company that designs GPUs, and they have created CUDA as a software platform that pairs with their GPU hardware making it easier for developers to build software that accelerates computations using the parallel processing power of Nvidia GPUs.\n",
    "Developers use CUDA by downloading the `CUDA toolkit`. With the toolkit comes specialized libraries like `cuDNN` - the CUDA Deep Neural Network library.\n",
    "\n",
    "With PyTorch, CUDA comes baked in from the start. There are no additional downloads required. All we need is to have a supported Nvidia GPU, and we can leverage CUDA using PyTorch. We don’t need to know how to use the CUDA API directly.\n",
    "\n",
    "After all, PyTorch is written in all of these: Python, C++, CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Suppose we have the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:52:02.092128Z",
     "start_time": "2020-04-13T04:52:02.088536Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The tensor object created in this way is on the CPU by default. As a result, any operations that we do using this tensor object will be carried out on the CPU.\n",
    "Now, to move the tensor onto the GPU, we just write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:52:20.885587Z",
     "start_time": "2020-04-13T04:52:20.882666Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = t.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This ability makes PyTorch very versatile because computations can be selectively carried out either on the CPU or on the GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: GPU Can Be Slower Than CPU. The answer is that a GPU is only faster for particular (specialized) tasks. For example, moving data from the CPU to the GPU is costly, so in this case, the overall performance might be slower if the computation task is a simple one.\n",
    "Moving relatively small computational tasks to the GPU won’t speed us up very much and may indeed slow us down. Remember, the GPU works well for tasks that can be broken into many smaller tasks, and if a compute task is already small, we won’t have much to gain by moving the task to the GPU.\n",
    "For this reason, it’s often acceptable to simply use a CPU when just starting out, and as we tackle larger more complicated problems, begin using the GPU more heavily.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Section 2: Introducing Tensors\n",
    "This section we'll talk all about tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Tensors - Data Structures of Deep Learning\n",
    "\n",
    "A tensor is the primary data structure used by neural networks. The inputs, outputs, and transformations within neural networks are all represented using tensors, and as a result, neural network programming utilizes tensors heavily.\n",
    "\n",
    "The below concepts, that we met in math or computer science, are all refered to `tensor` in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "| indexes required |  math  | computer science |\n",
    "|:----------------:|:------:|------------------|\n",
    "| 0                | scalar | number           |\n",
    "| 1                | vector | array            |\n",
    "| 2                | matrix | 2d-array         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The relationship within each of these pairs is that both elements require the same number of indexes to refer to a specific element within the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T03:56:56.843744Z",
     "start_time": "2020-04-13T03:56:56.839595Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an array or a vector requires 1 index to access its element\n",
    "a = [1,2,3,4]\n",
    "a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T03:57:42.727931Z",
     "start_time": "2020-04-13T03:57:42.723040Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an matrix or 2d-array requires 2 index to access its element\n",
    "a = [\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8]\n",
    "]\n",
    "a[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When more than two indexes are required to access a specific element, we stop giving specific names to the structures, and we begin using more general language.\n",
    "\n",
    "In mathematics, we stop using words like scalar, vector, and matrix, and we start using the word `tensor` or `nd-tensor`. The n tells us the number of indexes required to access a specific element within the structure.\n",
    "\n",
    "In computer science, we stop using words like, number, array, 2d-array, and start using the word `multidimensional array` or `nd-array`. The n tells us the number of indexes required to access a specific element within the structure.\n",
    "\n",
    "The reason we say a tensor is a generalization is because we use the word tensor for all values of n like so:\n",
    "- A scalar is a 0 dimensional tensor\n",
    "- A vector is a 1 dimensional tensor\n",
    "- A matrix is a 2 dimensional tensor\n",
    "- A nd-array is an n dimensional tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Tensors and nd-arrays are the same thing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Rank, Axes, and Shape -  fundamental tensor attributes for deep learning \n",
    "\n",
    "These concepts build on one another starting with rank, then axes, and building up to shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `rank` of a tensor refers to the number of dimensions present within the tensor. A rank-2 tensor means all of the following:\n",
    "- a matrix\n",
    "- a 2d-array\n",
    "- a 2d-tensor\n",
    "\n",
    "> Note: A tensor's rank tells us how many indexes are needed to refer to a specific element within the tensor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An `axis` of a tensor is a specific dimension of a tensor.  \n",
    "If we say that a tensor is a rank 2 tensor, we mean that the tensor has 2 dimensions, or equivalently, the tensor has two axes.\n",
    "\n",
    "The `length of each axis` tells us how many indexes are available along each axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:09:36.955138Z",
     "start_time": "2020-04-13T04:09:36.951955Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dd = [\n",
    "[1,2,3],\n",
    "[4,5,6],\n",
    "[7,8,9]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each element along the first axis, is an array:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:09:59.266138Z",
     "start_time": "2020-04-13T04:09:59.261296Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3], [4, 5, 6], [7, 8, 9])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[0], dd[1], dd[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each element along the second axis, is a number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:10:22.174494Z",
     "start_time": "2020-04-13T04:10:22.170115Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 7)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd[0][0], dd[1][0], dd[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: with tensors, the elements of the last axis are always numbers. Every other axis will contain n-dimensional arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `shape` of a tensor gives us the length of each axis of the tensor. \n",
    ">Note: The shape of a tensor is important because it encodes all of the relevant information about axes, rank, and therefore indexes. Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called `reshaping`.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T04:13:01.670957Z",
     "start_time": "2020-04-13T04:13:01.665545Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1,2,3],\n",
    "    [5,6,7]\n",
    "], dtype=torch.float)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The shape of 2 x 3 tells us that each axis of this rank two tensor has a length of 3 which means that we have three indexes available along each axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: size and shape of a tensor are the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Section 3: Pytorch Tensors\n",
    "\n",
    "PyTorch tensors are the data structures we'll be using when programming neural networks in PyTorch.\n",
    "When programming neural networks, data preprocessing is often one of the first steps in the overall process, and one goal of data preprocessing is to transform the raw input data into tensor form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### torch.Tensor class and its attributes\n",
    "PyTorch tensors are instances of the `torch.Tensor` Python class.   \n",
    "First, let’s look at a few tensor attributes. Every `torch.Tensor` has these attributes:\n",
    "- torch.dtype: \n",
    "- torch.device\n",
    "- torch.layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:38:37.279877Z",
     "start_time": "2020-04-13T01:38:37.275499Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "torch.strided\n"
     ]
    }
   ],
   "source": [
    "t = torch.Tensor()\n",
    "print(t.dtype)\n",
    "print(t.device)\n",
    "print(t.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:28:26.341835Z",
     "start_time": "2020-04-13T01:28:26.199513Z"
    },
    "hidden": true
   },
   "source": [
    "The `dtype` specifies the type of the data that is contained within the tensor.\n",
    "> Note: \n",
    "        - Each type has a CPU and GPU version\n",
    "        - Tensor operations between tensors must happen between tensors with the same type of data.\n",
    "        - Tensors contain uniform (of the same type) numerical data with one of these types: \n",
    "![pytorch datatype](data/pytorch_dtype.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `device` specifies the device (`CPU` or `GPU`) where the tensor's data is allocated. This determines where tensor computations for the given tensor will be performed.  \n",
    "PyTorch supports the use of multiple devices, and they are specified using an index like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:35:52.603803Z",
     "start_time": "2020-04-13T01:35:52.593007Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we have a device like above, we can create a tensor on the device by passing the device to the tensor’s constructor. \n",
    "> Note: tensor operations between tensors must happen between tensors that exists on the same device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `layout` specifies how the tensor is stored in memory. To learn more about stride check [here](https://en.wikipedia.org/wiki/Stride_of_an_array). For now, this is all we need to know.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Create a new tensor using data\n",
    "\n",
    "These are the primary ways of creating tensor objects (instances of the `torch.Tensor` class), with data (array-like) in PyTorch:\n",
    "1. torch.Tensor(data): is the constructor of the `torch.Tensor` class\n",
    "2. torch.tensor(data): is the `factory function` that constructs `torch.Tensor` objects. Factory functions are a software design pattern for creating objects. If you want to read more about it check [here]\n",
    "3. torch.as_tensor(data)\n",
    "4. torch.from_numpy(data)\n",
    "\n",
    "Let’s look at each of these. They all accept some form of data and give us an instance of the `torch.Tensor` class. Sometimes when there are multiple ways to achieve the same result, things can get confusing, so let’s break this down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T03:37:58.401115Z",
     "start_time": "2020-04-13T03:37:58.394648Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.array([1,2,3])\n",
    "\n",
    "o1 = torch.Tensor(data)\n",
    "o2 = torch.tensor(data)\n",
    "o3 = torch.as_tensor(data)\n",
    "o4 = torch.from_numpy(data)\n",
    "\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(o3)\n",
    "print(o4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T03:38:08.982576Z",
     "start_time": "2020-04-13T03:38:08.977416Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.get_default_dtype())\n",
    "o1.dtype == torch.get_default_dtype()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The table below compare 4 options and propose which one to use:\n",
    "\n",
    "|          method         |         which one to use       |                 dtype         | data in memory |\n",
    "|:-----------------------|:------------------------------:|:-----------------------------:|:--------------:|\n",
    "| torch.Tensor(data)      | |infer from torch default dtype, unable to pass a `dtype` to the constructor. |copy|\n",
    "| **torch.tensor(data)**    |best option to go, better doc and more config options than `torch.Tensor`|inferred from data or be explicitly set.|copy|\n",
    "| *torch.as_tensor(data)* | to-go when we want to tune for performance, better than `torch.from_numpy` because it accepts a wide variety of array-like objects including other  Pytorch tensor. |inferred from data or be explicitly set.|share|\n",
    "| torch.from_nummpy(data) | only accepts numpy.ndarray|inferred from data or be explicitly set.|share|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Data memory is shared means that the actual data in memory exists in a single place. As a result, any changes that occur in the underlying data will be reflected in both objects, the `torch.Tensor` and the `numpy.ndarray`.  \n",
    "Sharing data is more efficient and uses less memory than copying data because the data is not written to two locations in memory.\n",
    "However, there are something to keep in mind about memory sharing:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: \n",
    "        - Since `numpy.ndarray` objects are allocated on the CPU, the `as_tensor()` function must copy the data from the CPU to the GPU when a GPU is being used.\n",
    "        - The memory sharing of `as_tensor()` doesn’t work with built-in Python data structures like lists.\n",
    "        - The `as_tensor()` call requires developer knowledge of the sharing feature. This is necessary so we don’t inadvertently make an unwanted change in the underlying data without realizing the change impacts multiple objects.\n",
    "        - The `as_tensor()` performance improvement will be greater if there are a lot of back and forth operations between `numpy.ndarray` objects and tensor objects. However, if there is just a single load operation, there shouldn’t be much impact from a performance perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Create a new tensor without data\n",
    "Here are some other creation options that are available.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:46:11.612741Z",
     "start_time": "2020-04-13T01:46:11.608093Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create identity matrix\n",
    "torch.eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:46:22.493974Z",
     "start_time": "2020-04-13T01:46:22.487993Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor of zeros with the shape of specified shape argument\n",
    "torch.zeros([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:47:03.594806Z",
     "start_time": "2020-04-13T01:47:03.583642Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor of ones with the shape of specified shape argument\n",
    "torch.ones([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:47:32.090530Z",
     "start_time": "2020-04-13T01:47:32.076459Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3088, 0.4226],\n",
       "        [0.8102, 0.9129]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a tensor of random values with the shape of specified shape argument\n",
    "torch.rand([2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is a small subset of the available creation functions that don’t require data. Check with the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for the full list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Section 4: Tensor Operations\n",
    "We have the following high-level categories of tensor operations:\n",
    "- Reshaping operations: gave us the ability to position our elements along particular axes. \n",
    "- Element-wise operations: allow us to perform operations on elements between two tensors.\n",
    "- Reduction operations: allow us to perform operations on elements within a single tensor.\n",
    "- Access operations\n",
    "\n",
    "There are a lot of individual operations out there, so much so that it can sometimes be intimidating when you're just beginning, but grouping similar operations into categories based on their likeness can help make learning about tensor operations more manageable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###  Broadcasting Tensors\n",
    "\n",
    "To understand this concept, let's take a look at an example. Suppose we have the following tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:46:37.931639Z",
     "start_time": "2020-04-12T07:46:37.926107Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2]), torch.Size([2]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,1],\n",
    "    [1,1]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "t2 = torch.tensor([2,4], dtype=torch.float32)\n",
    "t1.shape, t2.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What will be the result of this element-wise addition operation, t1 + t2 ?\n",
    "\n",
    "Even though these two tenors have differing shapes, the element-wise operation is possible, and broadcasting is what makes the operation possible. The lower rank tensor t2 will be transformed via broadcasting to match the shape of the higher rank tensor t1, and the element-wise operation will be performed as usual.\n",
    "\n",
    "we can check the broadcast transformation using the `broadcast_to()` numpy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:48:23.362778Z",
     "start_time": "2020-04-12T07:48:23.350517Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 4.],\n",
       "       [2., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.broadcast_to(t2.numpy(), t1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:48:28.826126Z",
     "start_time": "2020-04-12T07:48:28.821405Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 5.],\n",
       "        [3., 5.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After broadcasting, the addition operation between these two tensors is a regular element-wise operation between tensors of the same shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###  Tensor reshape operation\n",
    "\n",
    "Reshaping operations are perhaps the most important type of tensor operations because the shape of a tensor gives us something concrete we can use to shape an intuition for our tensors.\n",
    ">Note: Reshaping changes the shape but not the underlying data elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T00:57:06.729530Z",
     "start_time": "2020-04-13T00:57:00.023118Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T00:57:16.196599Z",
     "start_time": "2020-04-13T00:57:16.153417Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2.],\n",
       "        [3., 3., 3., 3.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Other ways to flatten a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:11:03.737715Z",
     "start_time": "2020-04-13T01:11:03.731571Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:11:09.827793Z",
     "start_time": "2020-04-13T01:11:09.821945Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:11:15.044632Z",
     "start_time": "2020-04-13T01:11:15.035608Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.view(t.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Using the `reshape()` function, we can specify the `row x column` shape that we are seeking. Notice that the product of the shape's components has to be equal to the number of elements in the original tensor.\n",
    "\n",
    "Pytorch has another function called `view()` that does the same thing as `reshape` function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As neural network programmers, we have to do the same with our tensors, and usually shaping and reshaping our tensors is a frequent task.  \n",
    "Our networks operate on tensors, after all, and this is why understanding a tensor’s shape and the available reshaping operations are super important.\n",
    "\n",
    "The next way we can change the shape of our tensors is by squeezing and unsqueezing them.\n",
    "- Squeezing a tensor removes the dimensions or axes that have a length of one.\n",
    "- Unsqueezing a tensor adds a dimension with a length of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:01:10.603951Z",
     "start_time": "2020-04-13T01:01:10.592176Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([1,12]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:01:22.117373Z",
     "start_time": "2020-04-13T01:01:22.111978Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([1,12]).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:01:58.409610Z",
     "start_time": "2020-04-13T01:01:58.395995Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 12])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape([1,12]).unsqueeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let’s look at a common use case for reshaping a tensor - `flatten a tensor`.  \n",
    "A flatten operation on a tensor reshapes the tensor to have a shape that is equal to the number of elements contained in the tensor. This is the same thing as a 1d-array of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:04:44.938204Z",
     "start_time": "2020-04-13T01:04:44.925262Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "flatten operation = reshape operation + squeeze operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:05:56.847349Z",
     "start_time": "2020-04-13T01:05:56.840315Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_ex(t):\n",
    "    t = t.reshape(1, -1)\n",
    "    t = t.squeeze()\n",
    "    return t\n",
    "flatten_ex(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is possible to flatten only specific parts of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:08:34.734018Z",
     "start_time": "2020-04-13T01:08:34.727426Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "]], dtype=torch.float32)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-13T01:08:37.978364Z",
     "start_time": "2020-04-13T01:08:37.972845Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten(start_dim=1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A tensor flatten operation is a common operation inside convolutional neural networks. This is because convolutional layer outputs that are passed to fully connected layers must be flatted out before the fully connected layer will accept the input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tensor element-wise operation\n",
    "\n",
    "An element-wise operation is an operation between two tensors that operates on corresponding elements within the respective tensors. Two tensors must have the same shape in order to perform element-wise operations on them.\n",
    "\n",
    "Some common element-wise operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:50.391690Z",
     "start_time": "2020-04-12T07:49:50.388169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t1 = torch.tensor([[1,2],\n",
    "                   [3,4]], dtype=torch.float32)\n",
    "t2 = torch.tensor([[9,8],\n",
    "                   [7,6]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:50.612975Z",
     "start_time": "2020-04-12T07:49:50.607796Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10.],\n",
       "        [10., 10.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + t2 # equivalent with t1.add(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:50.835664Z",
     "start_time": "2020-04-12T07:49:50.830431Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 + 2 # equivalent with t1.add(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:51.052223Z",
     "start_time": "2020-04-12T07:49:51.046968Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  0.],\n",
       "        [ 1.,  2.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 - 2 # equivalent with t1.sub(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:51.264834Z",
     "start_time": "2020-04-12T07:49:51.259196Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 4.],\n",
       "        [6., 8.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 * 2 # equivalent with t1.mul(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:51.476863Z",
     "start_time": "2020-04-12T07:49:51.471819Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 1.0000],\n",
       "        [1.5000, 2.0000]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 / 2 # equivalent with t1.div(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Comparison Operation is element-wise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:51.909639Z",
     "start_time": "2020-04-12T07:49:51.904742Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True],\n",
       "        [False,  True, False],\n",
       "        [ True, False,  True]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:52.134464Z",
     "start_time": "2020-04-12T07:49:52.128893Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False],\n",
       "        [ True, False,  True],\n",
       "        [False,  True, False]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.gt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:52.406219Z",
     "start_time": "2020-04-12T07:49:52.401338Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.lt(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With element-wise operations that are functions, it’s fine to assume that the function is applied to each element of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:52.841982Z",
     "start_time": "2020-04-12T07:49:52.837285Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [2., 0., 2.],\n",
       "        [0., 3., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:53.060024Z",
     "start_time": "2020-04-12T07:49:53.054915Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.0000, 0.0000],\n",
       "        [1.4142, 0.0000, 1.4142],\n",
       "        [0.0000, 1.7321, 0.0000]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:53.559141Z",
     "start_time": "2020-04-12T07:49:53.554168Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0., -1., -0.],\n",
       "        [-2., -0., -2.],\n",
       "        [-0., -3., -0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:53.644620Z",
     "start_time": "2020-04-12T07:49:53.639000Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [2., 0., 2.],\n",
       "        [0., 3., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg().abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Element-wise = Component-wise = Point-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tensor reduction operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor.\n",
    "Tensors give us the ability to manage our data. The tensor can be reduced to a single scalar value or reduced along an axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let’s look at common tensor reduction operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:54.256198Z",
     "start_time": "2020-04-12T07:49:54.252620Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.tensor([\n",
    "    [0,1,0],\n",
    "    [2,0,2],\n",
    "    [0,3,0]\n",
    "], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Reducing to a tensor with a single element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:54.468754Z",
     "start_time": "2020-04-12T07:49:54.463253Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(8.), tensor(0.), tensor(0.8889), tensor(1.1667))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(), t.prod(), t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Reducing tensors By Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:57.259599Z",
     "start_time": "2020-04-12T07:49:57.253925Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 2.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Argmax` tensor reduction operation is very common in neural network.  \n",
    "This operation returns the index location of the maximum value inside a tensor.\n",
    "In practice, we often use the `argmax()` function on a network’s output prediction tensor, to determine which category has the highest prediction value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:58.048395Z",
     "start_time": "2020-04-12T07:49:58.042858Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([1., 2., 3.]),\n",
       "indices=tensor([1, 2, 1]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:58.418903Z",
     "start_time": "2020-04-12T07:49:58.414338Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tensor access operation\n",
    "\n",
    "This operation provides the ability to access data within the tensor.   \n",
    "Common tensor access operations are `item(), tolist(), numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:49:59.799175Z",
     "start_time": "2020-04-12T07:49:59.794095Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888955116272"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:50:00.018601Z",
     "start_time": "2020-04-12T07:50:00.013310Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6666666865348816, 1.3333333730697632, 0.6666666865348816]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(dim=0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T07:50:00.217873Z",
     "start_time": "2020-04-12T07:50:00.210173Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6666667, 1.3333334, 0.6666667], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(dim=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## References\n",
    "Some good sources:\n",
    "- deeplizard : https://deeplizard.com/learn/video/v5cngxo4mIg\n",
    "- effective pytorch - vahidk https://github.com/vahidk/EffectivePyTorch?fbclid=IwAR1MhsjnjccWy6dIVtibFOCZbWhLtAj5pSTobnkUDxw_gHgfEswnVzqrKQ0#torchscript  \n",
    "- recommend walk with pytorch: https://forums.fast.ai/t/getting-comfortable-with-pytorch-projects/28371\n",
    "- official tutorial: https://pytorch.org/tutorials/\n",
    "- DL(with Pytorch): https://github.com/Atcold/pytorch-Deep-Learning\n",
    "- Pytorch project template: https://github.com/moemen95/PyTorch-Project-Template\n",
    "- nlp turorial with pytorch : https://github.com/graykode/nlp-tutorial\n",
    "- UDACITY course https://www.udacity.com/course/deep-learning-pytorch--ud188\n",
    "- awesome pytorch list: https://github.com/bharathgs/Awesome-pytorch-list\n",
    "- deep learning with pytorch https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf\n",
    "- pytorch zero to all: https://github.com/hunkim/PyTorchZeroToAll\n",
    "- others:\n",
    "    - https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc\n",
    "    - grokking book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai2]",
   "language": "python",
   "name": "conda-env-fastai2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
