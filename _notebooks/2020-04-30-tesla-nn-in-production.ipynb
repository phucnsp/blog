{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Tesla uses neural network at scale in production\n",
    "> This notebook summarizes the recent spectacular talk of Andrej Kaparthy showing how Tesla is using neural network at scale in production.\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [self-learning]\n",
    "- image: images/selfdrivingcar.png\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the [Scaled Machine Learning Conference](http://scaledml.org/2020/) this year 2020, Andrej Kaparthy - Director of AI at Tesla - has given a [spectacular talk](https://www.youtube.com/watch?time_continue=1&v=hx7BXih7zx8&feature=emb_logo) about how Tesla is applying AI into their system. There are so much information and distilled knowlege came out from this talk which made me can not resist to write this blog post. If you want to access to other talks from Scaled Machine Learning Conference, go [here](https://info.matroid.com/scaledml-media-archive-preview).\n",
    "\n",
    "> Note: all the images used in this note are from the slide used in Andrej's video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The talk is about `AI for Full-Self Driving` where Andrej talked about how Tesla are improving the safety and convenience of driving, how they deploy deep learning into production and supports all the features of autopilot today, how the neural net is eating through the software stack and how they are putting vision and AI at the front and center of this effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full self-driving is a non-trivial task which requires you to not only follow the driving law but also to satisfy  massive number of users. Tesla has built their cars to be like a real computer with eyes(cameras) on it. Beside the main functions of self-driving, they also have other great functionalities such as active safely (e.g auto detect pedestrians even when self-driving mode is off) and auto parking (auto search for the parking lot).\n",
    "\n",
    "Different with other companies where Lidar is used as car's eyes, Tesla is using `vision-based` approach with cameras. The advantage of this approach is its scalable where cameras can be easily installed in millions of car. \n",
    "> Note: Briefly describe about Lidar, it shoots out the laser, create point cloud map and print out high definition map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/tesla/visionapproach.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HydraNet\n",
    "\n",
    "Using the images from cameras, Tesla AI team has built a very large network for detecting objects that their cars have to encounter on the street. As you can see the image on the left below, in order to make decision the car needs to detect a lot of objects around it such as lane lines, static objects, road signs, crosswalks, etc.\n",
    "Their huge object detection network, HydraNet, has shared backbone and multiple heads, each of them is responsible for a number of tasks.  And for each detecting task, there can be multiple subtasks come with it and if we list them out all, the number can be even thousand of tasks.\n",
    "\n",
    "![](data/tesla/hydranet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data engine and operation vacation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to boost the performance of `HydraNet`, Tesla needs a lot of data. So you have thousand of tasks to solve and each of them requires thousands of images. Wouldn't you need thousand of engineer to make it work?  \n",
    "\n",
    "> Note: Tesla has around dozens of engineer! Small team but super elite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deal with such large amount of work with not so many engineers, Andrej has introduced the concept of `operation vacation` and `data engine`.\n",
    "\n",
    "`Operation vacation` means the engineers might take vacation while the system still operate well. They have tried to develop as much automation machinery to support the development of new tasks and remove engineers from that loop. They have built infrastructure so that the labeling team, PMs can actually use that infrastructure to create new detectors whenever they have new tasks. For every new task, there will be a latency for it to actually work and they know exactly the process for getting a new task to work. It is fully automatic.\n",
    "\n",
    "Getting an example of new task detecting `caution lights`. Tesla has built a family of prototype infrastructure for different tasks. For example this `caution lights` is treated as landmark task and if it is a member of task family then all the infrastructure for it already in place so they just plug-and-play landmark prototype infrastructure and go through the `Data Engine`. \n",
    "\n",
    "![](data/tesla/op_vac_dataengine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Data Engine` is the process by which they iteractively apply active learning to source additional examples in cases detector misbehaving.   \n",
    "Breaking down steps of applying data engine for the task detecting `caution lights`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T04:28:46.333265Z",
     "start_time": "2020-04-30T04:28:46.323123Z"
    }
   },
   "outputs": [],
   "source": [
    "- Init new layer of annotation: CAUTION_LIGHT (LANDMARK)\n",
    "- Label a seed set of images\n",
    "- Develop a seed set of unit tests\n",
    "- Train network head on current data \n",
    "- While QA does not sign off:\n",
    "    - While unit tests fail:\n",
    "        - Deploy a trigger(an approximate detector trained offline) \n",
    "          to the fleet(millions of Tesla car running on the street) \n",
    "          in order to source more images in failing scenarios. \n",
    "        - Label the resulting images and incorporate them into the training set.\n",
    "        - Retrain network on data \n",
    "        - Enrich/grow set of unit tests\n",
    "    - Deploy to run on the FSD computer\n",
    "    - (Optional) Deploy to the fleet in shadow mode\n",
    "    - Collect telemetry and evaluate the performance of the feature\n",
    "- Ship feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Data Engine helps to accumulate large dataset in the full tail distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-30T03:16:02.226256Z",
     "start_time": "2020-04-30T03:16:02.222032Z"
    }
   },
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above part, we have seen the tremendous efforts that Andrej's team has been working on for growing their training set. But Tesla has also placed just as much work into massaging the test set as they do in the training set. They spent a lot of time, inspired by test-driven development, to build out very complicated test set. Their test set is treated like unit tests which has to cover all cases. This set is so important and it is the objective for the whole system improvement. Whether you are going in right or wrong direction, it all depends on the quality of the test set.\n",
    "\n",
    "A mechanism has been built for creating and massaging the test set. And this set is a growing collection thanks to the fleet running along the street and encounters different problems daily. \n",
    "\n",
    "Just reporting the loss function or the mean average precision on the test set is not enough for Tesla. Below is an example of test set for `stop sign detector` which is actually broken down into all these different issues like heavy rain/snow, heavily occluded, tilted stop signs and each of them is tracked separately and is pursued one-by-one with data engine to actually make them work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/tesla/eval_metric.png\" width=\"500\" height=\"500\"/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling: Bird's Eye View networks\n",
    "\n",
    "In this part, Andrej talked about how the neural network has to change in order to actually support full self-driving. \n",
    "\n",
    "The self driving system can not just work with raw predictions from 2d pixel space, it is needed to project them out to some kind of top-down view. A traditional approach is to create `occupancy tracker` where 2D images are projected into 3D and stitched up across cameras and then across time. This tracker will keep the temporal context  and create a small local map which helps the car winds its way thru the parking lot for example(see top left image below).    \n",
    "However, there are a lot of problems doing the stitching because these cameras are pointing in arbitrary directions and it is very hard to align them across cameras. Very difficult to develop.\n",
    "\n",
    "<img src=\"data/tesla/occupance_tracking.png\" width=\"400\" height=\"400\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So Tesla's AI team has decided to move from occupancy tracker (software 1.0 approach) to BEV Net (software 2.0 approach) and they see typically that works really well.\n",
    "\n",
    "Briefly remind about software 1.0 and software 2.0 that Andrej has many times mentioned in his previous talks. Software 1.0 is the traditional way of coding where developers directly write the code to create the software.\n",
    "Software 2.0 is the neural network-based approach where developers indirectly write the code. The optimization algorithm is the one compile data into the code. Software 2.0 is extremely useful when the problem is so messy and super hard to code. The trend is moving toward software 2.0 but each of them has their own pros and cons. The best way is still to combine them in a clever way.\n",
    "![](data/tesla/BEV.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the image above, images from cameras are fed to backbone and then going to a `Fusion layer` that stitches up the feature maps accross different view and also does the projection from image space to bird eye view. And then you have a `Temporal module` to actually smooth out these predictions. The `BEV Net` decoder actually create top-down space. That is how we can directly predict objects in top-down view from camera images. \n",
    "\n",
    "Most of the things that Andrej has described so far reply primarily on supervise learning where Tesla AI team has spent efforts to create and massage massive dataset. However, he have seen a lot of progress today in self-supervise learning so he wants to leverage some of that to speed up the process and the rate at which they learn from very little supervised data. So let's wait for Andrej's next talk with self-supervised models deployed in Tesla autopilot system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ocr]",
   "language": "python",
   "name": "conda-env-ocr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
