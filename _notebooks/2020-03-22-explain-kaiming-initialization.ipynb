{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain Kaiming Initialization\n",
    "> This notebook I will explain the Kaiming Initialization, both code and math. \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [self-learning]\n",
    "- image: images/bone.jpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **nn.init.kaiming_uniform_**\n",
    "    - this funciton implements the initialization recommendation from the sound paper [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)\n",
    "    - https://towardsdatascience.com/understand-kaiming-initialization-and-implementation-detail-in-pytorch-f7aa967e9138   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we take a brief look on notation of a standard neural network. The image below shows notation of 4 layer neural network.\n",
    "![](my_icons/neuralnet_notation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialialization method proposed in this paper was to tackle the problem of hard convergence with randomly initialized weight drawn from Gaussian distribution. Earlier, there was another paper ,`Xavier initialization`, which also tackled this problem but they only considered linear layer and did not consider non-linear layer.  \n",
    "\n",
    "I will go along part 2.2 in the original paper,`Delving Deep in Rectifiers: Surpassing Human-Level Performance on ImageNet Classification`, with some explanations in detail either in math or code.\n",
    "\n",
    "The central idea is to investigate the impact of initialization in the variance of responses in each layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each layer, the response is:\n",
    "\\begin{equation}\n",
    "\\mathbf{y}_{l}=\\mathrm{W}_{l} \\mathbf{x}_{l}+\\mathbf{b}_{l} \n",
    "\\label{eq1}\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_{l}=f\\left(\\mathbf{y}_{l-1}\\right)\n",
    "\\label{eq2}\n",
    "\\tag{2}\n",
    "\\end{equation}\n",
    "\n",
    "        To be consistent with the paper, `x` is used instead of `a` - response after activation fucntion.\n",
    "\n",
    "A few assumption about the initialization:\n",
    "- Initialized elements of $\\mathrm{W}_{l}$ and $\\mathrm{x}_{l}$ are mutually independent and share the same distribution.\n",
    "- $\\mathrm{W}_{l}$ and $\\mathrm{x}_{l}$ are independent each other. \n",
    "\n",
    "Now, let do some transformation from the function \\ref{eq1}, \\ref{eq2}\n",
    "\n",
    "Given $\\mathrm{y'}_{l}$, $\\mathrm{x'}_{l}$, $\\mathrm{w'}_{l}$ is the random variables of each element in $\\mathrm{y}_{l}$, $\\mathrm{x}_{l}$ and $\\mathrm{W}_{l}$ respectively. Then we have:\n",
    "\n",
    "\n",
    "\\begin{equation}  \n",
    "\\begin{aligned}\n",
    "\\operatorname{Var}\\left[y'_{l}\\right] &=\\operatorname{Var} \\sum_{1}^{n_{l}} \\left(w'_{l} x'_{l}\\right) \\\\\n",
    "&= \\sum_{1}^{n_{l}} \\operatorname{Var}\\left[w'_{l} x'_{l}\\right] \\\\ \n",
    "&= n_{l} \\operatorname{Var}\\left[w'_{l} x'_{l}\\right]\n",
    "\\end{aligned}\n",
    "\\label{eq3}\n",
    "\\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Intuitively explaination:  \n",
    "- for each node, such as node 1 in layer 2, its value will be sum of product of x and w. Therefore, the variance of y will be variance of sum of those products. Because all the w and x follow the same distribution (respectively), I am using a common notation $\\mathrm{w'}_{l}$$\\mathrm{x'}_{l}$ which represent those products.   \n",
    "- $\\mathrm{n}_{l-1}$ represent the number of product between $\\mathrm{w'}_{l}$ and $\\mathrm{x'}_{l}$      \n",
    "- Bias term $\\mathrm{b}$ is ignore because it usually is initialized with a constant value, so it variance is 0.\n",
    "- Because w and x are independent, so equation 3 can be transformed to equation 4.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\operatorname{Var}\\left[y'_{l}\\right] &= n_{l} \\operatorname{Var}\\left[w'_{l} x'_{l}\\right] \\\\\n",
    "&= n_{l}(\\underbrace{\\mathbb{E}\\left[{w'}_{l}^{2}\\right]}_{=\\operatorname{Var}\\left[w'_{l}\\right]} \\mathbb{E}\\left[{x'}_{l}^{2}\\right]-\\underbrace{\\mathbb{E}\\left[w'_{l}\\right]^{2}}_{=0} \\mathbb{E}\\left[x'_{l}\\right]^{2}) \\\\\n",
    "&=n_{l} \\operatorname{Var}\\left[w'_{l}\\right] \\mathbb{E}\\left[{x'}_{l}^{2}\\right]\n",
    "\\end{aligned}\n",
    "\\label{eq4}\n",
    "\\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "Intuitively explaination:  \n",
    "- assuming 2 random variables are independent, we can derive line 1 into line 2 by applying formular [wiki formular variance ](https://en.wikipedia.org/wiki/Variance#Product_of_independent_variables).\n",
    "- By assuming random variable $\\mathrm{w}_{l}$ has zero mean, we have: \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\operatorname{Var}\\left[w'_{l}\\right] &=\\mathbb{E}\\left[w'^{2}\\right]-\\mathbb{E}[w']^{2} \\\\\n",
    "&=\\mathbb{E}\\left[w'^{2}\\right]\n",
    "\\end{aligned}\n",
    "\\label{eq5}\n",
    "\\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "But $\\mathbb{E}[x']^{2} \\neq \\operatorname{Var}\\left[x'\\right]$ because $\\mathrm{E}[x']$ does not have zero mean, it is the result of ReLU function, $x_{l}=\\max \\left(0, y_{l-1}\\right)$, from previous layer. \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[{x'}_{l}^{2}\\right] &=\\mathbb{E}\\left[\\max \\left(0, y'_{l-1}\\right)^{2}\\right] \\\\\n",
    "&=\\frac{1}{2} \\mathbb{E}\\left[{y'}_{l-1}^{2}\\right] \\\\\n",
    "&=\\frac{1}{2} \\operatorname{Var}\\left[y'_{l-1}\\right]\n",
    "\\end{aligned}\n",
    "\\label{eq6}\n",
    "\\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "Intuitively explaination:  \n",
    "- Assuming $\\mathrm{w}_{l-1}$ has a symmetric distribution around 0 and $\\mathrm{b}_{l-1}$ = 0 then $\\mathrm{y}_{l-1}$ has zero mean and symmetric distribution around 0 => that's why we can derive equation \\ref{eq6}.\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left(y_{l-1}\\right) &=\\mathbb{E}\\left(w_{l-1} x_{l-1}\\right) \\\\\n",
    "&=\\mathbb{E}\\left(w_{l-1}\\right) \\mathbb{E}\\left(x_{l-1}\\right) \\\\\n",
    "&=0\n",
    "\\end{aligned}\n",
    "\\label{eq7}\n",
    "\\tag{7}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathbb{P}\\left(y_{l-1}>0\\right) &=\\mathbb{P}\\left(w_{l-1} x_{l-1}>0\\right) \\\\\n",
    "&=\\mathbb{P}\\left(\\left(w_{l-1}>0 \\text { and } x_{l-1}>0\\right) \\text { or }\\left(w_{l-1}<0 \\text { and } x_{l-1}<0\\right)\\right) \\\\\n",
    "&=\\mathbb{P}\\left(w_{l-1}>0\\right) \\mathbb{P}\\left(x_{l-1}>0\\right)+\\mathbb{P}\\left(w_{l-1}<0\\right) \\mathbb{P}\\left(x_{l-1}<0\\right) \\\\\n",
    "&=\\frac{1}{2} \\mathbb{P}\\left(x_{l-1}>0\\right)+\\frac{1}{2} \\mathbb{P}\\left(x_{l-1}<0\\right) \\\\\n",
    "&=\\frac{1}{2}\n",
    "\\end{aligned}\n",
    "\\label{eq8}\n",
    "\\tag{8}\n",
    "\\end{equation}\n",
    "\n",
    "Plugging back to equation \\ref{eq4} we have:\n",
    "\\begin{equation}\n",
    "\\operatorname{Var}\\left[y_{l}\\right]=\\frac{1}{2} n_{l} \\operatorname{Var}\\left[w_{l}\\right] \\operatorname{Var}\\left[y_{l-1}\\right]\n",
    "\\label{eq9}\n",
    "\\tag{9}\n",
    "\\end{equation}\n",
    "\n",
    "With L layers put together, we have:\n",
    "\\begin{equation}\n",
    "\\operatorname{Var}\\left[y_{L}\\right]=\\operatorname{Var}\\left[y_{1}\\right]\\left(\\prod_{l=2}^{L} \\frac{1}{2} n_{l} \\operatorname{Var}\\left[w_{l}\\right]\\right)\n",
    "\\label{eq10}\n",
    "\\tag{10}\n",
    "\\end{equation}\n",
    "\n",
    "From here, the layer part on paper is quite clear, equation \\ref{eq10} is the key to the initialization design. A proper initialization method should avoid reducing or magnifying the magnitudes of input signals exponentially. So we expect the above product to take a proper scalar, eg 1. A sufficient condition is:\n",
    "\\begin{equation}\n",
    "\\frac{1}{2} n_{l} \\operatorname{Var}\\left[w_{l}\\right]=1, \\quad \\forall l\n",
    "\\label{eq11}\n",
    "\\tag{11}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\Rightarrow \\operatorname{Var}\\left[w_{l}\\right] = \\frac{2}{n_{l}} \\\\\n",
    "\\Rightarrow \\mathbb{E}\\left[w_{l}\\right] = \\sqrt{\\frac{2}{n_{l}}}\n",
    "\\end{aligned}\n",
    "\\label{eq12}\n",
    "\\tag{12}\n",
    "\\end{equation}\n",
    "\n",
    "Equation \\ref{12} is the He. initialilzation, together with b=0. \n",
    "\n",
    "Note: from equation \\ref{eq6} we can see that if previous layer is not ReLU-kind, such as the first layer, we will have $\\mathbb{E}\\left[{x'}_{l}^{2}\\right] =\\operatorname{Var}\\left[y'_{l-1}\\right]$ then $\\mathbb{E}\\left[w_{l}\\right] = \\sqrt{\\frac{1}{n_{l}}}$. But the factor 1/2 here does not matter if it just exists on one layer. So we adopt equation \\ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd  \n",
    "https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/  \n",
    "https://medium.com/a-paper-a-day-will-have-you-screaming-hurray/day-8-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification-f449a886e604"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **trace of matrix**: deep learning page 44\n",
    "- **expectation, variance and covariance**: deep learning page 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('fastaixxx': conda)",
   "language": "python",
   "name": "python37664bitfastaixxxconda5491a1ef6f2042618623e5178030a047"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
