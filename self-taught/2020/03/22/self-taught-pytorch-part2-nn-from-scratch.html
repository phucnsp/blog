<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Pytorch part 2 - neural net from scratch | Phuc Ng. Su</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Pytorch part 2 - neural net from scratch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This notebook will create and train a very simple model from scratch and then gradually refactor it using built-in pytorch modules." />
<meta property="og:description" content="This notebook will create and train a very simple model from scratch and then gradually refactor it using built-in pytorch modules." />
<link rel="canonical" href="https://phucnsp.github.io/blog/self-taught/2020/03/22/self-taught-pytorch-part2-nn-from-scratch.html" />
<meta property="og:url" content="https://phucnsp.github.io/blog/self-taught/2020/03/22/self-taught-pytorch-part2-nn-from-scratch.html" />
<meta property="og:site_name" content="Phuc Ng. Su" />
<meta property="og:image" content="https://phucnsp.github.io/blog/images/pytorch_ava.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-22T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"This notebook will create and train a very simple model from scratch and then gradually refactor it using built-in pytorch modules.","@type":"BlogPosting","headline":"Pytorch part 2 - neural net from scratch","dateModified":"2020-03-22T00:00:00-05:00","datePublished":"2020-03-22T00:00:00-05:00","image":"https://phucnsp.github.io/blog/images/pytorch_ava.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://phucnsp.github.io/blog/self-taught/2020/03/22/self-taught-pytorch-part2-nn-from-scratch.html"},"url":"https://phucnsp.github.io/blog/self-taught/2020/03/22/self-taught-pytorch-part2-nn-from-scratch.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://phucnsp.github.io/blog/feed.xml" title="Phuc Ng. Su" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Pytorch part 2 - neural net from scratch | Phuc Ng. Su</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Pytorch part 2 - neural net from scratch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This notebook will create and train a very simple model from scratch and then gradually refactor it using built-in pytorch modules." />
<meta property="og:description" content="This notebook will create and train a very simple model from scratch and then gradually refactor it using built-in pytorch modules." />
<link rel="canonical" href="https://phucnsp.github.io/blog/self-taught/2020/03/22/self-taught-pytorch-part2-nn-from-scratch.html" />
<meta property="og:url" content="https://phucnsp.github.io/blog/self-taught/2020/03/22/self-taught-pytorch-part2-nn-from-scratch.html" />
<meta property="og:site_name" content="Phuc Ng. Su" />
<meta property="og:image" content="https://phucnsp.github.io/blog/images/pytorch_ava.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-03-22T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"This notebook will create and train a very simple model from scratch and then gradually refactor it using built-in pytorch modules.","@type":"BlogPosting","headline":"Pytorch part 2 - neural net from scratch","dateModified":"2020-03-22T00:00:00-05:00","datePublished":"2020-03-22T00:00:00-05:00","image":"https://phucnsp.github.io/blog/images/pytorch_ava.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://phucnsp.github.io/blog/self-taught/2020/03/22/self-taught-pytorch-part2-nn-from-scratch.html"},"url":"https://phucnsp.github.io/blog/self-taught/2020/03/22/self-taught-pytorch-part2-nn-from-scratch.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://phucnsp.github.io/blog/feed.xml" title="Phuc Ng. Su" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Phuc Ng. Su</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Pytorch part 2 - neural net from scratch</h1><p class="page-description">This notebook will create and train a very simple model from scratch and then gradually refactor it using built-in pytorch modules.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-03-22T00:00:00-05:00" itemprop="datePublished">
        Mar 22, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      28 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#self-taught">self-taught</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/phucnsp/blog/tree/master/_notebooks/2020-03-22-self-taught-pytorch-part2-nn-from-scratch.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/phucnsp/blog/master?filepath=_notebooks%2F2020-03-22-self-taught-pytorch-part2-nn-from-scratch.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/phucnsp/blog/blob/master/_notebooks/2020-03-22-self-taught-pytorch-part2-nn-from-scratch.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Section-1:-data-and-data-processing">Section 1: data and data processing </a></li>
<li class="toc-entry toc-h2"><a href="#Section-2:-create-and-train-model-(from-scratch)">Section 2: create and train model (from scratch) </a></li>
<li class="toc-entry toc-h2"><a href="#Section-3:-refactor-model-using-Pytorch-built-in-modules">Section 3: refactor model using Pytorch built-in modules </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Refactor-stage-1">Refactor stage 1 </a></li>
<li class="toc-entry toc-h3"><a href="#Refactor-stage-2">Refactor stage 2 </a></li>
<li class="toc-entry toc-h3"><a href="#Refactor-stage-3">Refactor stage 3 </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Section-4:-random-topics">Section 4: random topics </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Disabling-PyTorch-Gradient-Tracking">Disabling PyTorch Gradient Tracking </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Reference">Reference </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-03-22-self-taught-pytorch-part2-nn-from-scratch.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In <a href="https://phucnsp.github.io/blog/self-taught/2020/03/22/self-taught-pytorch-part1-tensor.html">part 1</a> of this serie, we have gone through the basic elements of neural network. In this part we will start writing a program.</p>
<p>Computer programs in general consist of two primary components, <code>code</code> and <code>data</code>. With traditional programming, the programmer’s job is to explicitly write the software or code to perform computations. But with deep learning and neural networks, this job explicitly belongs to the optimization algorithm. It will compile our data into code which is actually neural net's weights. The programmer’s job is to oversee and guide the learning process though training. We can think of this as an indirect way of writing software or code.</p>
<p>In reality, creating and trainning model is just one of the stages in a fullscale machine learning project. There are 4 fundamental stages that a machine learning project need to have:</p>
<ol>
<li>
<strong>Project planning and project setup:</strong> gather team, define requirements, goals and allocate resources.</li>
<li>
<strong>Data collection and labelling:</strong> define which data to collect and label them.</li>
<li>
<strong>Training and debugging:</strong> start implementing, debugging and improving model. This notebook will focus on this stage where we will create and train a simple model using Pytorch framework. </li>
<li>
<strong>Deploying and testing</strong> write tests to prevent regresison, roll out in production.
<div class="flash flash-warn">
    <svg class="octicon octicon-zap octicon octicon-zap octicon octicon-zap" viewbox="0 0 10 16" version="1.1" width="10" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10 7H6l3-7-9 9h4l-3 7 9-9z"></path></svg>
    <strong>Important: </strong>it is worth to note that machine learning project does not fit well with either waterfall or agile workflow. It is somewhere in between them and the world is in progress to figure out a new workflow for it. However, it can be sure that this type of project needs to be highly iteractive and flexible. We will try to cover this topic more detail in later parts of this serie.
</div>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So now we will go to the main topic today.<br>
There are 3 steps that we need to iteractively tackle during the training and debugging stage:</p>
<ul>
<li>
<strong>data and data processing:</strong> data augmentation, data transformation, data cleaning, etc</li>
<li>
<strong>create and train model</strong> guide the optimization algorithm toward the right direction.</li>
<li>
<strong>debug and improve:</strong> analyse model's results to see where might need to improve.</li>
</ul>
<p>Debugging machine learning is always a hot topic and hard to digest, we will cover it in a separate notebook. Today we will talk about data and model, but not too fast. In order to fully understand exactly what and how things are doing, we will create and train a very basic neural network from scratch which initially only use the most basic Pytorch tensor functionality and gradually refactor it using Pytorch built-in modules.</p>
<p>Remind the most fundamental Pytorch modules which we will repeatly work with along the way.</p>
<table>
<thead>
<tr>
<th style="text-align:left">Package</th>
<th style="text-align:left">Description                                                                                                                                                                 </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">torch</td>
<td style="text-align:left">The top-level PyTorch package and tensor library.</td>
</tr>
<tr>
<td style="text-align:left">torch.nn</td>
<td style="text-align:left">A subpackage that contains modules and extensible classes for building neural networks.</td>
</tr>
<tr>
<td style="text-align:left">torch.autograd</td>
<td style="text-align:left">A subpackage that supports all the differentiable Tensor operations in PyTorch.</td>
</tr>
<tr>
<td style="text-align:left">torch.nn.functional</td>
<td style="text-align:left">A functional interface that contains operations used for building neural net like loss, activation, layer operations...</td>
</tr>
<tr>
<td style="text-align:left">torch.optim</td>
<td style="text-align:left">A subpackage that contains standard optimization operations like SGD and Adam.</td>
</tr>
<tr>
<td style="text-align:left">torch.utils</td>
<td style="text-align:left">A subpackage that contains utility classes like data sets and data loaders that make data preprocessing easier.</td>
</tr>
<tr>
<td style="text-align:left">torchvision</td>
<td style="text-align:left">A package that provides access to popular datasets, models, and image transformations for computer vision.</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Section-1:-data-and-data-processing">
<a class="anchor" href="#Section-1:-data-and-data-processing" aria-hidden="true"><span class="octicon octicon-link"></span></a>Section 1: data and data processing<a class="anchor-link" href="#Section-1:-data-and-data-processing"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Data is the primary ingredient of deep learning. Before feeding data into our network, we need to consider many aspects such as:</p>
<ul>
<li>Who created the dataset?</li>
<li>How was the dataset created?</li>
<li>What transformations were used?</li>
<li>What intent does the dataset have?</li>
<li>Possible unintentional consequences?</li>
<li>Is the dataset biased?</li>
<li>Are there ethical issues with the dataset?</li>
</ul>
<p>In this tutorial, we will use the well-prepared <code>Fashion-MNIST</code> dataset which was created by research lab of Zalando - a German based multi-national fashion commerce company. The dataset was designed to mirror the original MNIST dataset as closely as possible while introducing higher difficulty in training due to simply having more complex data than hand written images. The abstract from its <a href="https://arxiv.org/abs/1708.07747">paper</a>:</p>
<p><code>We present Fashion-MNIST, a new dataset comprising of 28 × 28 grayscale images of 70, 000 fashion products from 10 categories, with 7, 000 images per category. The training set has 60, 000 images and the test set has 10, 000 images. Fashion-MNIST is intended to serve as a direct dropin replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist.</code></p>
<p>The Fashion-MNIST was built, unlike the hand-drawn MNIST dataset, from actual images on Zalando’s website. However, they have been transformed to more closely correspond to the MNIST specifications. This is the general conversion process that each image from the site went through:</p>
<ul>
<li>Converted to PNG</li>
<li>Trimmed</li>
<li>Resized</li>
<li>Sharpened</li>
<li>Extended</li>
<li>Negated</li>
<li>Gray-scaled</li>
</ul>
<p>The dataset has the following ten classes of fashion items:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">idx2clas</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s2">"T-shirt/top"</span><span class="p">,</span>
            <span class="mi">1</span> <span class="p">:</span> <span class="s2">"Trouser"</span><span class="p">,</span>
            <span class="mi">2</span> <span class="p">:</span> <span class="s2">"Pullover"</span><span class="p">,</span>
            <span class="mi">3</span> <span class="p">:</span> <span class="s2">"Dress"</span><span class="p">,</span>
            <span class="mi">4</span> <span class="p">:</span> <span class="s2">"Coat"</span><span class="p">,</span>
            <span class="mi">5</span> <span class="p">:</span> <span class="s2">"Sandal"</span><span class="p">,</span>
            <span class="mi">6</span> <span class="p">:</span> <span class="s2">"Shirt"</span><span class="p">,</span>
            <span class="mi">7</span> <span class="p">:</span> <span class="s2">"Sneaker"</span><span class="p">,</span>
            <span class="mi">8</span> <span class="p">:</span> <span class="s2">"Bag"</span><span class="p">,</span>
            <span class="mi">9</span> <span class="p">:</span> <span class="s2">"Ankle boot"</span><span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A sample of the items look like this:</p>
<p><img src="/blog/images/copied_from_nb/data/sample_fashion_mnist.png" alt="sample_fashition_mnist"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>That's enough background information about the dataset. Now we will prepare data for our network.</p>
<p>The general idea of this step is to transform our dataset into tensor format so we can take advantages of GPU's parallel computing for later steps such as data augmentation, training model, etc.</p>
<p>We'll follow the ETL process to prepare data:</p>
<ul>
<li>Extract: Get the Fashion-MNIST image data from the source.  </li>
<li>Transform: Put our data into tensor form.  </li>
<li>Load: Put our data into an object to make it easily accessible.  </li>
</ul>
<p>The Fashion-MNIST source code can be accessed <a href="https://github.com/zalandoresearch/fashion-mnist">here</a>. We will use <code>pathlib</code> for dealing with paths and will download 4 parts - <code>training set images, training set labels, test set images and test set labels</code> - using <code>requests</code> library. Since this dataset has been stored using pickle, a python-specific format for serializing data, we need to unzip and deserialize it in order to read the content.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># a. extract data from source</span>

<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">PATH_ROOT</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">"data/fashion_mnist"</span><span class="p">)</span>
<span class="n">PATH_ROOT</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">URLs</span> <span class="o">=</span> <span class="p">[</span>
<span class="s2">"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz"</span><span class="p">,</span>
<span class="s2">"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz"</span><span class="p">,</span>
<span class="s2">"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz"</span><span class="p">,</span>
<span class="s2">"http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz"</span>
<span class="p">]</span>

<span class="k">def</span> <span class="nf">download_data</span><span class="p">(</span><span class="n">path_root</span><span class="p">,</span> <span class="n">url</span><span class="p">):</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">name</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">path_root</span> <span class="o">/</span> <span class="n">filename</span><span class="p">)</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
        <span class="n">content</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">content</span>
        <span class="p">(</span><span class="n">path_root</span> <span class="o">/</span> <span class="n">filename</span><span class="p">)</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">"wb"</span><span class="p">)</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">content</span><span class="p">)</span>
        
<span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">URLs</span><span class="p">:</span> <span class="n">download_data</span><span class="p">(</span><span class="n">PATH_ROOT</span><span class="p">,</span> <span class="n">url</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">'train'</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">os</span>
    <span class="kn">import</span> <span class="nn">gzip</span>
    <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

    <span class="sd">"""Load MNIST data from `path`"""</span>
    <span class="n">labels_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span>
                               <span class="s1">'</span><span class="si">%s</span><span class="s1">-labels-idx1-ubyte.gz'</span>
                               <span class="o">%</span> <span class="n">kind</span><span class="p">)</span>
    <span class="n">images_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span>
                               <span class="s1">'</span><span class="si">%s</span><span class="s1">-images-idx3-ubyte.gz'</span>
                               <span class="o">%</span> <span class="n">kind</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">labels_path</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">lbpath</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">lbpath</span><span class="o">.</span><span class="n">read</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
                               <span class="n">offset</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">gzip</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">images_path</span><span class="p">,</span> <span class="s1">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">imgpath</span><span class="p">:</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">frombuffer</span><span class="p">(</span><span class="n">imgpath</span><span class="o">.</span><span class="n">read</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">,</span>
                               <span class="n">offset</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">),</span> <span class="mi">784</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">images</span><span class="p">,</span> <span class="n">labels</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">PATH_ROOT</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">'train'</span><span class="p">)</span>
<span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">PATH_ROOT</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">'t10k'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>I am using the word <code>valid</code> and <code>test</code> interchanged but in reality they are different. 
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This dataset is in numpy array format. Each image is <code>28*28</code> and is being stored as a flattened row of length 784.<br>
Let's reshape and take a look at one.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((60000, 784), (60000,))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_train</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">y_train</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(9, 0)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x_valid</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_valid</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>((10000, 784), (10000,))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_valid</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">y_valid</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(9, 0)</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s2">"gray"</span><span class="p">)</span>
<span class="n">idx2clas</span><span class="p">[</span><span class="n">y_train</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> 
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>'Ankle boot'</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So we have finished the <code>extract</code> data step and now we will to to <code>transform</code> step. In the context of image, there are 2 basic transform steps are <code>convert to tensor</code> and <code>normalization</code>. <code>Normalization</code> is a standard step in image processing which helps faster convergence.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="c1"># transform 1. convert to tensor</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_valid</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_valid</span><span class="p">))</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span> <span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># transform 2. normalize images</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">std</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"mean: </span><span class="si">{</span><span class="n">mean</span><span class="si">}</span><span class="s2">, std: </span><span class="si">{</span><span class="n">std</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_valid</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="p">(</span><span class="n">p</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span><span class="o">/</span><span class="n">std</span><span class="p">,</span> <span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_valid</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>mean: 72.9342041015625, std: 90.02118682861328
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x_train</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">x_train</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor(-4.0474e-07), tensor(1.0000))</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x_valid</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">x_valid</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor(0.0023), tensor(0.9984))</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash flash-warn">
    <svg class="octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap" viewbox="0 0 10 16" version="1.1" width="10" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10 7H6l3-7-9 9h4l-3 7 9-9z"></path></svg>
    <strong>Important: </strong>we need to set dtype=torch.float32 in order to be able to make matrix multiplication later on with  linear layer’s weight matrix. Two tensor have to have the same datatype and device so as to do operations.
</div>
And finally, <code>load</code> data into an object to make it easily accessible. This task normally is handled by Pytorch <code>DataLoader</code> object but since we are building everything from scratch, let's use the traditional <code>for loop</code> to access batches of data.

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># c. load batches of data</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">nr_iters</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">nr_iters</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
    <span class="n">start_index</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">batch_size</span>
    <span class="n">end_index</span> <span class="o">=</span> <span class="n">start_index</span> <span class="o">+</span> <span class="n">batch_size</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span><span class="n">end_index</span><span class="p">]</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span><span class="n">end_index</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">xs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">ys</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Section-2:-create-and-train-model-(from-scratch)">
<a class="anchor" href="#Section-2:-create-and-train-model-(from-scratch)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Section 2: create and train model (from scratch)<a class="anchor-link" href="#Section-2:-create-and-train-model-(from-scratch)"> </a>
</h2>
<p>For our 10-class classification problem, we will create a simple network which contains only a linear layer and a non-linear layer - softmax.</p>
<p>In general, a layer contains 2 parts:</p>
<ul>
<li>
<code>data</code>: represents the state of that layer. In particular, they are weight and bias - learnable parameters which are updated/learned during training process. </li>
<li>
<code>transformation</code>: the operation which transform layer's input to output using learnable parameters.</li>
</ul>
<p><code>Linear layers</code>'s data is <code>weight matrix tensor</code> and <code>bias tensor</code> while its transformation is the <code>matrix multiplication</code>. Weight matrix defines the linear function that maps a 1-dimentional tensor with 784 elements to a 1-dimensional tensor with 10 elements. 
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>Briefly remind the mathematical function of linear layer. Given $A$, $x$, $b$, $y$ are <code>Weight matrix tensor</code>, <code>Input tensor</code>, <code>Bias tensor</code> and <code>Output tensor</code>, respectively. Mathematical notation of a linear transformation is: $y=Ax+b$
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The weight matrix tensor will be initialized following the recommendation from <a href="https://arxiv.org/abs/1502.01852">Xavier initialisation paper</a>. This paper tackled the problem with randomly initialized weight drawn from Gaussian distribution which caused hard convergence for deep network.
</p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M16 8.5l-6 6-3-3L8.5 10l1.5 1.5L14.5 7 16 8.5zM5.7 12.2l.8.8H2c-.55 0-1-.45-1-1V3c0-.55.45-1 1-1h7c.55 0 1 .45 1 1v6.5l-.8-.8c-.39-.39-1.03-.39-1.42 0L5.7 10.8a.996.996 0 000 1.41v-.01zM4 4h5V3H4v1zm0 2h5V5H4v1zm0 2h3V7H4v1zM3 9H2v1h1V9zm0-2H2v1h1V7zm0-2H2v1h1V5zm0-2H2v1h1V3z"></path></svg>
    <strong>Tip: </strong>we set <code>requires_grad_</code> after initialization, since we don’t want that step included in the gradident. The trailing <code>_</code> in Pytorch signifies that the operation is performed in-place.
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">784</span><span class="p">)</span>
<span class="n">weights</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">bias</span>    <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Thanks to Pytorch's ability to calculate gradients automatically, we can use any standard Python function (or callable object) as a model.<br>
The log_softmax function is implemented using <code>log-sum-exp trick</code> for numerically stable. We will not go to detail this trick but you can go <a href="https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/">here</a> or <a href="https://www.tensorflow.org/api_docs/python/tf/nn/log_softmax">here</a> for details explanation. The formular for this trick is: 
$$ log\_softmax(x) = x - logsumexp(x) $$</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="n">x</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">simplenet</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash flash-success">
    <svg class="octicon octicon-checklist octicon octicon-checklist octicon octicon-checklist" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M16 8.5l-6 6-3-3L8.5 10l1.5 1.5L14.5 7 16 8.5zM5.7 12.2l.8.8H2c-.55 0-1-.45-1-1V3c0-.55.45-1 1-1h7c.55 0 1 .45 1 1v6.5l-.8-.8c-.39-.39-1.03-.39-1.42 0L5.7 10.8a.996.996 0 000 1.41v-.01zM4 4h5V3H4v1zm0 2h5V5H4v1zm0 2h3V7H4v1zM3 9H2v1h1V9zm0-2H2v1h1V7zm0-2H2v1h1V5zm0-2H2v1h1V3z"></path></svg>
    <strong>Tip: </strong><code>@</code> stand for dot product operation.
</div>
Now we have had a simple network and data setup. Let's try to predict a batch.

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[:</span><span class="n">bs</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="n">bs</span><span class="p">]</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">simplenet</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">preds</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([16, 10])</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-1.4405, -3.5105, -2.7565, -1.4291, -2.4875, -2.0924, -4.0225, -4.0119,
         -2.5381, -2.2188],
        [-4.8259, -3.7734, -2.0315, -2.3298, -0.9287, -2.6093, -3.6926, -2.2518,
         -1.9918, -5.2320],
        [-2.6215, -2.3225, -1.9596, -1.8593, -2.6342, -2.3268, -1.6219, -2.6988,
         -2.7874, -3.3023],
        [-3.2222, -3.1183, -1.6531, -1.5975, -2.2923, -2.7484, -1.7113, -2.3899,
         -2.6972, -4.0560],
        [-4.0472, -4.3383, -2.3563, -1.5838, -0.7511, -2.6288, -3.0189, -3.5576,
         -3.2544, -4.6537],
        [-2.2369, -3.2984, -0.9843, -2.8722, -2.3741, -1.8116, -4.1416, -5.1253,
         -2.9522, -2.3493],
        [-1.8748, -4.8596, -2.7152, -1.2111, -3.2741, -2.0744, -2.4669, -2.8398,
         -2.7450, -2.2655],
        [-2.7887, -4.9238, -1.3429, -3.5693, -2.6362, -1.4553, -5.4066, -6.6005,
         -3.2211, -1.2336],
        [-3.9236, -2.5255, -2.9410, -0.6889, -4.0574, -3.9685, -2.6249, -1.6157,
         -4.2080, -3.7726],
        [-1.7502, -1.8132, -1.7902, -1.9453, -3.3830, -3.7103, -2.6871, -3.1987,
         -2.2021, -2.5852],
        [-3.7683, -3.3224, -2.6373, -2.0115, -0.7844, -2.8311, -2.6774, -2.6067,
         -2.6543, -4.9239],
        [-3.2636, -3.2367, -1.4613, -2.2395, -2.4437, -2.0885, -2.9661, -3.5086,
         -1.3777, -3.2262],
        [-1.4874, -4.1664, -2.1750, -1.5891, -3.1738, -2.5524, -2.2160, -3.7046,
         -2.8008, -2.0662],
        [-2.6747, -3.6151, -2.5766, -1.4708, -3.7484, -3.1546, -1.2979, -3.1983,
         -2.5499, -1.9650],
        [-2.2535, -4.0095, -3.1679, -0.7156, -3.5641, -2.4768, -2.4170, -3.0718,
         -3.2315, -2.8394],
        [-1.1524, -4.3791, -3.3212, -1.0543, -3.4922, -2.1228, -4.1253, -3.9749,
         -2.9178, -3.0380]], grad_fn=&lt;SubBackward0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>What we have done is one <code>forward pass</code>, we load a batch of image add feed it through the network. The result will not be better than a random prediction at this stage because we start with random weights.</p>
<p>It can be seen in the <code>preds</code> tensor that it contains not only the tensor values but also a gradient function. As mentioned in part 1, pytorch use dynamic computational graph to track function operations that occur on tensors. These graph are then used to compute the derivatives.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preds</span><span class="o">.</span><span class="n">grad_fn</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;SubBackward0 at 0x12bde0310&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we need to define the loss function which is the model's objective. Our weights and bias will be updated in the direction which make this loss decreased. One of the most common loss function is <code>negative log-likelihood</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">nll</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="nb">input</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">target</span><span class="o">.</span><span class="n">tolist</span><span class="p">()]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">loss_function</span> <span class="o">=</span> <span class="n">nll</span>

<span class="n">loss_function</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(2.9355, grad_fn=&lt;NegBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, We will define a <code>metric</code>. During the training, reducing the <code>loss</code> is what our model tries to do but it is hard for us, as human, can intuitively understand how good the weights set are along the way. So we need a human-interpretable value which help us understand the training progress and it is the <code>metric</code>.<br>
</p>
<div class="flash flash-warn">
    <svg class="octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap" viewbox="0 0 10 16" version="1.1" width="10" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10 7H6l3-7-9 9h4l-3 7 9-9z"></path></svg>
    <strong>Important: </strong>while training your model, there will be the case when your loss has stopped decreasing but your accuracy is still increasing. The recommendation here is to save both models, one at minimum loss and one at maximum accuracy. And you, yourself, need to make decision which one to choose. For me, it is always the maximum accuracy because the accuracy measurement is what we care about.
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span> 
    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(0.1250)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We are now ready to begin the training process. The training process is an iterative process which including following steps:</p>
<ol>
<li>
<strong>Get a batch from the training set</strong>.  <ul>
<li>Since we have 60,000 samples in our training set, we will have 938 iterations with batch_size 64. Something to notice, batch_size will directly impact to the number of times the weights updated. In our case, the weights will be updated 938 times by the end of each loop. So far, there is no rule-of-thump for selecting the value of batch size so we still need to do trial and error to figure out the best value.
<div class="flash flash-warn">
    <svg class="octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap" viewbox="0 0 10 16" version="1.1" width="10" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M10 7H6l3-7-9 9h4l-3 7 9-9z"></path></svg>
    <strong>Important: </strong>to be simple, I am not shuffling the training set at this stage. In reality, the training set should be shuffled to prevent correlation between batches and overfitting. If we keep feeding the network batch-by-batch in an exact order many times, the network might remember this order and causes overfitting with it. On the other hand, the validation loss will be identical whether we shuffle the validation set or not. Since shuffling takes extra time, it makes no sense to shuffle the validation data. 
</div>    </li>
</ul>
</li>
<li>
<p><strong>Pass batch to network.</strong></p>
</li>
<li>
<p><strong>Calculate the loss value.</strong></p>
</li>
<li>
<p><strong>Calculate the gradient of the loss function w.r.t the network's weights.</strong></p>
<ul>
<li>Calculating the gradients is very easy using PyTorch. Since PyTorch has created a computation graph under the hood. As our batch tensor steps forward through our network, all the computations are recorded in the computational graph. And this graph is then used by PyTorch to calculate the gradients of the loss function with respect to the network's weights.</li>
</ul>
</li>
<li>
<p><strong>Update the weights.</strong></p>
<ul>
<li>The gradients calculated from step 4 are used by the optimizer to update the respective weights. </li>
<li>We have disabled PyTorch gradient tracking at this step because we don't want these actions to be recorded for our next calculation of the gradient. There are many ways to disable this functionality, please check <code>Random topics</code> at the end of notebook for more information.</li>
<li>After updating the weight, we need to zero out the gradients because the gradients will be calculated and added to the grad attributes of our network's parameters after calling <code>loss.backward()</code> at the next iteration.<br>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>Zero out the gradient after updating parameters is not always the case, there are some special cases where we want to <code>accumulate gradient</code>. But you only have to deal with it at advance level. So take care^^.
</div>
</li>
</ul>
</li>
<li>
<p><strong>Repeat steps 1-5 until one epoch is completed.</strong></p>
</li>
<li>
<p><strong>Calculate mean loss of validation set</strong>
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>We can use a batch size for the validation set that is twice as large as that for the training set. This is because the validation set does not need backpropagation and thus takes less memory (it doesn’t need to store the gradients). We take advantage of this to use a larger batch size and compute the loss more quickly.
</div>
</li>
<li><strong>Repeat steps 1-6 for as many epochs required to reach the minimum loss.</strong></li>
</ol>
<p>We will use <code>Stochastic Gradient Descent (SGD)</code> optimizer to update our learnable parameters during training. <code>lr</code> tells the optimizer how far to step in the direction of minimizing loss function.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>    
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>  
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>     

<span class="n">nr_iters</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">nr_iters</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
        <span class="c1"># step 1. get batch of training set</span>
        <span class="n">start_index</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">batch_size</span>
        <span class="n">end_index</span> <span class="o">=</span> <span class="n">start_index</span> <span class="o">+</span> <span class="n">batch_size</span>
        <span class="n">xs</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span><span class="n">end_index</span><span class="p">]</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span><span class="n">end_index</span><span class="p">]</span>
        <span class="c1"># step 2. pass batch to network</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">simplenet</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
        <span class="c1"># step 3. calculate the loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
        <span class="c1"># step 4. calculate the gradient of the loss w.r.t the network's parameters</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> 
            <span class="c1"># step 5. update the weights using SGD algorithm</span>
            <span class="n">weights</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">weights</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">bias</span>    <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">bias</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">weights</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>          
            <span class="n">bias</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="c1"># step 6. calculate mean of valid loss after each epoch to see the improvement</span>
    <span class="n">batch_size_valid</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="n">nr_iters_valid</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_valid</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size_valid</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_acc</span>  <span class="o">=</span> <span class="mi">0</span>    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">nr_iters_valid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
            <span class="n">start_index</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">batch_size_valid</span>
            <span class="n">end_index</span> <span class="o">=</span> <span class="n">start_index</span> <span class="o">+</span> <span class="n">batch_size_valid</span>
            <span class="n">xs</span> <span class="o">=</span> <span class="n">x_valid</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span><span class="n">end_index</span><span class="p">]</span>
            <span class="n">ys</span> <span class="o">=</span> <span class="n">y_valid</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span><span class="n">end_index</span><span class="p">]</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">simplenet</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span> 
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">xs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
            
            <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
            <span class="n">total_acc</span> <span class="o">+=</span> <span class="n">acc</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">xs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, valid_loss </span><span class="si">{</span><span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_valid</span><span class="p">)</span><span class="si">}</span><span class="s2">, accuracy </span><span class="si">{</span><span class="n">total_acc</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_valid</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
            
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch 0, valid_loss 0.5325431520462036, accuracy 0.8139
epoch 1, valid_loss 0.49808417506217956, accuracy 0.8252
epoch 2, valid_loss 0.48265715327262876, accuracy 0.8305
epoch 3, valid_loss 0.4735100971221924, accuracy 0.8335
epoch 4, valid_loss 0.46733067717552185, accuracy 0.8356
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>During the first training epoches, the valid loss should decrease and the accuracy should increase. Otherwise, you did something wrong.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Section-3:-refactor-model-using-Pytorch-built-in-modules">
<a class="anchor" href="#Section-3:-refactor-model-using-Pytorch-built-in-modules" aria-hidden="true"><span class="octicon octicon-link"></span></a>Section 3: refactor model using Pytorch built-in modules<a class="anchor-link" href="#Section-3:-refactor-model-using-Pytorch-built-in-modules"> </a>
</h2>
<p>We will gradually refactor our <code>simplenet</code> with Pytorch built-in modules, so that it does the same thing as before but start taking advantage of Pytorch's modules to make it more concise, more understandable and/or flexible.<br>
To make things more gradual and more understandable, the refactoring will be divided into 3 stages.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Refactor-stage-1">
<a class="anchor" href="#Refactor-stage-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Refactor stage 1<a class="anchor-link" href="#Refactor-stage-1"> </a>
</h3>
<ul>
<li>Refactor loss fuction with <code>torch.nn.functional.cross_entropy</code> function.</li>
<li>Refactor model with <code>nn.Module</code>, <code>nn.Parameter</code> class.</li>
<li>Refactor optimization algorithm with <code>model.parameters</code> and <code>model.zero_grad</code> method.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We first will refactor the <code>log_softmax</code> and <code>nll</code> method with Pytorch built-in function <code>torch.nn.functional.cross_entropy</code> that combines the two. So we can even remove the activation function from our model.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="c1"># old code</span>
<span class="c1"># def log_softmax(x): return x - x.exp().sum(-1).log().unsqueeze(-1)</span>
<span class="c1"># def simplenet(x): return log_softmax(x @ weights + bias)</span>

<span class="c1"># refactor code</span>
<span class="n">loss_function</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span>
<span class="k">def</span> <span class="nf">simplenet</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">weights</span> <span class="o">+</span> <span class="n">bias</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we will refactor our <code>simplenet</code> using <code>torch.nn</code> module.</p>
<p><code>torch.nn</code> is PyTorch’s neural network (nn) library which contains the primary components to construct network's layers. Within the <code>torch.nn</code> package, there is a class called <code>Module</code>, and it is the base class for all of neural network modules, including layers. All of the layers in PyTorch need to extend this base class in order to inherit all of PyTorch’s built-in functionality within the <code>nn.Module</code> class.<br>
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong><code>nn.Module</code> (uppercase M) is a Pytorch specific concept, and is a class we’ll be using a lot. Do not confuse with the Python concept of a (lowercase m) <code>module</code>, which is a file of Python code that can be imported.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In order to create model using <code>nn.Module</code>, we have 3 essential steps:</p>
<ol>
<li>Create a neural network class that extends the <code>nn.Module</code> base class.</li>
<li>Define the network's layers as class attributes in <code>__init__</code> method.<ul>
<li>The layers's learnable parameters are initialized in this step. But they need to be wrapped in <code>nn.Parameters</code> class in order to help <code>nn.Module</code> know those are learnable parameter. The weight tensor inside every layer is an instance of this <code>Parameter</code> class. PyTorch’s <code>nn.Module</code> class is basically looking for any attributes whose values are instances of the <code>Parameter</code> class, and when it finds an instance of the parameter class, it keeps track of it. Take a look at <code>Random topics</code> section for more detail information about network parameters.</li>
</ul>
</li>
<li>Define the network's transformation (operation) in <code>forward</code> method.<ul>
<li>Every Pytorch <code>nn.Module</code> has a <code>forward()</code> method and so when we are building layers and networks, we must provide an implementation of the <code>forward()</code> method. The forward method is the actual transformation.</li>
<li>The tensor input is passed forward though each layer transformation until the tensor reaches the output layer. The composition of all the individual layer forward passes defines the overall forward pass transformation for the network. The goal of the overall transformation is to transform or map the input to the correct prediction output class, and during the training process, the layer weights (data) are updated in such a way that cause the mapping to adjust to make the output closer to the correct prediction.</li>
<li>When we implement the <code>forward()</code> method of our <code>nn.Module</code> subclass, we will typically use layers'attributes and functions from the <code>nn.functional</code> package. This package provides us with many neural network operations that we can use for building layers. </li>
</ul>
</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="c1"># old code</span>
<span class="c1"># weights = torch.randn(784,10) / math.sqrt(784)</span>
<span class="c1"># weights.requires_grad_()</span>
<span class="c1"># bias    = torch.zeros(10, requires_grad=True)</span>
<span class="c1"># def simplenet(x): return log_softmax(x @ weights + bias)</span>

<span class="c1"># refactor code</span>
<span class="k">class</span> <span class="nc">SimpleNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">784</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>    <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
    
<span class="n">simplenet</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>One thing to point out that Pytorch neural network modules are <code>callable Python objects</code>. It means we can call the <code>SimplenNet</code>'s object as it was a function.<br>
What makes this possible is that PyTorch module classes implement a special Python function called <code>__call__()</code>. which will be invoked anytime the object instance is called. After the object instance is called, the <code>__call__()</code> method is invoked under the hood, and the <code>__call__()</code> in turn invokes the <code>forward()</code> method. Instead of calling the <code>forward()</code> method directly, we call the object instance. This applies to all PyTorch neural network modules, namely, networks and layers.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In order to access the model parameters, we can use <code>parameters()</code> or <code>named_parameters()</code> method. Next we will refactor optimization algorithm using <code>nn.Module.parameters</code> and <code>nn.Module.zero_grad</code> method.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>    
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>  
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>     

<span class="n">nr_iters</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span> <span class="o">//</span> <span class="n">bs</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">nr_iters</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
        <span class="c1"># step 1. get batch of training set</span>
        <span class="n">start_index</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">batch_size</span>
        <span class="n">end_index</span> <span class="o">=</span> <span class="n">start_index</span> <span class="o">+</span> <span class="n">batch_size</span>
        <span class="n">xs</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span><span class="n">end_index</span><span class="p">]</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span><span class="n">end_index</span><span class="p">]</span>
        <span class="c1"># step 2. pass batch to network</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">simplenet</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
        <span class="c1"># step 3. calculate the loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
        <span class="c1"># step 4. calculate the gradient of the loss w.r.t the network's parameters</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span> 
            <span class="c1"># step 5. update the weights using SGD algorithm</span>
            
            <span class="c1"># old code</span>
            <span class="c1"># weights -= lr * weights.grad</span>
            <span class="c1"># bias    -= lr * bias.grad</span>
            <span class="c1"># weights.grad.zero_()          </span>
            <span class="c1"># bias.grad.zero_()</span>

            <span class="c1"># refactor code</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">simplenet</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span> <span class="n">p</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">simplenet</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>


    <span class="c1"># step 6. calculate mean of valid loss after each epoch to see the improvement</span>
    <span class="n">batch_size_valid</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="n">nr_iters_valid</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_valid</span><span class="p">)</span> <span class="o">//</span> <span class="n">batch_size_valid</span>
    <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_acc</span>  <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">((</span><span class="n">nr_iters_valid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)):</span>
            <span class="n">start_index</span> <span class="o">=</span> <span class="n">i</span> <span class="o">*</span> <span class="n">batch_size_valid</span>
            <span class="n">end_index</span> <span class="o">=</span> <span class="n">start_index</span> <span class="o">+</span> <span class="n">batch_size_valid</span>
            <span class="n">xs</span> <span class="o">=</span> <span class="n">x_valid</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span><span class="n">end_index</span><span class="p">]</span>
            <span class="n">ys</span> <span class="o">=</span> <span class="n">y_valid</span><span class="p">[</span><span class="n">start_index</span><span class="p">:</span><span class="n">end_index</span><span class="p">]</span>
            <span class="n">preds</span> <span class="o">=</span> <span class="n">simplenet</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span> 
            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">xs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 

            <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
            <span class="n">total_acc</span> <span class="o">+=</span> <span class="n">acc</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">xs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, valid_loss </span><span class="si">{</span><span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_valid</span><span class="p">)</span><span class="si">}</span><span class="s2">, accuracy </span><span class="si">{</span><span class="n">total_acc</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_valid</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch 0, valid_loss 0.5341379848957062, accuracy 0.8131
epoch 1, valid_loss 0.49990872111320495, accuracy 0.8275
epoch 2, valid_loss 0.48446780610084533, accuracy 0.8317
epoch 3, valid_loss 0.47528300595283507, accuracy 0.8343
epoch 4, valid_loss 0.4690569020748138, accuracy 0.8355
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Ok so we have finished refactor stage 1. Let's move to refactor stage 2.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Refactor-stage-2">
<a class="anchor" href="#Refactor-stage-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Refactor stage 2<a class="anchor-link" href="#Refactor-stage-2"> </a>
</h3>
<ul>
<li>Refactor model with <code>nn.Linear</code> class.</li>
<li>Refactor data setup with <code>torch.utils.data.TensorDataset</code>, <code>torch.utils.data.DataLoader</code>.</li>
<li>Refactor optimization algorithm with <code>torch.optim</code> module.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Pytorch <code>nn.Linear</code> class does all the things that we have done for linear layer, including intialize learnable parameters and define linear operation. 
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>We used the abbreviation <code>fc</code> below because linear layers are also called fully connected layers or dense layer. Thus, <code>linear</code> = <code>dense</code> = <code>fully connected.</code>
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">SimpleNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
<span class="n">simplenet</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span>    
<span class="n">simplenet</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SimpleNet(
  (fc): Linear(in_features=784, out_features=10, bias=True)
)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next is <code>Dataset</code> and <code>DataLoader</code>.</p>
<p><code>torch.utils.data.Dataset</code> is an abstract class for representing a dataset. An abstract class is a Python class that has methods we must implement, in our case are <code>__getitem__</code> and <code>__len__</code>. In order to create a custom dataset, we need to subclass the <code>Dataset</code> class and override <code>__len__</code>, that provides the size of the dataset, and <code>__getitem__</code>, supporting integer indexing in range from 0 to len(self) exclusive. Upon doing this, our new subclass can then be passed to the a PyTorch DataLoader object.</p>
<p>PyTorch’s <code>TensorDataset</code> is a Dataset wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice <code>along the first dimension of a tensor</code>. This will make it easier to access both the independent and dependent variables in the same line as we train.</p>
<p><code>torch.utils.data.DataLoader</code> is responsible for managing batches. It makes life easier to iterate over batches.
We can create a <code>DataLoader</code> from any <code>Dataset</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>     

<span class="n">train_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">valid_ds</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">x_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">)</span>

<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The code in training loop is now changed from</p>

<pre><code>for i in range((nr_iters + 1)):
    # step 1. get batch of training set
    start_index = i * batch_size
    end_index = start_index + batch_size
    xs = x_train[start_index:end_index]
    ys = y_train[start_index:end_index]
    # step 2. pass batch to network
    preds = simplenet(xs)</code></pre>
<p>to</p>

<pre><code>for xs,ys in train_dl:
    preds = simplenet(xs)</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>And <code>torch.optim</code> package.<br>
This module provides various optimization algorithms. Its API provides <code>step</code> and <code>zero_grad</code> method for weight updating and zero out gradient which will help us refactor our code further.</p>
<p>Here is the training loop after applying all of those above steps.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>    
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>  

<span class="n">simplenet</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">simplenet</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">simplenet</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">xs</span><span class="p">,</span><span class="n">ys</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
        <span class="n">preds</span> <span class="o">=</span> <span class="n">simplenet</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
        
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    
    <span class="n">simplenet</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">loss_function</span><span class="p">(</span><span class="n">simplenet</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span><span class="n">ys</span><span class="p">)</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>  <span class="k">for</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="ow">in</span> <span class="n">valid_dl</span><span class="p">)</span>
        <span class="n">total_acc</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">accuracy</span><span class="p">(</span><span class="n">simplenet</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span><span class="n">ys</span><span class="p">)</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span> <span class="k">for</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="ow">in</span> <span class="n">valid_dl</span><span class="p">)</span>
        
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, valid_loss </span><span class="si">{</span><span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_valid</span><span class="p">)</span><span class="si">}</span><span class="s2">, accuracy </span><span class="si">{</span><span class="n">total_acc</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">x_valid</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch 0, valid_loss 0.5226423740386963, accuracy 0.8163999915122986
epoch 1, valid_loss 0.49401089549064636, accuracy 0.82669997215271
epoch 2, valid_loss 0.48364755511283875, accuracy 0.8306000232696533
epoch 3, valid_loss 0.4771580994129181, accuracy 0.8323000073432922
epoch 4, valid_loss 0.4616956412792206, accuracy 0.8371000289916992
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p></p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>we always call <code>model.train()</code> before training, and <code>model.eval()</code> before inference, because these are used by layers such as <code>nn.BatchNorm2d</code> and <code>nn.Dropout</code> to ensure appropriate behaviour for these different phases.
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Done!<br>
We have finished refactor stage 2 thanks to Pytorch built-in modules. Our training loop is now dramatically smaller and easier to understand.<br>
The refactor stage 3 does not introduce any new Pytorch modules. It is only an bonus step which help the code a bit cleaner and less code.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Refactor-stage-3">
<a class="anchor" href="#Refactor-stage-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Refactor stage 3<a class="anchor-link" href="#Refactor-stage-3"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>get_data</code> function returns dataloaders for the training set and validation set.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_ds</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">bs</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>get_model</code> function returns instance of our model and the optimizer.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">model</span><span class="p">()</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">m</span><span class="p">,</span> <span class="n">opt</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>calc_loss_batch</code> function returns loss value of batch and number of samples in that batch. We create this function because we go through this process twice, calculating the loss for both the training set and the validation set.
We pass an optimizer in for the training set, and use it to perform backprop. For the validation set, we don’t pass an optimizer, so the method doesn’t perform backprop. As a bonus, the accuracy is calculated if it is not None.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">calc_loss_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">opt</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">ys</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">opt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        
    <span class="k">if</span> <span class="n">metric</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">acc</span> <span class="o">=</span> <span class="n">metric</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">ys</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">acc</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><code>fit</code> function runs the necessary operations to train our model and compute the training loss, as well as validation losses and validation accuracy at each epoch.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">metric</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
            <span class="n">loss_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>
            
        <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">losses</span><span class="p">,</span> <span class="n">accs</span><span class="p">,</span> <span class="n">nums</span> <span class="o">=</span>  <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">loss_batch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="p">,</span> <span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">)</span> <span class="k">for</span> <span class="n">xs</span><span class="p">,</span><span class="n">ys</span> <span class="ow">in</span> <span class="n">valid_dl</span><span class="p">])</span>
            
        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">losses</span><span class="p">,</span> <span class="n">nums</span><span class="p">))</span>
        <span class="n">total_acc</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">accs</span><span class="p">,</span> <span class="n">nums</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">, valid_loss </span><span class="si">{</span><span class="n">total_loss</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">nums</span><span class="p">)</span><span class="si">}</span><span class="s2">, accuracy </span><span class="si">{</span><span class="n">total_acc</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">nums</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>

<span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span> <span class="o">=</span> <span class="n">get_data</span><span class="p">(</span><span class="n">train_ds</span><span class="p">,</span> <span class="n">valid_ds</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
<span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">get_model</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">SimpleNet</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
<span class="n">fit</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_function</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">,</span> <span class="n">opt</span><span class="p">,</span> <span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>epoch 0, valid_loss 0.5245783556461334, accuracy 0.8162
epoch 1, valid_loss 0.49596426906585694, accuracy 0.8261
epoch 2, valid_loss 0.4808829068660736, accuracy 0.8334
epoch 3, valid_loss 0.47098530049324033, accuracy 0.8347
epoch 4, valid_loss 0.4786785946369171, accuracy 0.831
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Done!<br>
So we have gone through all 3 refactor stages and now we have a clean and flexible function for getting data, create and training model.<br>
In part 3 of this serie, we will use those functions to train a Convolutional Neural Network (CNN).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Section-4:-random-topics">
<a class="anchor" href="#Section-4:-random-topics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Section 4: random topics<a class="anchor-link" href="#Section-4:-random-topics"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Disabling-PyTorch-Gradient-Tracking">
<a class="anchor" href="#Disabling-PyTorch-Gradient-Tracking" aria-hidden="true"><span class="octicon octicon-link"></span></a>Disabling PyTorch Gradient Tracking<a class="anchor-link" href="#Disabling-PyTorch-Gradient-Tracking"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Get predictions for the entire training set
Note at the top, we have annotated the function using the @torch.no_grad() PyTorch decoration. 
This is because we want this functions execution to omit gradient tracking. This is because gradient 
tracking uses memory, and during inference (getting predictions while not training) there is no need to 
keep track of the computational graph. The decoration is one way of locally turning off the gradient tracking
feature while executing specific functions. We specifically need the gradient calculation feature anytime we 
are going to calculate gradients using the backward() function. Otherwise, it is a good idea to turn it off
because having it off will reduce memory consumption for computations, e.g. when we are using networks for 
predicting (inference).</p>
<p>As another example, we can use Python's with context manger keyword to specify that a specify block of code
should exclude gradient computations.
Both of these options are valid</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">get_all_preds</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">loader</span><span class="p">):</span>
    <span class="n">all_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">loader</span><span class="p">:</span>
        <span class="n">imgs</span><span class="p">,</span> <span class="n">lbs</span> <span class="o">=</span> <span class="n">batch</span>
        
        <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
        <span class="n">all_preds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">all_preds</span><span class="p">,</span> <span class="n">preds</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">all_preds</span>

<span class="c1"># Locally Disabling PyTorch Gradient Tracking</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">prediction_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
    <span class="n">train_preds</span> <span class="o">=</span> <span class="n">get_all_preds</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">prediction_loader</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For more information, please check <a href="https://pytorch.org/docs/stable/notes/autograd.html">here</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Reference">
<a class="anchor" href="#Reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference<a class="anchor-link" href="#Reference"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Some good sources:</p>
<ul>
<li><a href="https://github.com/hunkim/PyTorchZeroToAll">pytorch zero to all</a></li>
<li><a href="https://deeplizard.com/learn/video/v5cngxo4mIg">deeplizard</a></li>
<li><a href="https://github.com/vahidk/EffectivePyTorch?fbclid=IwAR1MhsjnjccWy6dIVtibFOCZbWhLtAj5pSTobnkUDxw_gHgfEswnVzqrKQ0#torchscript">effective pytorch</a></li>
<li><a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html">what is torch.nn really?</a></li>
<li><a href="https://forums.fast.ai/t/getting-comfortable-with-pytorch-projects/28371">recommend walk with pytorch</a></li>
<li><a href="https://pytorch.org/tutorials/">official tutorial</a></li>
<li><a href="https://github.com/Atcold/pytorch-Deep-Learning">DL(with Pytorch)</a></li>
<li><a href="https://github.com/moemen95/PyTorch-Project-Template">Pytorch project template</a></li>
<li><a href="https://github.com/graykode/nlp-tutorial">nlp turorial with pytorch</a></li>
<li><a href="https://www.udacity.com/course/deep-learning-pytorch--ud188">UDACITY course</a></li>
<li><a href="https://github.com/bharathgs/Awesome-pytorch-list">awesome pytorch list</a></li>
<li><a href="https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf">deep learning with pytorch</a></li>
<li>others:<ul>
<li><a href="https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc">https://medium.com/pytorch/get-started-with-pytorch-cloud-tpus-and-colab-a24757b8f7fc</a></li>
<li>Grokking Algorithms: An illustrated guide for programmers and other curious people 1st Edition</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="phucnsp/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/self-taught/2020/03/22/self-taught-pytorch-part2-nn-from-scratch.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>This blog contains my personal view about everything either in work and life.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/phucnsp" title="phucnsp"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/Phuc_Nguyen_Su" title="Phuc_Nguyen_Su"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
